{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il9xnF2BE42X"
   },
   "source": [
    "## Matrix factorization using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVYNPoYEMJaX"
   },
   "source": [
    "Solution inspired from https://developers.google.com/machine-learning/recommendation/overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QaJw1C1nCGb"
   },
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_2fysrN07w1-"
   },
   "outputs": [],
   "source": [
    "bx_preprocessed_dataset_path = \"bx_data/preprocessed_dataset/\"\n",
    "\n",
    "ratings_path = bx_preprocessed_dataset_path + \"preprocessed_ratings_data.pkl\"\n",
    "book_metadata_path = bx_preprocessed_dataset_path + \"preprocessed_book_metadata.pkl\"\n",
    "\n",
    "embeddings_save_path = \"book_embeddings/gd_book_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptC8-cqv5OT"
   },
   "source": [
    "\n",
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v_u4Tdr6rBAq"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc_op4-Qv61E"
   },
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ER8CIDhiEKjv"
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_pickle(ratings_path)\n",
    "books = pd.read_pickle(book_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2TG04CUg7xOt"
   },
   "outputs": [],
   "source": [
    "# Split the ratings dataframe into train and test parts\n",
    "def split_dataframe(ratings_df, test_size=0.2):\n",
    "\n",
    "    test_indices = np.array(\n",
    "        torch.randperm(len(ratings_df)) < len(ratings_df) * test_size)\n",
    "\n",
    "    train_df = ratings_df[~test_indices]\n",
    "    test_df = ratings_df[test_indices]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# SparseTensor representation of the train and test datasets\n",
    "def build_rating_sparse_tensor(ratings_df, users_count, book_count):\n",
    "\n",
    "    indices = torch.tensor(\n",
    "        ratings_df[['User-Embedding-ID', 'Book-Embedding-ID']].values,\n",
    "        dtype=torch.long).t()\n",
    "    values = torch.tensor(\n",
    "        ratings_df['Book-Rating'].values,\n",
    "        dtype=torch.float)\n",
    "    shape = (users_count, book_count)\n",
    "\n",
    "    sparse_rating_tensor = torch.sparse.FloatTensor(\n",
    "        indices=indices,\n",
    "        values=values,\n",
    "        size=shape)\n",
    "\n",
    "    return sparse_rating_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W0XmtTD2q_Rl"
   },
   "outputs": [],
   "source": [
    "# Calculate the mean squared error loss\n",
    "def sparse_mse_loss_func(sparse_ratings, user_embeddings, movie_embeddings):\n",
    "    sparse_ratings = sparse_ratings.coalesce()\n",
    "    user_indices = sparse_ratings.indices()[0,:]\n",
    "    movie_indices = sparse_ratings.indices()[1,:]\n",
    "\n",
    "    user_selected = user_embeddings(user_indices)\n",
    "    movie_selected = movie_embeddings(movie_indices)\n",
    "\n",
    "    predictions = torch.sum(user_selected * movie_selected, axis=1)\n",
    "    loss = F.mse_loss(predictions, sparse_ratings.values())\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Regularization by gravity function\n",
    "def gravity_loss_func(U, V):\n",
    "    return 1 / (U.shape[0] * V.shape[0]) * torch.sum(\n",
    "        torch.matmul(U.t(), U) * torch.matmul(V.t(), V))\n",
    "\n",
    "\n",
    "# Regularization by L1 norm\n",
    "def regularization_loss_func(U, V):\n",
    "    return (torch.sum(U * U) / U.shape[0] +\n",
    "            torch.sum(V * V) / V.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-vr50w37uEy"
   },
   "source": [
    "### Define Gradient Descent model for Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AOCbOpCp8zrp"
   },
   "outputs": [],
   "source": [
    "class GDModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_users, num_books,\n",
    "                 embedding_dim, weight_init_std=0.5):\n",
    "        super(GDModel, self).__init__()\n",
    "\n",
    "        self.user_embeddings = torch.nn.Embedding(\n",
    "            num_users, embedding_dim)\n",
    "        self.book_embeddings = torch.nn.Embedding(\n",
    "            num_books, embedding_dim)\n",
    "\n",
    "        self.user_embeddings.weight.data.normal_(\n",
    "            std=weight_init_std)\n",
    "        self.book_embeddings.weight.data.normal_(\n",
    "            std=weight_init_std)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        selected_users = self.user_embeddings(input_ids[0])\n",
    "        selected_books = self.book_embeddings(input_ids[1])\n",
    "        predictions = torch.sum(selected_users * selected_books, dim=1)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EApXaoFh6bSL"
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qYChznx-9Lnl",
    "outputId": "e7c86d85-ea8e-4a8e-d85b-9d2af58ae373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users 792 and Books 657\n"
     ]
    }
   ],
   "source": [
    "# Number of embedded items for model embedding layers\n",
    "users_count = ratings[\"User-Embedding-ID\"].nunique()\n",
    "book_count = ratings[\"Book-Embedding-ID\"].nunique()\n",
    "print(f\"Users {users_count} and Books {book_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "26j2eNmO9hWr"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epoch_count = 1000\n",
    "learning_rate = 0.001\n",
    "embedding_dim = 35\n",
    "\n",
    "gravity_loss_ratio = 1\n",
    "reg_loss_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3dkts0xg6aHc",
    "outputId": "b380b352-ae7c-4022-9e62-cc9e56688a56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 1/1 [00:00<00:00, 127.34epoch/s, test_loss=18.5, train_loss=18.8]\n",
      "Epoch 2/1000: 100%|██████████| 1/1 [00:00<00:00, 309.54epoch/s, test_loss=18.5, train_loss=18.8]\n",
      "Epoch 3/1000: 100%|██████████| 1/1 [00:00<00:00, 215.88epoch/s, test_loss=18.5, train_loss=18.7]\n",
      "Epoch 4/1000: 100%|██████████| 1/1 [00:00<00:00, 205.58epoch/s, test_loss=18.5, train_loss=18.7]\n",
      "Epoch 5/1000: 100%|██████████| 1/1 [00:00<00:00, 246.74epoch/s, test_loss=18.5, train_loss=18.7]\n",
      "Epoch 6/1000: 100%|██████████| 1/1 [00:00<00:00, 228.24epoch/s, test_loss=18.5, train_loss=18.7]\n",
      "Epoch 7/1000: 100%|██████████| 1/1 [00:00<00:00, 156.08epoch/s, test_loss=18.5, train_loss=18.6]\n",
      "Epoch 8/1000: 100%|██████████| 1/1 [00:00<00:00, 217.66epoch/s, test_loss=18.5, train_loss=18.6]\n",
      "Epoch 9/1000: 100%|██████████| 1/1 [00:00<00:00, 229.23epoch/s, test_loss=18.4, train_loss=18.6]\n",
      "Epoch 10/1000: 100%|██████████| 1/1 [00:00<00:00, 230.52epoch/s, test_loss=18.4, train_loss=18.5]\n",
      "Epoch 11/1000: 100%|██████████| 1/1 [00:00<00:00, 223.22epoch/s, test_loss=18.4, train_loss=18.5]\n",
      "Epoch 12/1000: 100%|██████████| 1/1 [00:00<00:00, 240.94epoch/s, test_loss=18.4, train_loss=18.5]\n",
      "Epoch 13/1000: 100%|██████████| 1/1 [00:00<00:00, 236.09epoch/s, test_loss=18.4, train_loss=18.5]\n",
      "Epoch 14/1000: 100%|██████████| 1/1 [00:00<00:00, 246.40epoch/s, test_loss=18.4, train_loss=18.4]\n",
      "Epoch 15/1000: 100%|██████████| 1/1 [00:00<00:00, 243.12epoch/s, test_loss=18.4, train_loss=18.4]\n",
      "Epoch 16/1000: 100%|██████████| 1/1 [00:00<00:00, 212.41epoch/s, test_loss=18.4, train_loss=18.4]\n",
      "Epoch 17/1000: 100%|██████████| 1/1 [00:00<00:00, 249.26epoch/s, test_loss=18.4, train_loss=18.3]\n",
      "Epoch 18/1000: 100%|██████████| 1/1 [00:00<00:00, 232.26epoch/s, test_loss=18.4, train_loss=18.3]\n",
      "Epoch 19/1000: 100%|██████████| 1/1 [00:00<00:00, 234.27epoch/s, test_loss=18.3, train_loss=18.3]\n",
      "Epoch 20/1000: 100%|██████████| 1/1 [00:00<00:00, 238.27epoch/s, test_loss=18.3, train_loss=18.3]\n",
      "Epoch 21/1000: 100%|██████████| 1/1 [00:00<00:00, 106.95epoch/s, test_loss=18.3, train_loss=18.2]\n",
      "Epoch 22/1000: 100%|██████████| 1/1 [00:00<00:00, 246.93epoch/s, test_loss=18.3, train_loss=18.2]\n",
      "Epoch 23/1000: 100%|██████████| 1/1 [00:00<00:00, 249.35epoch/s, test_loss=18.3, train_loss=18.2]\n",
      "Epoch 24/1000: 100%|██████████| 1/1 [00:00<00:00, 162.07epoch/s, test_loss=18.3, train_loss=18.1]\n",
      "Epoch 25/1000: 100%|██████████| 1/1 [00:00<00:00, 146.87epoch/s, test_loss=18.3, train_loss=18.1]\n",
      "Epoch 26/1000: 100%|██████████| 1/1 [00:00<00:00, 240.61epoch/s, test_loss=18.3, train_loss=18.1]\n",
      "Epoch 27/1000: 100%|██████████| 1/1 [00:00<00:00, 257.43epoch/s, test_loss=18.3, train_loss=18.1]\n",
      "Epoch 28/1000: 100%|██████████| 1/1 [00:00<00:00, 238.71epoch/s, test_loss=18.3, train_loss=18]\n",
      "Epoch 29/1000: 100%|██████████| 1/1 [00:00<00:00, 239.40epoch/s, test_loss=18.3, train_loss=18]\n",
      "Epoch 30/1000: 100%|██████████| 1/1 [00:00<00:00, 244.20epoch/s, test_loss=18.3, train_loss=18]\n",
      "Epoch 31/1000: 100%|██████████| 1/1 [00:00<00:00, 251.23epoch/s, test_loss=18.2, train_loss=18]\n",
      "Epoch 32/1000: 100%|██████████| 1/1 [00:00<00:00, 179.84epoch/s, test_loss=18.2, train_loss=17.9]\n",
      "Epoch 33/1000: 100%|██████████| 1/1 [00:00<00:00, 72.41epoch/s, test_loss=18.2, train_loss=17.9]\n",
      "Epoch 34/1000: 100%|██████████| 1/1 [00:00<00:00, 138.67epoch/s, test_loss=18.2, train_loss=17.9]\n",
      "Epoch 35/1000: 100%|██████████| 1/1 [00:00<00:00, 160.70epoch/s, test_loss=18.2, train_loss=17.9]\n",
      "Epoch 36/1000: 100%|██████████| 1/1 [00:00<00:00, 156.73epoch/s, test_loss=18.2, train_loss=17.8]\n",
      "Epoch 37/1000: 100%|██████████| 1/1 [00:00<00:00, 143.80epoch/s, test_loss=18.2, train_loss=17.8]\n",
      "Epoch 38/1000: 100%|██████████| 1/1 [00:00<00:00, 140.71epoch/s, test_loss=18.2, train_loss=17.8]\n",
      "Epoch 39/1000: 100%|██████████| 1/1 [00:00<00:00, 83.71epoch/s, test_loss=18.2, train_loss=17.8]\n",
      "Epoch 40/1000: 100%|██████████| 1/1 [00:00<00:00, 260.81epoch/s, test_loss=18.2, train_loss=17.7]\n",
      "Epoch 41/1000: 100%|██████████| 1/1 [00:00<00:00, 36.89epoch/s, test_loss=18.2, train_loss=17.7]\n",
      "Epoch 42/1000: 100%|██████████| 1/1 [00:00<00:00, 92.65epoch/s, test_loss=18.2, train_loss=17.7]\n",
      "Epoch 43/1000: 100%|██████████| 1/1 [00:00<00:00, 248.21epoch/s, test_loss=18.2, train_loss=17.7]\n",
      "Epoch 44/1000: 100%|██████████| 1/1 [00:00<00:00, 101.92epoch/s, test_loss=18.1, train_loss=17.6]\n",
      "Epoch 45/1000: 100%|██████████| 1/1 [00:00<00:00, 114.49epoch/s, test_loss=18.1, train_loss=17.6]\n",
      "Epoch 46/1000: 100%|██████████| 1/1 [00:00<00:00, 115.33epoch/s, test_loss=18.1, train_loss=17.6]\n",
      "Epoch 47/1000: 100%|██████████| 1/1 [00:00<00:00, 193.74epoch/s, test_loss=18.1, train_loss=17.6]\n",
      "Epoch 48/1000: 100%|██████████| 1/1 [00:00<00:00, 227.08epoch/s, test_loss=18.1, train_loss=17.5]\n",
      "Epoch 49/1000: 100%|██████████| 1/1 [00:00<00:00, 156.66epoch/s, test_loss=18.1, train_loss=17.5]\n",
      "Epoch 50/1000: 100%|██████████| 1/1 [00:00<00:00, 218.77epoch/s, test_loss=18.1, train_loss=17.5]\n",
      "Epoch 51/1000: 100%|██████████| 1/1 [00:00<00:00, 154.50epoch/s, test_loss=18.1, train_loss=17.5]\n",
      "Epoch 52/1000: 100%|██████████| 1/1 [00:00<00:00, 251.41epoch/s, test_loss=18.1, train_loss=17.4]\n",
      "Epoch 53/1000: 100%|██████████| 1/1 [00:00<00:00, 138.94epoch/s, test_loss=18.1, train_loss=17.4]\n",
      "Epoch 54/1000: 100%|██████████| 1/1 [00:00<00:00, 99.21epoch/s, test_loss=18.1, train_loss=17.4]\n",
      "Epoch 55/1000: 100%|██████████| 1/1 [00:00<00:00, 221.42epoch/s, test_loss=18.1, train_loss=17.4]\n",
      "Epoch 56/1000: 100%|██████████| 1/1 [00:00<00:00, 244.58epoch/s, test_loss=18.1, train_loss=17.3]\n",
      "Epoch 57/1000: 100%|██████████| 1/1 [00:00<00:00, 237.97epoch/s, test_loss=18.1, train_loss=17.3]\n",
      "Epoch 58/1000: 100%|██████████| 1/1 [00:00<00:00, 418.51epoch/s, test_loss=18, train_loss=17.3]\n",
      "Epoch 59/1000: 100%|██████████| 1/1 [00:00<00:00, 220.78epoch/s, test_loss=18, train_loss=17.3]\n",
      "Epoch 60/1000: 100%|██████████| 1/1 [00:00<00:00, 158.83epoch/s, test_loss=18, train_loss=17.2]\n",
      "Epoch 61/1000: 100%|██████████| 1/1 [00:00<00:00, 205.41epoch/s, test_loss=18, train_loss=17.2]\n",
      "Epoch 62/1000: 100%|██████████| 1/1 [00:00<00:00, 201.46epoch/s, test_loss=18, train_loss=17.2]\n",
      "Epoch 63/1000: 100%|██████████| 1/1 [00:00<00:00, 157.41epoch/s, test_loss=18, train_loss=17.2]\n",
      "Epoch 64/1000: 100%|██████████| 1/1 [00:00<00:00, 94.70epoch/s, test_loss=18, train_loss=17.2]\n",
      "Epoch 65/1000: 100%|██████████| 1/1 [00:00<00:00, 143.08epoch/s, test_loss=18, train_loss=17.1]\n",
      "Epoch 66/1000: 100%|██████████| 1/1 [00:00<00:00, 99.80epoch/s, test_loss=18, train_loss=17.1]\n",
      "Epoch 67/1000: 100%|██████████| 1/1 [00:00<00:00, 242.92epoch/s, test_loss=18, train_loss=17.1]\n",
      "Epoch 68/1000: 100%|██████████| 1/1 [00:00<00:00, 239.85epoch/s, test_loss=18, train_loss=17.1]\n",
      "Epoch 69/1000: 100%|██████████| 1/1 [00:00<00:00, 108.67epoch/s, test_loss=18, train_loss=17]\n",
      "Epoch 70/1000: 100%|██████████| 1/1 [00:00<00:00, 86.27epoch/s, test_loss=18, train_loss=17]\n",
      "Epoch 71/1000: 100%|██████████| 1/1 [00:00<00:00, 125.86epoch/s, test_loss=18, train_loss=17]\n",
      "Epoch 72/1000: 100%|██████████| 1/1 [00:00<00:00, 172.51epoch/s, test_loss=18, train_loss=17]\n",
      "Epoch 73/1000: 100%|██████████| 1/1 [00:00<00:00, 66.40epoch/s, test_loss=18, train_loss=16.9]\n",
      "Epoch 74/1000: 100%|██████████| 1/1 [00:00<00:00, 177.81epoch/s, test_loss=17.9, train_loss=16.9]\n",
      "Epoch 75/1000: 100%|██████████| 1/1 [00:00<00:00, 206.57epoch/s, test_loss=17.9, train_loss=16.9]\n",
      "Epoch 76/1000: 100%|██████████| 1/1 [00:00<00:00, 184.80epoch/s, test_loss=17.9, train_loss=16.9]\n",
      "Epoch 77/1000: 100%|██████████| 1/1 [00:00<00:00, 193.59epoch/s, test_loss=17.9, train_loss=16.9]\n",
      "Epoch 78/1000: 100%|██████████| 1/1 [00:00<00:00, 71.81epoch/s, test_loss=17.9, train_loss=16.8]\n",
      "Epoch 79/1000: 100%|██████████| 1/1 [00:00<00:00, 108.91epoch/s, test_loss=17.9, train_loss=16.8]\n",
      "Epoch 80/1000: 100%|██████████| 1/1 [00:00<00:00, 108.10epoch/s, test_loss=17.9, train_loss=16.8]\n",
      "Epoch 81/1000: 100%|██████████| 1/1 [00:00<00:00, 183.56epoch/s, test_loss=17.9, train_loss=16.8]\n",
      "Epoch 82/1000: 100%|██████████| 1/1 [00:00<00:00, 104.56epoch/s, test_loss=17.9, train_loss=16.8]\n",
      "Epoch 83/1000: 100%|██████████| 1/1 [00:00<00:00, 213.30epoch/s, test_loss=17.9, train_loss=16.7]\n",
      "Epoch 84/1000: 100%|██████████| 1/1 [00:00<00:00, 87.77epoch/s, test_loss=17.9, train_loss=16.7]\n",
      "Epoch 85/1000: 100%|██████████| 1/1 [00:00<00:00, 82.69epoch/s, test_loss=17.9, train_loss=16.7]\n",
      "Epoch 86/1000: 100%|██████████| 1/1 [00:00<00:00, 204.19epoch/s, test_loss=17.9, train_loss=16.7]\n",
      "Epoch 87/1000: 100%|██████████| 1/1 [00:00<00:00, 207.24epoch/s, test_loss=17.9, train_loss=16.6]\n",
      "Epoch 88/1000: 100%|██████████| 1/1 [00:00<00:00, 189.44epoch/s, test_loss=17.9, train_loss=16.6]\n",
      "Epoch 89/1000: 100%|██████████| 1/1 [00:00<00:00, 166.98epoch/s, test_loss=17.9, train_loss=16.6]\n",
      "Epoch 90/1000: 100%|██████████| 1/1 [00:00<00:00, 103.90epoch/s, test_loss=17.9, train_loss=16.6]\n",
      "Epoch 91/1000: 100%|██████████| 1/1 [00:00<00:00, 110.22epoch/s, test_loss=17.8, train_loss=16.6]\n",
      "Epoch 92/1000: 100%|██████████| 1/1 [00:00<00:00, 81.77epoch/s, test_loss=17.8, train_loss=16.5]\n",
      "Epoch 93/1000: 100%|██████████| 1/1 [00:00<00:00, 85.44epoch/s, test_loss=17.8, train_loss=16.5]\n",
      "Epoch 94/1000: 100%|██████████| 1/1 [00:00<00:00, 99.90epoch/s, test_loss=17.8, train_loss=16.5]\n",
      "Epoch 95/1000: 100%|██████████| 1/1 [00:00<00:00, 84.24epoch/s, test_loss=17.8, train_loss=16.5]\n",
      "Epoch 96/1000: 100%|██████████| 1/1 [00:00<00:00, 76.55epoch/s, test_loss=17.8, train_loss=16.5]\n",
      "Epoch 97/1000: 100%|██████████| 1/1 [00:00<00:00, 99.73epoch/s, test_loss=17.8, train_loss=16.4]\n",
      "Epoch 98/1000: 100%|██████████| 1/1 [00:00<00:00, 146.89epoch/s, test_loss=17.8, train_loss=16.4]\n",
      "Epoch 99/1000: 100%|██████████| 1/1 [00:00<00:00, 94.84epoch/s, test_loss=17.8, train_loss=16.4]\n",
      "Epoch 100/1000: 100%|██████████| 1/1 [00:00<00:00, 118.62epoch/s, test_loss=17.8, train_loss=16.4]\n",
      "Epoch 101/1000: 100%|██████████| 1/1 [00:00<00:00, 110.45epoch/s, test_loss=17.8, train_loss=16.4]\n",
      "Epoch 102/1000: 100%|██████████| 1/1 [00:00<00:00, 105.66epoch/s, test_loss=17.8, train_loss=16.3]\n",
      "Epoch 103/1000: 100%|██████████| 1/1 [00:00<00:00, 104.40epoch/s, test_loss=17.8, train_loss=16.3]\n",
      "Epoch 104/1000: 100%|██████████| 1/1 [00:00<00:00, 193.23epoch/s, test_loss=17.8, train_loss=16.3]\n",
      "Epoch 105/1000: 100%|██████████| 1/1 [00:00<00:00, 146.96epoch/s, test_loss=17.8, train_loss=16.3]\n",
      "Epoch 106/1000: 100%|██████████| 1/1 [00:00<00:00, 191.70epoch/s, test_loss=17.8, train_loss=16.2]\n",
      "Epoch 107/1000: 100%|██████████| 1/1 [00:00<00:00, 147.08epoch/s, test_loss=17.8, train_loss=16.2]\n",
      "Epoch 108/1000: 100%|██████████| 1/1 [00:00<00:00, 65.34epoch/s, test_loss=17.8, train_loss=16.2]\n",
      "Epoch 109/1000: 100%|██████████| 1/1 [00:00<00:00, 114.95epoch/s, test_loss=17.7, train_loss=16.2]\n",
      "Epoch 110/1000: 100%|██████████| 1/1 [00:00<00:00, 243.43epoch/s, test_loss=17.7, train_loss=16.2]\n",
      "Epoch 111/1000: 100%|██████████| 1/1 [00:00<00:00, 212.60epoch/s, test_loss=17.7, train_loss=16.1]\n",
      "Epoch 112/1000: 100%|██████████| 1/1 [00:00<00:00, 217.92epoch/s, test_loss=17.7, train_loss=16.1]\n",
      "Epoch 113/1000: 100%|██████████| 1/1 [00:00<00:00, 137.45epoch/s, test_loss=17.7, train_loss=16.1]\n",
      "Epoch 114/1000: 100%|██████████| 1/1 [00:00<00:00, 74.52epoch/s, test_loss=17.7, train_loss=16.1]\n",
      "Epoch 115/1000: 100%|██████████| 1/1 [00:00<00:00, 73.61epoch/s, test_loss=17.7, train_loss=16.1]\n",
      "Epoch 116/1000: 100%|██████████| 1/1 [00:00<00:00, 101.44epoch/s, test_loss=17.7, train_loss=16]\n",
      "Epoch 117/1000: 100%|██████████| 1/1 [00:00<00:00, 87.56epoch/s, test_loss=17.7, train_loss=16]\n",
      "Epoch 118/1000: 100%|██████████| 1/1 [00:00<00:00, 102.32epoch/s, test_loss=17.7, train_loss=16]\n",
      "Epoch 119/1000: 100%|██████████| 1/1 [00:00<00:00, 108.00epoch/s, test_loss=17.7, train_loss=16]\n",
      "Epoch 120/1000: 100%|██████████| 1/1 [00:00<00:00, 36.10epoch/s, test_loss=17.7, train_loss=16]\n",
      "Epoch 121/1000: 100%|██████████| 1/1 [00:00<00:00, 116.60epoch/s, test_loss=17.7, train_loss=15.9]\n",
      "Epoch 122/1000: 100%|██████████| 1/1 [00:00<00:00, 198.33epoch/s, test_loss=17.7, train_loss=15.9]\n",
      "Epoch 123/1000: 100%|██████████| 1/1 [00:00<00:00, 61.09epoch/s, test_loss=17.7, train_loss=15.9]\n",
      "Epoch 124/1000: 100%|██████████| 1/1 [00:00<00:00, 85.53epoch/s, test_loss=17.7, train_loss=15.9]\n",
      "Epoch 125/1000: 100%|██████████| 1/1 [00:00<00:00, 144.54epoch/s, test_loss=17.7, train_loss=15.9]\n",
      "Epoch 126/1000: 100%|██████████| 1/1 [00:00<00:00, 92.68epoch/s, test_loss=17.7, train_loss=15.8]\n",
      "Epoch 127/1000: 100%|██████████| 1/1 [00:00<00:00, 99.75epoch/s, test_loss=17.6, train_loss=15.8]\n",
      "Epoch 128/1000: 100%|██████████| 1/1 [00:00<00:00, 319.25epoch/s, test_loss=17.6, train_loss=15.8]\n",
      "Epoch 129/1000: 100%|██████████| 1/1 [00:00<00:00, 212.81epoch/s, test_loss=17.6, train_loss=15.8]\n",
      "Epoch 130/1000: 100%|██████████| 1/1 [00:00<00:00, 125.82epoch/s, test_loss=17.6, train_loss=15.8]\n",
      "Epoch 131/1000: 100%|██████████| 1/1 [00:00<00:00, 210.79epoch/s, test_loss=17.6, train_loss=15.7]\n",
      "Epoch 132/1000: 100%|██████████| 1/1 [00:00<00:00, 211.32epoch/s, test_loss=17.6, train_loss=15.7]\n",
      "Epoch 133/1000: 100%|██████████| 1/1 [00:00<00:00, 113.62epoch/s, test_loss=17.6, train_loss=15.7]\n",
      "Epoch 134/1000: 100%|██████████| 1/1 [00:00<00:00, 94.64epoch/s, test_loss=17.6, train_loss=15.7]\n",
      "Epoch 135/1000: 100%|██████████| 1/1 [00:00<00:00, 81.23epoch/s, test_loss=17.6, train_loss=15.7]\n",
      "Epoch 136/1000: 100%|██████████| 1/1 [00:00<00:00, 80.53epoch/s, test_loss=17.6, train_loss=15.6]\n",
      "Epoch 137/1000: 100%|██████████| 1/1 [00:00<00:00, 78.32epoch/s, test_loss=17.6, train_loss=15.6]\n",
      "Epoch 138/1000: 100%|██████████| 1/1 [00:00<00:00, 235.67epoch/s, test_loss=17.6, train_loss=15.6]\n",
      "Epoch 139/1000: 100%|██████████| 1/1 [00:00<00:00, 264.69epoch/s, test_loss=17.6, train_loss=15.6]\n",
      "Epoch 140/1000: 100%|██████████| 1/1 [00:00<00:00, 87.18epoch/s, test_loss=17.6, train_loss=15.6]\n",
      "Epoch 141/1000: 100%|██████████| 1/1 [00:00<00:00, 108.24epoch/s, test_loss=17.6, train_loss=15.5]\n",
      "Epoch 142/1000: 100%|██████████| 1/1 [00:00<00:00, 87.95epoch/s, test_loss=17.6, train_loss=15.5]\n",
      "Epoch 143/1000: 100%|██████████| 1/1 [00:00<00:00, 110.86epoch/s, test_loss=17.6, train_loss=15.5]\n",
      "Epoch 144/1000: 100%|██████████| 1/1 [00:00<00:00, 93.37epoch/s, test_loss=17.5, train_loss=15.5]\n",
      "Epoch 145/1000: 100%|██████████| 1/1 [00:00<00:00, 94.80epoch/s, test_loss=17.5, train_loss=15.5]\n",
      "Epoch 146/1000: 100%|██████████| 1/1 [00:00<00:00, 96.34epoch/s, test_loss=17.5, train_loss=15.4]\n",
      "Epoch 147/1000: 100%|██████████| 1/1 [00:00<00:00, 90.01epoch/s, test_loss=17.5, train_loss=15.4]\n",
      "Epoch 148/1000: 100%|██████████| 1/1 [00:00<00:00, 49.79epoch/s, test_loss=17.5, train_loss=15.4]\n",
      "Epoch 149/1000: 100%|██████████| 1/1 [00:00<00:00, 47.43epoch/s, test_loss=17.5, train_loss=15.4]\n",
      "Epoch 150/1000: 100%|██████████| 1/1 [00:00<00:00, 203.20epoch/s, test_loss=17.5, train_loss=15.4]\n",
      "Epoch 151/1000: 100%|██████████| 1/1 [00:00<00:00, 271.02epoch/s, test_loss=17.5, train_loss=15.3]\n",
      "Epoch 152/1000: 100%|██████████| 1/1 [00:00<00:00, 151.02epoch/s, test_loss=17.5, train_loss=15.3]\n",
      "Epoch 153/1000: 100%|██████████| 1/1 [00:00<00:00, 79.21epoch/s, test_loss=17.5, train_loss=15.3]\n",
      "Epoch 154/1000: 100%|██████████| 1/1 [00:00<00:00, 110.66epoch/s, test_loss=17.5, train_loss=15.3]\n",
      "Epoch 155/1000: 100%|██████████| 1/1 [00:00<00:00, 154.44epoch/s, test_loss=17.5, train_loss=15.3]\n",
      "Epoch 156/1000: 100%|██████████| 1/1 [00:00<00:00, 90.34epoch/s, test_loss=17.5, train_loss=15.2]\n",
      "Epoch 157/1000: 100%|██████████| 1/1 [00:00<00:00, 67.93epoch/s, test_loss=17.5, train_loss=15.2]\n",
      "Epoch 158/1000: 100%|██████████| 1/1 [00:00<00:00, 41.45epoch/s, test_loss=17.5, train_loss=15.2]\n",
      "Epoch 159/1000: 100%|██████████| 1/1 [00:00<00:00, 55.89epoch/s, test_loss=17.5, train_loss=15.2]\n",
      "Epoch 160/1000: 100%|██████████| 1/1 [00:00<00:00, 219.48epoch/s, test_loss=17.4, train_loss=15.2]\n",
      "Epoch 161/1000: 100%|██████████| 1/1 [00:00<00:00, 219.51epoch/s, test_loss=17.4, train_loss=15.1]\n",
      "Epoch 162/1000: 100%|██████████| 1/1 [00:00<00:00, 54.23epoch/s, test_loss=17.4, train_loss=15.1]\n",
      "Epoch 163/1000: 100%|██████████| 1/1 [00:00<00:00, 167.02epoch/s, test_loss=17.4, train_loss=15.1]\n",
      "Epoch 164/1000: 100%|██████████| 1/1 [00:00<00:00, 170.99epoch/s, test_loss=17.4, train_loss=15.1]\n",
      "Epoch 165/1000: 100%|██████████| 1/1 [00:00<00:00, 127.17epoch/s, test_loss=17.4, train_loss=15.1]\n",
      "Epoch 166/1000: 100%|██████████| 1/1 [00:00<00:00, 123.31epoch/s, test_loss=17.4, train_loss=15]\n",
      "Epoch 167/1000: 100%|██████████| 1/1 [00:00<00:00, 192.40epoch/s, test_loss=17.4, train_loss=15]\n",
      "Epoch 168/1000: 100%|██████████| 1/1 [00:00<00:00, 186.77epoch/s, test_loss=17.4, train_loss=15]\n",
      "Epoch 169/1000: 100%|██████████| 1/1 [00:00<00:00, 97.12epoch/s, test_loss=17.4, train_loss=15]\n",
      "Epoch 170/1000: 100%|██████████| 1/1 [00:00<00:00, 103.52epoch/s, test_loss=17.4, train_loss=15]\n",
      "Epoch 171/1000: 100%|██████████| 1/1 [00:00<00:00, 79.48epoch/s, test_loss=17.4, train_loss=14.9]\n",
      "Epoch 172/1000: 100%|██████████| 1/1 [00:00<00:00, 175.51epoch/s, test_loss=17.4, train_loss=14.9]\n",
      "Epoch 173/1000: 100%|██████████| 1/1 [00:00<00:00, 205.82epoch/s, test_loss=17.4, train_loss=14.9]\n",
      "Epoch 174/1000: 100%|██████████| 1/1 [00:00<00:00, 74.22epoch/s, test_loss=17.3, train_loss=14.9]\n",
      "Epoch 175/1000: 100%|██████████| 1/1 [00:00<00:00, 137.80epoch/s, test_loss=17.3, train_loss=14.9]\n",
      "Epoch 176/1000: 100%|██████████| 1/1 [00:00<00:00, 200.89epoch/s, test_loss=17.3, train_loss=14.8]\n",
      "Epoch 177/1000: 100%|██████████| 1/1 [00:00<00:00, 122.80epoch/s, test_loss=17.3, train_loss=14.8]\n",
      "Epoch 178/1000: 100%|██████████| 1/1 [00:00<00:00, 52.17epoch/s, test_loss=17.3, train_loss=14.8]\n",
      "Epoch 179/1000: 100%|██████████| 1/1 [00:00<00:00, 228.05epoch/s, test_loss=17.3, train_loss=14.8]\n",
      "Epoch 180/1000: 100%|██████████| 1/1 [00:00<00:00, 34.77epoch/s, test_loss=17.3, train_loss=14.8]\n",
      "Epoch 181/1000: 100%|██████████| 1/1 [00:00<00:00, 232.01epoch/s, test_loss=17.3, train_loss=14.7]\n",
      "Epoch 182/1000: 100%|██████████| 1/1 [00:00<00:00, 53.71epoch/s, test_loss=17.3, train_loss=14.7]\n",
      "Epoch 183/1000: 100%|██████████| 1/1 [00:00<00:00, 63.10epoch/s, test_loss=17.3, train_loss=14.7]\n",
      "Epoch 184/1000: 100%|██████████| 1/1 [00:00<00:00, 227.16epoch/s, test_loss=17.3, train_loss=14.7]\n",
      "Epoch 185/1000: 100%|██████████| 1/1 [00:00<00:00, 235.69epoch/s, test_loss=17.3, train_loss=14.6]\n",
      "Epoch 186/1000: 100%|██████████| 1/1 [00:00<00:00, 116.44epoch/s, test_loss=17.2, train_loss=14.6]\n",
      "Epoch 187/1000: 100%|██████████| 1/1 [00:00<00:00, 234.92epoch/s, test_loss=17.2, train_loss=14.6]\n",
      "Epoch 188/1000: 100%|██████████| 1/1 [00:00<00:00, 217.47epoch/s, test_loss=17.2, train_loss=14.6]\n",
      "Epoch 189/1000: 100%|██████████| 1/1 [00:00<00:00, 218.93epoch/s, test_loss=17.2, train_loss=14.6]\n",
      "Epoch 190/1000: 100%|██████████| 1/1 [00:00<00:00, 225.71epoch/s, test_loss=17.2, train_loss=14.5]\n",
      "Epoch 191/1000: 100%|██████████| 1/1 [00:00<00:00, 124.65epoch/s, test_loss=17.2, train_loss=14.5]\n",
      "Epoch 192/1000: 100%|██████████| 1/1 [00:00<00:00, 170.30epoch/s, test_loss=17.2, train_loss=14.5]\n",
      "Epoch 193/1000: 100%|██████████| 1/1 [00:00<00:00, 108.34epoch/s, test_loss=17.2, train_loss=14.5]\n",
      "Epoch 194/1000: 100%|██████████| 1/1 [00:00<00:00, 176.92epoch/s, test_loss=17.2, train_loss=14.5]\n",
      "Epoch 195/1000: 100%|██████████| 1/1 [00:00<00:00, 269.76epoch/s, test_loss=17.2, train_loss=14.4]\n",
      "Epoch 196/1000: 100%|██████████| 1/1 [00:00<00:00, 179.56epoch/s, test_loss=17.2, train_loss=14.4]\n",
      "Epoch 197/1000: 100%|██████████| 1/1 [00:00<00:00, 106.14epoch/s, test_loss=17.1, train_loss=14.4]\n",
      "Epoch 198/1000: 100%|██████████| 1/1 [00:00<00:00, 199.53epoch/s, test_loss=17.1, train_loss=14.4]\n",
      "Epoch 199/1000: 100%|██████████| 1/1 [00:00<00:00, 108.30epoch/s, test_loss=17.1, train_loss=14.3]\n",
      "Epoch 200/1000: 100%|██████████| 1/1 [00:00<00:00, 178.15epoch/s, test_loss=17.1, train_loss=14.3]\n",
      "Epoch 201/1000: 100%|██████████| 1/1 [00:00<00:00, 147.45epoch/s, test_loss=17.1, train_loss=14.3]\n",
      "Epoch 202/1000: 100%|██████████| 1/1 [00:00<00:00, 148.73epoch/s, test_loss=17.1, train_loss=14.3]\n",
      "Epoch 203/1000: 100%|██████████| 1/1 [00:00<00:00, 130.23epoch/s, test_loss=17.1, train_loss=14.3]\n",
      "Epoch 204/1000: 100%|██████████| 1/1 [00:00<00:00, 241.72epoch/s, test_loss=17.1, train_loss=14.2]\n",
      "Epoch 205/1000: 100%|██████████| 1/1 [00:00<00:00, 175.44epoch/s, test_loss=17.1, train_loss=14.2]\n",
      "Epoch 206/1000: 100%|██████████| 1/1 [00:00<00:00, 118.32epoch/s, test_loss=17.1, train_loss=14.2]\n",
      "Epoch 207/1000: 100%|██████████| 1/1 [00:00<00:00, 109.90epoch/s, test_loss=17, train_loss=14.2]\n",
      "Epoch 208/1000: 100%|██████████| 1/1 [00:00<00:00, 127.33epoch/s, test_loss=17, train_loss=14.1]\n",
      "Epoch 209/1000: 100%|██████████| 1/1 [00:00<00:00, 67.07epoch/s, test_loss=17, train_loss=14.1]\n",
      "Epoch 210/1000: 100%|██████████| 1/1 [00:00<00:00, 224.68epoch/s, test_loss=17, train_loss=14.1]\n",
      "Epoch 211/1000: 100%|██████████| 1/1 [00:00<00:00, 161.78epoch/s, test_loss=17, train_loss=14.1]\n",
      "Epoch 212/1000: 100%|██████████| 1/1 [00:00<00:00, 33.37epoch/s, test_loss=17, train_loss=14.1]\n",
      "Epoch 213/1000: 100%|██████████| 1/1 [00:00<00:00, 114.91epoch/s, test_loss=17, train_loss=14]\n",
      "Epoch 214/1000: 100%|██████████| 1/1 [00:00<00:00, 77.83epoch/s, test_loss=17, train_loss=14]\n",
      "Epoch 215/1000: 100%|██████████| 1/1 [00:00<00:00, 116.21epoch/s, test_loss=17, train_loss=14]\n",
      "Epoch 216/1000: 100%|██████████| 1/1 [00:00<00:00, 242.35epoch/s, test_loss=16.9, train_loss=14]\n",
      "Epoch 217/1000: 100%|██████████| 1/1 [00:00<00:00, 294.36epoch/s, test_loss=16.9, train_loss=13.9]\n",
      "Epoch 218/1000: 100%|██████████| 1/1 [00:00<00:00, 178.26epoch/s, test_loss=16.9, train_loss=13.9]\n",
      "Epoch 219/1000: 100%|██████████| 1/1 [00:00<00:00, 78.55epoch/s, test_loss=16.9, train_loss=13.9]\n",
      "Epoch 220/1000: 100%|██████████| 1/1 [00:00<00:00, 85.60epoch/s, test_loss=16.9, train_loss=13.9]\n",
      "Epoch 221/1000: 100%|██████████| 1/1 [00:00<00:00, 60.66epoch/s, test_loss=16.9, train_loss=13.9]\n",
      "Epoch 222/1000: 100%|██████████| 1/1 [00:00<00:00, 101.17epoch/s, test_loss=16.9, train_loss=13.8]\n",
      "Epoch 223/1000: 100%|██████████| 1/1 [00:00<00:00, 107.70epoch/s, test_loss=16.9, train_loss=13.8]\n",
      "Epoch 224/1000: 100%|██████████| 1/1 [00:00<00:00, 167.24epoch/s, test_loss=16.9, train_loss=13.8]\n",
      "Epoch 225/1000: 100%|██████████| 1/1 [00:00<00:00, 244.94epoch/s, test_loss=16.8, train_loss=13.8]\n",
      "Epoch 226/1000: 100%|██████████| 1/1 [00:00<00:00, 99.37epoch/s, test_loss=16.8, train_loss=13.7]\n",
      "Epoch 227/1000: 100%|██████████| 1/1 [00:00<00:00, 115.86epoch/s, test_loss=16.8, train_loss=13.7]\n",
      "Epoch 228/1000: 100%|██████████| 1/1 [00:00<00:00, 104.54epoch/s, test_loss=16.8, train_loss=13.7]\n",
      "Epoch 229/1000: 100%|██████████| 1/1 [00:00<00:00, 71.10epoch/s, test_loss=16.8, train_loss=13.7]\n",
      "Epoch 230/1000: 100%|██████████| 1/1 [00:00<00:00, 109.21epoch/s, test_loss=16.8, train_loss=13.6]\n",
      "Epoch 231/1000: 100%|██████████| 1/1 [00:00<00:00, 73.93epoch/s, test_loss=16.8, train_loss=13.6]\n",
      "Epoch 232/1000: 100%|██████████| 1/1 [00:00<00:00, 237.30epoch/s, test_loss=16.8, train_loss=13.6]\n",
      "Epoch 233/1000: 100%|██████████| 1/1 [00:00<00:00, 241.30epoch/s, test_loss=16.7, train_loss=13.6]\n",
      "Epoch 234/1000: 100%|██████████| 1/1 [00:00<00:00, 250.50epoch/s, test_loss=16.7, train_loss=13.6]\n",
      "Epoch 235/1000: 100%|██████████| 1/1 [00:00<00:00, 170.20epoch/s, test_loss=16.7, train_loss=13.5]\n",
      "Epoch 236/1000: 100%|██████████| 1/1 [00:00<00:00, 250.30epoch/s, test_loss=16.7, train_loss=13.5]\n",
      "Epoch 237/1000: 100%|██████████| 1/1 [00:00<00:00, 217.46epoch/s, test_loss=16.7, train_loss=13.5]\n",
      "Epoch 238/1000: 100%|██████████| 1/1 [00:00<00:00, 232.59epoch/s, test_loss=16.7, train_loss=13.5]\n",
      "Epoch 239/1000: 100%|██████████| 1/1 [00:00<00:00, 96.72epoch/s, test_loss=16.7, train_loss=13.4]\n",
      "Epoch 240/1000: 100%|██████████| 1/1 [00:00<00:00, 151.93epoch/s, test_loss=16.6, train_loss=13.4]\n",
      "Epoch 241/1000: 100%|██████████| 1/1 [00:00<00:00, 136.67epoch/s, test_loss=16.6, train_loss=13.4]\n",
      "Epoch 242/1000: 100%|██████████| 1/1 [00:00<00:00, 31.71epoch/s, test_loss=16.6, train_loss=13.4]\n",
      "Epoch 243/1000: 100%|██████████| 1/1 [00:00<00:00, 116.37epoch/s, test_loss=16.6, train_loss=13.3]\n",
      "Epoch 244/1000: 100%|██████████| 1/1 [00:00<00:00, 119.22epoch/s, test_loss=16.6, train_loss=13.3]\n",
      "Epoch 245/1000: 100%|██████████| 1/1 [00:00<00:00, 136.29epoch/s, test_loss=16.6, train_loss=13.3]\n",
      "Epoch 246/1000: 100%|██████████| 1/1 [00:00<00:00, 74.60epoch/s, test_loss=16.6, train_loss=13.3]\n",
      "Epoch 247/1000: 100%|██████████| 1/1 [00:00<00:00, 29.31epoch/s, test_loss=16.5, train_loss=13.2]\n",
      "Epoch 248/1000: 100%|██████████| 1/1 [00:00<00:00, 75.83epoch/s, test_loss=16.5, train_loss=13.2]\n",
      "Epoch 249/1000: 100%|██████████| 1/1 [00:00<00:00, 92.55epoch/s, test_loss=16.5, train_loss=13.2]\n",
      "Epoch 250/1000: 100%|██████████| 1/1 [00:00<00:00, 32.17epoch/s, test_loss=16.5, train_loss=13.2]\n",
      "Epoch 251/1000: 100%|██████████| 1/1 [00:00<00:00, 69.56epoch/s, test_loss=16.5, train_loss=13.1]\n",
      "Epoch 252/1000: 100%|██████████| 1/1 [00:00<00:00, 223.18epoch/s, test_loss=16.5, train_loss=13.1]\n",
      "Epoch 253/1000: 100%|██████████| 1/1 [00:00<00:00, 309.70epoch/s, test_loss=16.5, train_loss=13.1]\n",
      "Epoch 254/1000: 100%|██████████| 1/1 [00:00<00:00, 84.31epoch/s, test_loss=16.4, train_loss=13.1]\n",
      "Epoch 255/1000: 100%|██████████| 1/1 [00:00<00:00, 533.49epoch/s, test_loss=16.4, train_loss=13]\n",
      "Epoch 256/1000: 100%|██████████| 1/1 [00:00<00:00, 81.88epoch/s, test_loss=16.4, train_loss=13]\n",
      "Epoch 257/1000: 100%|██████████| 1/1 [00:00<00:00, 165.12epoch/s, test_loss=16.4, train_loss=13]\n",
      "Epoch 258/1000: 100%|██████████| 1/1 [00:00<00:00, 105.79epoch/s, test_loss=16.4, train_loss=13]\n",
      "Epoch 259/1000: 100%|██████████| 1/1 [00:00<00:00, 429.61epoch/s, test_loss=16.4, train_loss=12.9]\n",
      "Epoch 260/1000: 100%|██████████| 1/1 [00:00<00:00, 215.71epoch/s, test_loss=16.4, train_loss=12.9]\n",
      "Epoch 261/1000: 100%|██████████| 1/1 [00:00<00:00, 174.31epoch/s, test_loss=16.3, train_loss=12.9]\n",
      "Epoch 262/1000: 100%|██████████| 1/1 [00:00<00:00, 199.52epoch/s, test_loss=16.3, train_loss=12.9]\n",
      "Epoch 263/1000: 100%|██████████| 1/1 [00:00<00:00, 183.74epoch/s, test_loss=16.3, train_loss=12.8]\n",
      "Epoch 264/1000: 100%|██████████| 1/1 [00:00<00:00, 161.18epoch/s, test_loss=16.3, train_loss=12.8]\n",
      "Epoch 265/1000: 100%|██████████| 1/1 [00:00<00:00, 265.82epoch/s, test_loss=16.3, train_loss=12.8]\n",
      "Epoch 266/1000: 100%|██████████| 1/1 [00:00<00:00, 238.22epoch/s, test_loss=16.3, train_loss=12.8]\n",
      "Epoch 267/1000: 100%|██████████| 1/1 [00:00<00:00, 186.41epoch/s, test_loss=16.2, train_loss=12.7]\n",
      "Epoch 268/1000: 100%|██████████| 1/1 [00:00<00:00, 160.27epoch/s, test_loss=16.2, train_loss=12.7]\n",
      "Epoch 269/1000: 100%|██████████| 1/1 [00:00<00:00, 203.70epoch/s, test_loss=16.2, train_loss=12.7]\n",
      "Epoch 270/1000: 100%|██████████| 1/1 [00:00<00:00, 227.52epoch/s, test_loss=16.2, train_loss=12.7]\n",
      "Epoch 271/1000: 100%|██████████| 1/1 [00:00<00:00, 346.67epoch/s, test_loss=16.2, train_loss=12.6]\n",
      "Epoch 272/1000: 100%|██████████| 1/1 [00:00<00:00, 252.93epoch/s, test_loss=16.2, train_loss=12.6]\n",
      "Epoch 273/1000: 100%|██████████| 1/1 [00:00<00:00, 261.21epoch/s, test_loss=16.2, train_loss=12.6]\n",
      "Epoch 274/1000: 100%|██████████| 1/1 [00:00<00:00, 280.93epoch/s, test_loss=16.1, train_loss=12.6]\n",
      "Epoch 275/1000: 100%|██████████| 1/1 [00:00<00:00, 237.77epoch/s, test_loss=16.1, train_loss=12.5]\n",
      "Epoch 276/1000: 100%|██████████| 1/1 [00:00<00:00, 191.15epoch/s, test_loss=16.1, train_loss=12.5]\n",
      "Epoch 277/1000: 100%|██████████| 1/1 [00:00<00:00, 411.61epoch/s, test_loss=16.1, train_loss=12.5]\n",
      "Epoch 278/1000: 100%|██████████| 1/1 [00:00<00:00, 74.27epoch/s, test_loss=16.1, train_loss=12.5]\n",
      "Epoch 279/1000: 100%|██████████| 1/1 [00:00<00:00, 199.49epoch/s, test_loss=16.1, train_loss=12.4]\n",
      "Epoch 280/1000: 100%|██████████| 1/1 [00:00<00:00, 286.59epoch/s, test_loss=16, train_loss=12.4]\n",
      "Epoch 281/1000: 100%|██████████| 1/1 [00:00<00:00, 140.50epoch/s, test_loss=16, train_loss=12.4]\n",
      "Epoch 282/1000: 100%|██████████| 1/1 [00:00<00:00, 290.52epoch/s, test_loss=16, train_loss=12.4]\n",
      "Epoch 283/1000: 100%|██████████| 1/1 [00:00<00:00, 149.36epoch/s, test_loss=16, train_loss=12.3]\n",
      "Epoch 284/1000: 100%|██████████| 1/1 [00:00<00:00, 104.73epoch/s, test_loss=16, train_loss=12.3]\n",
      "Epoch 285/1000: 100%|██████████| 1/1 [00:00<00:00, 262.75epoch/s, test_loss=16, train_loss=12.3]\n",
      "Epoch 286/1000: 100%|██████████| 1/1 [00:00<00:00, 488.68epoch/s, test_loss=15.9, train_loss=12.3]\n",
      "Epoch 287/1000: 100%|██████████| 1/1 [00:00<00:00, 256.42epoch/s, test_loss=15.9, train_loss=12.2]\n",
      "Epoch 288/1000: 100%|██████████| 1/1 [00:00<00:00, 110.65epoch/s, test_loss=15.9, train_loss=12.2]\n",
      "Epoch 289/1000: 100%|██████████| 1/1 [00:00<00:00, 242.31epoch/s, test_loss=15.9, train_loss=12.2]\n",
      "Epoch 290/1000: 100%|██████████| 1/1 [00:00<00:00, 253.51epoch/s, test_loss=15.9, train_loss=12.2]\n",
      "Epoch 291/1000: 100%|██████████| 1/1 [00:00<00:00, 239.22epoch/s, test_loss=15.9, train_loss=12.1]\n",
      "Epoch 292/1000: 100%|██████████| 1/1 [00:00<00:00, 165.85epoch/s, test_loss=15.8, train_loss=12.1]\n",
      "Epoch 293/1000: 100%|██████████| 1/1 [00:00<00:00, 142.41epoch/s, test_loss=15.8, train_loss=12.1]\n",
      "Epoch 294/1000: 100%|██████████| 1/1 [00:00<00:00, 272.39epoch/s, test_loss=15.8, train_loss=12.1]\n",
      "Epoch 295/1000: 100%|██████████| 1/1 [00:00<00:00, 178.67epoch/s, test_loss=15.8, train_loss=12]\n",
      "Epoch 296/1000: 100%|██████████| 1/1 [00:00<00:00, 341.86epoch/s, test_loss=15.8, train_loss=12]\n",
      "Epoch 297/1000: 100%|██████████| 1/1 [00:00<00:00, 156.67epoch/s, test_loss=15.8, train_loss=12]\n",
      "Epoch 298/1000: 100%|██████████| 1/1 [00:00<00:00, 153.38epoch/s, test_loss=15.7, train_loss=12]\n",
      "Epoch 299/1000: 100%|██████████| 1/1 [00:00<00:00, 213.15epoch/s, test_loss=15.7, train_loss=11.9]\n",
      "Epoch 300/1000: 100%|██████████| 1/1 [00:00<00:00, 371.54epoch/s, test_loss=15.7, train_loss=11.9]\n",
      "Epoch 301/1000: 100%|██████████| 1/1 [00:00<00:00, 95.40epoch/s, test_loss=15.7, train_loss=11.9]\n",
      "Epoch 302/1000: 100%|██████████| 1/1 [00:00<00:00, 380.37epoch/s, test_loss=15.7, train_loss=11.9]\n",
      "Epoch 303/1000: 100%|██████████| 1/1 [00:00<00:00, 259.47epoch/s, test_loss=15.7, train_loss=11.8]\n",
      "Epoch 304/1000: 100%|██████████| 1/1 [00:00<00:00, 195.41epoch/s, test_loss=15.6, train_loss=11.8]\n",
      "Epoch 305/1000: 100%|██████████| 1/1 [00:00<00:00, 173.78epoch/s, test_loss=15.6, train_loss=11.8]\n",
      "Epoch 306/1000: 100%|██████████| 1/1 [00:00<00:00, 260.71epoch/s, test_loss=15.6, train_loss=11.8]\n",
      "Epoch 307/1000: 100%|██████████| 1/1 [00:00<00:00, 252.68epoch/s, test_loss=15.6, train_loss=11.7]\n",
      "Epoch 308/1000: 100%|██████████| 1/1 [00:00<00:00, 229.94epoch/s, test_loss=15.6, train_loss=11.7]\n",
      "Epoch 309/1000: 100%|██████████| 1/1 [00:00<00:00, 243.12epoch/s, test_loss=15.6, train_loss=11.7]\n",
      "Epoch 310/1000: 100%|██████████| 1/1 [00:00<00:00, 225.48epoch/s, test_loss=15.5, train_loss=11.7]\n",
      "Epoch 311/1000: 100%|██████████| 1/1 [00:00<00:00, 241.72epoch/s, test_loss=15.5, train_loss=11.6]\n",
      "Epoch 312/1000: 100%|██████████| 1/1 [00:00<00:00, 141.53epoch/s, test_loss=15.5, train_loss=11.6]\n",
      "Epoch 313/1000: 100%|██████████| 1/1 [00:00<00:00, 216.73epoch/s, test_loss=15.5, train_loss=11.6]\n",
      "Epoch 314/1000: 100%|██████████| 1/1 [00:00<00:00, 267.84epoch/s, test_loss=15.5, train_loss=11.6]\n",
      "Epoch 315/1000: 100%|██████████| 1/1 [00:00<00:00, 377.36epoch/s, test_loss=15.5, train_loss=11.5]\n",
      "Epoch 316/1000: 100%|██████████| 1/1 [00:00<00:00, 190.71epoch/s, test_loss=15.4, train_loss=11.5]\n",
      "Epoch 317/1000: 100%|██████████| 1/1 [00:00<00:00, 202.86epoch/s, test_loss=15.4, train_loss=11.5]\n",
      "Epoch 318/1000: 100%|██████████| 1/1 [00:00<00:00, 145.51epoch/s, test_loss=15.4, train_loss=11.5]\n",
      "Epoch 319/1000: 100%|██████████| 1/1 [00:00<00:00, 372.60epoch/s, test_loss=15.4, train_loss=11.4]\n",
      "Epoch 320/1000: 100%|██████████| 1/1 [00:00<00:00, 217.10epoch/s, test_loss=15.4, train_loss=11.4]\n",
      "Epoch 321/1000: 100%|██████████| 1/1 [00:00<00:00, 173.61epoch/s, test_loss=15.4, train_loss=11.4]\n",
      "Epoch 322/1000: 100%|██████████| 1/1 [00:00<00:00, 234.80epoch/s, test_loss=15.3, train_loss=11.4]\n",
      "Epoch 323/1000: 100%|██████████| 1/1 [00:00<00:00, 230.37epoch/s, test_loss=15.3, train_loss=11.3]\n",
      "Epoch 324/1000: 100%|██████████| 1/1 [00:00<00:00, 149.14epoch/s, test_loss=15.3, train_loss=11.3]\n",
      "Epoch 325/1000: 100%|██████████| 1/1 [00:00<00:00, 245.48epoch/s, test_loss=15.3, train_loss=11.3]\n",
      "Epoch 326/1000: 100%|██████████| 1/1 [00:00<00:00, 278.06epoch/s, test_loss=15.3, train_loss=11.3]\n",
      "Epoch 327/1000: 100%|██████████| 1/1 [00:00<00:00, 232.42epoch/s, test_loss=15.3, train_loss=11.2]\n",
      "Epoch 328/1000: 100%|██████████| 1/1 [00:00<00:00, 226.39epoch/s, test_loss=15.2, train_loss=11.2]\n",
      "Epoch 329/1000: 100%|██████████| 1/1 [00:00<00:00, 187.40epoch/s, test_loss=15.2, train_loss=11.2]\n",
      "Epoch 330/1000: 100%|██████████| 1/1 [00:00<00:00, 225.65epoch/s, test_loss=15.2, train_loss=11.2]\n",
      "Epoch 331/1000: 100%|██████████| 1/1 [00:00<00:00, 156.69epoch/s, test_loss=15.2, train_loss=11.1]\n",
      "Epoch 332/1000: 100%|██████████| 1/1 [00:00<00:00, 242.03epoch/s, test_loss=15.2, train_loss=11.1]\n",
      "Epoch 333/1000: 100%|██████████| 1/1 [00:00<00:00, 360.96epoch/s, test_loss=15.2, train_loss=11.1]\n",
      "Epoch 334/1000: 100%|██████████| 1/1 [00:00<00:00, 194.78epoch/s, test_loss=15.2, train_loss=11.1]\n",
      "Epoch 335/1000: 100%|██████████| 1/1 [00:00<00:00, 245.41epoch/s, test_loss=15.1, train_loss=11]\n",
      "Epoch 336/1000: 100%|██████████| 1/1 [00:00<00:00, 264.49epoch/s, test_loss=15.1, train_loss=11]\n",
      "Epoch 337/1000: 100%|██████████| 1/1 [00:00<00:00, 176.25epoch/s, test_loss=15.1, train_loss=11]\n",
      "Epoch 338/1000: 100%|██████████| 1/1 [00:00<00:00, 198.52epoch/s, test_loss=15.1, train_loss=11]\n",
      "Epoch 339/1000: 100%|██████████| 1/1 [00:00<00:00, 251.94epoch/s, test_loss=15.1, train_loss=11]\n",
      "Epoch 340/1000: 100%|██████████| 1/1 [00:00<00:00, 98.54epoch/s, test_loss=15.1, train_loss=10.9]\n",
      "Epoch 341/1000: 100%|██████████| 1/1 [00:00<00:00, 202.08epoch/s, test_loss=15, train_loss=10.9]\n",
      "Epoch 342/1000: 100%|██████████| 1/1 [00:00<00:00, 173.25epoch/s, test_loss=15, train_loss=10.9]\n",
      "Epoch 343/1000: 100%|██████████| 1/1 [00:00<00:00, 402.87epoch/s, test_loss=15, train_loss=10.9]\n",
      "Epoch 344/1000: 100%|██████████| 1/1 [00:00<00:00, 183.04epoch/s, test_loss=15, train_loss=10.8]\n",
      "Epoch 345/1000: 100%|██████████| 1/1 [00:00<00:00, 202.56epoch/s, test_loss=15, train_loss=10.8]\n",
      "Epoch 346/1000: 100%|██████████| 1/1 [00:00<00:00, 210.06epoch/s, test_loss=15, train_loss=10.8]\n",
      "Epoch 347/1000: 100%|██████████| 1/1 [00:00<00:00, 130.43epoch/s, test_loss=15, train_loss=10.8]\n",
      "Epoch 348/1000: 100%|██████████| 1/1 [00:00<00:00, 246.64epoch/s, test_loss=14.9, train_loss=10.7]\n",
      "Epoch 349/1000: 100%|██████████| 1/1 [00:00<00:00, 136.77epoch/s, test_loss=14.9, train_loss=10.7]\n",
      "Epoch 350/1000: 100%|██████████| 1/1 [00:00<00:00, 219.88epoch/s, test_loss=14.9, train_loss=10.7]\n",
      "Epoch 351/1000: 100%|██████████| 1/1 [00:00<00:00, 228.66epoch/s, test_loss=14.9, train_loss=10.7]\n",
      "Epoch 352/1000: 100%|██████████| 1/1 [00:00<00:00, 222.70epoch/s, test_loss=14.9, train_loss=10.7]\n",
      "Epoch 353/1000: 100%|██████████| 1/1 [00:00<00:00, 262.98epoch/s, test_loss=14.9, train_loss=10.6]\n",
      "Epoch 354/1000: 100%|██████████| 1/1 [00:00<00:00, 230.18epoch/s, test_loss=14.9, train_loss=10.6]\n",
      "Epoch 355/1000: 100%|██████████| 1/1 [00:00<00:00, 215.27epoch/s, test_loss=14.8, train_loss=10.6]\n",
      "Epoch 356/1000: 100%|██████████| 1/1 [00:00<00:00, 233.24epoch/s, test_loss=14.8, train_loss=10.6]\n",
      "Epoch 357/1000: 100%|██████████| 1/1 [00:00<00:00, 232.76epoch/s, test_loss=14.8, train_loss=10.6]\n",
      "Epoch 358/1000: 100%|██████████| 1/1 [00:00<00:00, 202.83epoch/s, test_loss=14.8, train_loss=10.5]\n",
      "Epoch 359/1000: 100%|██████████| 1/1 [00:00<00:00, 253.36epoch/s, test_loss=14.8, train_loss=10.5]\n",
      "Epoch 360/1000: 100%|██████████| 1/1 [00:00<00:00, 240.94epoch/s, test_loss=14.8, train_loss=10.5]\n",
      "Epoch 361/1000: 100%|██████████| 1/1 [00:00<00:00, 155.38epoch/s, test_loss=14.8, train_loss=10.5]\n",
      "Epoch 362/1000: 100%|██████████| 1/1 [00:00<00:00, 161.46epoch/s, test_loss=14.7, train_loss=10.4]\n",
      "Epoch 363/1000: 100%|██████████| 1/1 [00:00<00:00, 226.44epoch/s, test_loss=14.7, train_loss=10.4]\n",
      "Epoch 364/1000: 100%|██████████| 1/1 [00:00<00:00, 251.86epoch/s, test_loss=14.7, train_loss=10.4]\n",
      "Epoch 365/1000: 100%|██████████| 1/1 [00:00<00:00, 236.86epoch/s, test_loss=14.7, train_loss=10.4]\n",
      "Epoch 366/1000: 100%|██████████| 1/1 [00:00<00:00, 236.77epoch/s, test_loss=14.7, train_loss=10.4]\n",
      "Epoch 367/1000: 100%|██████████| 1/1 [00:00<00:00, 135.73epoch/s, test_loss=14.7, train_loss=10.3]\n",
      "Epoch 368/1000: 100%|██████████| 1/1 [00:00<00:00, 236.38epoch/s, test_loss=14.7, train_loss=10.3]\n",
      "Epoch 369/1000: 100%|██████████| 1/1 [00:00<00:00, 232.22epoch/s, test_loss=14.7, train_loss=10.3]\n",
      "Epoch 370/1000: 100%|██████████| 1/1 [00:00<00:00, 230.62epoch/s, test_loss=14.6, train_loss=10.3]\n",
      "Epoch 371/1000: 100%|██████████| 1/1 [00:00<00:00, 169.40epoch/s, test_loss=14.6, train_loss=10.3]\n",
      "Epoch 372/1000: 100%|██████████| 1/1 [00:00<00:00, 286.24epoch/s, test_loss=14.6, train_loss=10.2]\n",
      "Epoch 373/1000: 100%|██████████| 1/1 [00:00<00:00, 244.94epoch/s, test_loss=14.6, train_loss=10.2]\n",
      "Epoch 374/1000: 100%|██████████| 1/1 [00:00<00:00, 168.41epoch/s, test_loss=14.6, train_loss=10.2]\n",
      "Epoch 375/1000: 100%|██████████| 1/1 [00:00<00:00, 247.74epoch/s, test_loss=14.6, train_loss=10.2]\n",
      "Epoch 376/1000: 100%|██████████| 1/1 [00:00<00:00, 236.78epoch/s, test_loss=14.6, train_loss=10.2]\n",
      "Epoch 377/1000: 100%|██████████| 1/1 [00:00<00:00, 237.06epoch/s, test_loss=14.6, train_loss=10.1]\n",
      "Epoch 378/1000: 100%|██████████| 1/1 [00:00<00:00, 247.32epoch/s, test_loss=14.6, train_loss=10.1]\n",
      "Epoch 379/1000: 100%|██████████| 1/1 [00:00<00:00, 150.57epoch/s, test_loss=14.5, train_loss=10.1]\n",
      "Epoch 380/1000: 100%|██████████| 1/1 [00:00<00:00, 138.58epoch/s, test_loss=14.5, train_loss=10.1]\n",
      "Epoch 381/1000: 100%|██████████| 1/1 [00:00<00:00, 428.73epoch/s, test_loss=14.5, train_loss=10.1]\n",
      "Epoch 382/1000: 100%|██████████| 1/1 [00:00<00:00, 248.58epoch/s, test_loss=14.5, train_loss=10]\n",
      "Epoch 383/1000: 100%|██████████| 1/1 [00:00<00:00, 226.49epoch/s, test_loss=14.5, train_loss=10]\n",
      "Epoch 384/1000: 100%|██████████| 1/1 [00:00<00:00, 238.43epoch/s, test_loss=14.5, train_loss=10]\n",
      "Epoch 385/1000: 100%|██████████| 1/1 [00:00<00:00, 248.05epoch/s, test_loss=14.5, train_loss=9.99]\n",
      "Epoch 386/1000: 100%|██████████| 1/1 [00:00<00:00, 248.01epoch/s, test_loss=14.5, train_loss=9.97]\n",
      "Epoch 387/1000: 100%|██████████| 1/1 [00:00<00:00, 237.72epoch/s, test_loss=14.4, train_loss=9.95]\n",
      "Epoch 388/1000: 100%|██████████| 1/1 [00:00<00:00, 241.16epoch/s, test_loss=14.4, train_loss=9.93]\n",
      "Epoch 389/1000: 100%|██████████| 1/1 [00:00<00:00, 235.30epoch/s, test_loss=14.4, train_loss=9.91]\n",
      "Epoch 390/1000: 100%|██████████| 1/1 [00:00<00:00, 233.26epoch/s, test_loss=14.4, train_loss=9.89]\n",
      "Epoch 391/1000: 100%|██████████| 1/1 [00:00<00:00, 243.39epoch/s, test_loss=14.4, train_loss=9.87]\n",
      "Epoch 392/1000: 100%|██████████| 1/1 [00:00<00:00, 251.77epoch/s, test_loss=14.4, train_loss=9.86]\n",
      "Epoch 393/1000: 100%|██████████| 1/1 [00:00<00:00, 246.09epoch/s, test_loss=14.4, train_loss=9.84]\n",
      "Epoch 394/1000: 100%|██████████| 1/1 [00:00<00:00, 242.01epoch/s, test_loss=14.4, train_loss=9.82]\n",
      "Epoch 395/1000: 100%|██████████| 1/1 [00:00<00:00, 236.62epoch/s, test_loss=14.4, train_loss=9.8]\n",
      "Epoch 396/1000: 100%|██████████| 1/1 [00:00<00:00, 229.98epoch/s, test_loss=14.4, train_loss=9.78]\n",
      "Epoch 397/1000: 100%|██████████| 1/1 [00:00<00:00, 147.94epoch/s, test_loss=14.3, train_loss=9.77]\n",
      "Epoch 398/1000: 100%|██████████| 1/1 [00:00<00:00, 259.15epoch/s, test_loss=14.3, train_loss=9.75]\n",
      "Epoch 399/1000: 100%|██████████| 1/1 [00:00<00:00, 244.14epoch/s, test_loss=14.3, train_loss=9.73]\n",
      "Epoch 400/1000: 100%|██████████| 1/1 [00:00<00:00, 236.09epoch/s, test_loss=14.3, train_loss=9.71]\n",
      "Epoch 401/1000: 100%|██████████| 1/1 [00:00<00:00, 227.60epoch/s, test_loss=14.3, train_loss=9.7]\n",
      "Epoch 402/1000: 100%|██████████| 1/1 [00:00<00:00, 218.44epoch/s, test_loss=14.3, train_loss=9.68]\n",
      "Epoch 403/1000: 100%|██████████| 1/1 [00:00<00:00, 219.13epoch/s, test_loss=14.3, train_loss=9.66]\n",
      "Epoch 404/1000: 100%|██████████| 1/1 [00:00<00:00, 176.01epoch/s, test_loss=14.3, train_loss=9.64]\n",
      "Epoch 405/1000: 100%|██████████| 1/1 [00:00<00:00, 140.80epoch/s, test_loss=14.3, train_loss=9.63]\n",
      "Epoch 406/1000: 100%|██████████| 1/1 [00:00<00:00, 125.77epoch/s, test_loss=14.3, train_loss=9.61]\n",
      "Epoch 407/1000: 100%|██████████| 1/1 [00:00<00:00, 211.00epoch/s, test_loss=14.3, train_loss=9.59]\n",
      "Epoch 408/1000: 100%|██████████| 1/1 [00:00<00:00, 271.90epoch/s, test_loss=14.2, train_loss=9.58]\n",
      "Epoch 409/1000: 100%|██████████| 1/1 [00:00<00:00, 229.71epoch/s, test_loss=14.2, train_loss=9.56]\n",
      "Epoch 410/1000: 100%|██████████| 1/1 [00:00<00:00, 227.44epoch/s, test_loss=14.2, train_loss=9.54]\n",
      "Epoch 411/1000: 100%|██████████| 1/1 [00:00<00:00, 206.44epoch/s, test_loss=14.2, train_loss=9.53]\n",
      "Epoch 412/1000: 100%|██████████| 1/1 [00:00<00:00, 252.29epoch/s, test_loss=14.2, train_loss=9.51]\n",
      "Epoch 413/1000: 100%|██████████| 1/1 [00:00<00:00, 206.78epoch/s, test_loss=14.2, train_loss=9.49]\n",
      "Epoch 414/1000: 100%|██████████| 1/1 [00:00<00:00, 213.79epoch/s, test_loss=14.2, train_loss=9.48]\n",
      "Epoch 415/1000: 100%|██████████| 1/1 [00:00<00:00, 196.08epoch/s, test_loss=14.2, train_loss=9.46]\n",
      "Epoch 416/1000: 100%|██████████| 1/1 [00:00<00:00, 194.06epoch/s, test_loss=14.2, train_loss=9.44]\n",
      "Epoch 417/1000: 100%|██████████| 1/1 [00:00<00:00, 209.48epoch/s, test_loss=14.2, train_loss=9.43]\n",
      "Epoch 418/1000: 100%|██████████| 1/1 [00:00<00:00, 193.75epoch/s, test_loss=14.2, train_loss=9.41]\n",
      "Epoch 419/1000: 100%|██████████| 1/1 [00:00<00:00, 195.93epoch/s, test_loss=14.1, train_loss=9.39]\n",
      "Epoch 420/1000: 100%|██████████| 1/1 [00:00<00:00, 220.00epoch/s, test_loss=14.1, train_loss=9.38]\n",
      "Epoch 421/1000: 100%|██████████| 1/1 [00:00<00:00, 200.79epoch/s, test_loss=14.1, train_loss=9.36]\n",
      "Epoch 422/1000: 100%|██████████| 1/1 [00:00<00:00, 151.69epoch/s, test_loss=14.1, train_loss=9.35]\n",
      "Epoch 423/1000: 100%|██████████| 1/1 [00:00<00:00, 250.66epoch/s, test_loss=14.1, train_loss=9.33]\n",
      "Epoch 424/1000: 100%|██████████| 1/1 [00:00<00:00, 239.16epoch/s, test_loss=14.1, train_loss=9.32]\n",
      "Epoch 425/1000: 100%|██████████| 1/1 [00:00<00:00, 244.28epoch/s, test_loss=14.1, train_loss=9.3]\n",
      "Epoch 426/1000: 100%|██████████| 1/1 [00:00<00:00, 246.04epoch/s, test_loss=14.1, train_loss=9.28]\n",
      "Epoch 427/1000: 100%|██████████| 1/1 [00:00<00:00, 250.95epoch/s, test_loss=14.1, train_loss=9.27]\n",
      "Epoch 428/1000: 100%|██████████| 1/1 [00:00<00:00, 214.24epoch/s, test_loss=14.1, train_loss=9.25]\n",
      "Epoch 429/1000: 100%|██████████| 1/1 [00:00<00:00, 233.85epoch/s, test_loss=14.1, train_loss=9.24]\n",
      "Epoch 430/1000: 100%|██████████| 1/1 [00:00<00:00, 243.97epoch/s, test_loss=14.1, train_loss=9.22]\n",
      "Epoch 431/1000: 100%|██████████| 1/1 [00:00<00:00, 237.17epoch/s, test_loss=14.1, train_loss=9.21]\n",
      "Epoch 432/1000: 100%|██████████| 1/1 [00:00<00:00, 239.51epoch/s, test_loss=14.1, train_loss=9.19]\n",
      "Epoch 433/1000: 100%|██████████| 1/1 [00:00<00:00, 235.74epoch/s, test_loss=14, train_loss=9.18]\n",
      "Epoch 434/1000: 100%|██████████| 1/1 [00:00<00:00, 149.07epoch/s, test_loss=14, train_loss=9.16]\n",
      "Epoch 435/1000: 100%|██████████| 1/1 [00:00<00:00, 243.57epoch/s, test_loss=14, train_loss=9.15]\n",
      "Epoch 436/1000: 100%|██████████| 1/1 [00:00<00:00, 243.29epoch/s, test_loss=14, train_loss=9.13]\n",
      "Epoch 437/1000: 100%|██████████| 1/1 [00:00<00:00, 264.06epoch/s, test_loss=14, train_loss=9.12]\n",
      "Epoch 438/1000: 100%|██████████| 1/1 [00:00<00:00, 237.42epoch/s, test_loss=14, train_loss=9.1]\n",
      "Epoch 439/1000: 100%|██████████| 1/1 [00:00<00:00, 254.69epoch/s, test_loss=14, train_loss=9.09]\n",
      "Epoch 440/1000: 100%|██████████| 1/1 [00:00<00:00, 171.75epoch/s, test_loss=14, train_loss=9.08]\n",
      "Epoch 441/1000: 100%|██████████| 1/1 [00:00<00:00, 203.20epoch/s, test_loss=14, train_loss=9.06]\n",
      "Epoch 442/1000: 100%|██████████| 1/1 [00:00<00:00, 218.37epoch/s, test_loss=14, train_loss=9.05]\n",
      "Epoch 443/1000: 100%|██████████| 1/1 [00:00<00:00, 103.32epoch/s, test_loss=14, train_loss=9.03]\n",
      "Epoch 444/1000: 100%|██████████| 1/1 [00:00<00:00, 215.10epoch/s, test_loss=14, train_loss=9.02]\n",
      "Epoch 445/1000: 100%|██████████| 1/1 [00:00<00:00, 234.23epoch/s, test_loss=14, train_loss=9]\n",
      "Epoch 446/1000: 100%|██████████| 1/1 [00:00<00:00, 184.98epoch/s, test_loss=14, train_loss=8.99]\n",
      "Epoch 447/1000: 100%|██████████| 1/1 [00:00<00:00, 255.25epoch/s, test_loss=14, train_loss=8.98]\n",
      "Epoch 448/1000: 100%|██████████| 1/1 [00:00<00:00, 251.43epoch/s, test_loss=14, train_loss=8.96]\n",
      "Epoch 449/1000: 100%|██████████| 1/1 [00:00<00:00, 216.07epoch/s, test_loss=13.9, train_loss=8.95]\n",
      "Epoch 450/1000: 100%|██████████| 1/1 [00:00<00:00, 216.40epoch/s, test_loss=13.9, train_loss=8.93]\n",
      "Epoch 451/1000: 100%|██████████| 1/1 [00:00<00:00, 241.22epoch/s, test_loss=13.9, train_loss=8.92]\n",
      "Epoch 452/1000: 100%|██████████| 1/1 [00:00<00:00, 250.83epoch/s, test_loss=13.9, train_loss=8.91]\n",
      "Epoch 453/1000: 100%|██████████| 1/1 [00:00<00:00, 254.35epoch/s, test_loss=13.9, train_loss=8.89]\n",
      "Epoch 454/1000: 100%|██████████| 1/1 [00:00<00:00, 227.85epoch/s, test_loss=13.9, train_loss=8.88]\n",
      "Epoch 455/1000: 100%|██████████| 1/1 [00:00<00:00, 272.87epoch/s, test_loss=13.9, train_loss=8.87]\n",
      "Epoch 456/1000: 100%|██████████| 1/1 [00:00<00:00, 243.83epoch/s, test_loss=13.9, train_loss=8.85]\n",
      "Epoch 457/1000: 100%|██████████| 1/1 [00:00<00:00, 257.38epoch/s, test_loss=13.9, train_loss=8.84]\n",
      "Epoch 458/1000: 100%|██████████| 1/1 [00:00<00:00, 262.31epoch/s, test_loss=13.9, train_loss=8.82]\n",
      "Epoch 459/1000: 100%|██████████| 1/1 [00:00<00:00, 251.52epoch/s, test_loss=13.9, train_loss=8.81]\n",
      "Epoch 460/1000: 100%|██████████| 1/1 [00:00<00:00, 241.40epoch/s, test_loss=13.9, train_loss=8.8]\n",
      "Epoch 461/1000: 100%|██████████| 1/1 [00:00<00:00, 242.21epoch/s, test_loss=13.9, train_loss=8.79]\n",
      "Epoch 462/1000: 100%|██████████| 1/1 [00:00<00:00, 251.55epoch/s, test_loss=13.9, train_loss=8.77]\n",
      "Epoch 463/1000: 100%|██████████| 1/1 [00:00<00:00, 226.69epoch/s, test_loss=13.9, train_loss=8.76]\n",
      "Epoch 464/1000: 100%|██████████| 1/1 [00:00<00:00, 252.55epoch/s, test_loss=13.9, train_loss=8.75]\n",
      "Epoch 465/1000: 100%|██████████| 1/1 [00:00<00:00, 247.57epoch/s, test_loss=13.9, train_loss=8.73]\n",
      "Epoch 466/1000: 100%|██████████| 1/1 [00:00<00:00, 252.76epoch/s, test_loss=13.9, train_loss=8.72]\n",
      "Epoch 467/1000: 100%|██████████| 1/1 [00:00<00:00, 155.26epoch/s, test_loss=13.9, train_loss=8.71]\n",
      "Epoch 468/1000: 100%|██████████| 1/1 [00:00<00:00, 153.38epoch/s, test_loss=13.8, train_loss=8.69]\n",
      "Epoch 469/1000: 100%|██████████| 1/1 [00:00<00:00, 149.69epoch/s, test_loss=13.8, train_loss=8.68]\n",
      "Epoch 470/1000: 100%|██████████| 1/1 [00:00<00:00, 272.75epoch/s, test_loss=13.8, train_loss=8.67]\n",
      "Epoch 471/1000: 100%|██████████| 1/1 [00:00<00:00, 249.51epoch/s, test_loss=13.8, train_loss=8.66]\n",
      "Epoch 472/1000: 100%|██████████| 1/1 [00:00<00:00, 268.52epoch/s, test_loss=13.8, train_loss=8.64]\n",
      "Epoch 473/1000: 100%|██████████| 1/1 [00:00<00:00, 253.00epoch/s, test_loss=13.8, train_loss=8.63]\n",
      "Epoch 474/1000: 100%|██████████| 1/1 [00:00<00:00, 265.09epoch/s, test_loss=13.8, train_loss=8.62]\n",
      "Epoch 475/1000: 100%|██████████| 1/1 [00:00<00:00, 262.93epoch/s, test_loss=13.8, train_loss=8.61]\n",
      "Epoch 476/1000: 100%|██████████| 1/1 [00:00<00:00, 248.42epoch/s, test_loss=13.8, train_loss=8.59]\n",
      "Epoch 477/1000: 100%|██████████| 1/1 [00:00<00:00, 267.41epoch/s, test_loss=13.8, train_loss=8.58]\n",
      "Epoch 478/1000: 100%|██████████| 1/1 [00:00<00:00, 260.71epoch/s, test_loss=13.8, train_loss=8.57]\n",
      "Epoch 479/1000: 100%|██████████| 1/1 [00:00<00:00, 171.73epoch/s, test_loss=13.8, train_loss=8.56]\n",
      "Epoch 480/1000: 100%|██████████| 1/1 [00:00<00:00, 262.98epoch/s, test_loss=13.8, train_loss=8.54]\n",
      "Epoch 481/1000: 100%|██████████| 1/1 [00:00<00:00, 247.28epoch/s, test_loss=13.8, train_loss=8.53]\n",
      "Epoch 482/1000: 100%|██████████| 1/1 [00:00<00:00, 141.95epoch/s, test_loss=13.8, train_loss=8.52]\n",
      "Epoch 483/1000: 100%|██████████| 1/1 [00:00<00:00, 222.16epoch/s, test_loss=13.8, train_loss=8.51]\n",
      "Epoch 484/1000: 100%|██████████| 1/1 [00:00<00:00, 244.11epoch/s, test_loss=13.8, train_loss=8.49]\n",
      "Epoch 485/1000: 100%|██████████| 1/1 [00:00<00:00, 253.86epoch/s, test_loss=13.8, train_loss=8.48]\n",
      "Epoch 486/1000: 100%|██████████| 1/1 [00:00<00:00, 247.99epoch/s, test_loss=13.8, train_loss=8.47]\n",
      "Epoch 487/1000: 100%|██████████| 1/1 [00:00<00:00, 245.63epoch/s, test_loss=13.8, train_loss=8.46]\n",
      "Epoch 488/1000: 100%|██████████| 1/1 [00:00<00:00, 262.52epoch/s, test_loss=13.8, train_loss=8.45]\n",
      "Epoch 489/1000: 100%|██████████| 1/1 [00:00<00:00, 269.87epoch/s, test_loss=13.8, train_loss=8.43]\n",
      "Epoch 490/1000: 100%|██████████| 1/1 [00:00<00:00, 251.56epoch/s, test_loss=13.8, train_loss=8.42]\n",
      "Epoch 491/1000: 100%|██████████| 1/1 [00:00<00:00, 260.48epoch/s, test_loss=13.8, train_loss=8.41]\n",
      "Epoch 492/1000: 100%|██████████| 1/1 [00:00<00:00, 259.16epoch/s, test_loss=13.8, train_loss=8.4]\n",
      "Epoch 493/1000: 100%|██████████| 1/1 [00:00<00:00, 251.85epoch/s, test_loss=13.7, train_loss=8.39]\n",
      "Epoch 494/1000: 100%|██████████| 1/1 [00:00<00:00, 275.25epoch/s, test_loss=13.7, train_loss=8.38]\n",
      "Epoch 495/1000: 100%|██████████| 1/1 [00:00<00:00, 168.16epoch/s, test_loss=13.7, train_loss=8.36]\n",
      "Epoch 496/1000: 100%|██████████| 1/1 [00:00<00:00, 255.88epoch/s, test_loss=13.7, train_loss=8.35]\n",
      "Epoch 497/1000: 100%|██████████| 1/1 [00:00<00:00, 252.43epoch/s, test_loss=13.7, train_loss=8.34]\n",
      "Epoch 498/1000: 100%|██████████| 1/1 [00:00<00:00, 243.59epoch/s, test_loss=13.7, train_loss=8.33]\n",
      "Epoch 499/1000: 100%|██████████| 1/1 [00:00<00:00, 244.38epoch/s, test_loss=13.7, train_loss=8.32]\n",
      "Epoch 500/1000: 100%|██████████| 1/1 [00:00<00:00, 242.12epoch/s, test_loss=13.7, train_loss=8.31]\n",
      "Epoch 501/1000: 100%|██████████| 1/1 [00:00<00:00, 137.40epoch/s, test_loss=13.7, train_loss=8.3]\n",
      "Epoch 502/1000: 100%|██████████| 1/1 [00:00<00:00, 261.16epoch/s, test_loss=13.7, train_loss=8.28]\n",
      "Epoch 503/1000: 100%|██████████| 1/1 [00:00<00:00, 230.94epoch/s, test_loss=13.7, train_loss=8.27]\n",
      "Epoch 504/1000: 100%|██████████| 1/1 [00:00<00:00, 239.70epoch/s, test_loss=13.7, train_loss=8.26]\n",
      "Epoch 505/1000: 100%|██████████| 1/1 [00:00<00:00, 251.82epoch/s, test_loss=13.7, train_loss=8.25]\n",
      "Epoch 506/1000: 100%|██████████| 1/1 [00:00<00:00, 256.78epoch/s, test_loss=13.7, train_loss=8.24]\n",
      "Epoch 507/1000: 100%|██████████| 1/1 [00:00<00:00, 261.52epoch/s, test_loss=13.7, train_loss=8.23]\n",
      "Epoch 508/1000: 100%|██████████| 1/1 [00:00<00:00, 141.92epoch/s, test_loss=13.7, train_loss=8.22]\n",
      "Epoch 509/1000: 100%|██████████| 1/1 [00:00<00:00, 259.34epoch/s, test_loss=13.7, train_loss=8.21]\n",
      "Epoch 510/1000: 100%|██████████| 1/1 [00:00<00:00, 243.29epoch/s, test_loss=13.7, train_loss=8.19]\n",
      "Epoch 511/1000: 100%|██████████| 1/1 [00:00<00:00, 256.00epoch/s, test_loss=13.7, train_loss=8.18]\n",
      "Epoch 512/1000: 100%|██████████| 1/1 [00:00<00:00, 109.47epoch/s, test_loss=13.7, train_loss=8.17]\n",
      "Epoch 513/1000: 100%|██████████| 1/1 [00:00<00:00, 262.41epoch/s, test_loss=13.7, train_loss=8.16]\n",
      "Epoch 514/1000: 100%|██████████| 1/1 [00:00<00:00, 251.01epoch/s, test_loss=13.7, train_loss=8.15]\n",
      "Epoch 515/1000: 100%|██████████| 1/1 [00:00<00:00, 248.55epoch/s, test_loss=13.7, train_loss=8.14]\n",
      "Epoch 516/1000: 100%|██████████| 1/1 [00:00<00:00, 260.66epoch/s, test_loss=13.7, train_loss=8.13]\n",
      "Epoch 517/1000: 100%|██████████| 1/1 [00:00<00:00, 239.44epoch/s, test_loss=13.7, train_loss=8.12]\n",
      "Epoch 518/1000: 100%|██████████| 1/1 [00:00<00:00, 251.16epoch/s, test_loss=13.7, train_loss=8.11]\n",
      "Epoch 519/1000: 100%|██████████| 1/1 [00:00<00:00, 280.46epoch/s, test_loss=13.7, train_loss=8.1]\n",
      "Epoch 520/1000: 100%|██████████| 1/1 [00:00<00:00, 262.80epoch/s, test_loss=13.7, train_loss=8.09]\n",
      "Epoch 521/1000: 100%|██████████| 1/1 [00:00<00:00, 253.66epoch/s, test_loss=13.7, train_loss=8.08]\n",
      "Epoch 522/1000: 100%|██████████| 1/1 [00:00<00:00, 300.69epoch/s, test_loss=13.7, train_loss=8.06]\n",
      "Epoch 523/1000: 100%|██████████| 1/1 [00:00<00:00, 244.39epoch/s, test_loss=13.7, train_loss=8.05]\n",
      "Epoch 524/1000: 100%|██████████| 1/1 [00:00<00:00, 247.36epoch/s, test_loss=13.7, train_loss=8.04]\n",
      "Epoch 525/1000: 100%|██████████| 1/1 [00:00<00:00, 115.60epoch/s, test_loss=13.7, train_loss=8.03]\n",
      "Epoch 526/1000: 100%|██████████| 1/1 [00:00<00:00, 234.78epoch/s, test_loss=13.7, train_loss=8.02]\n",
      "Epoch 527/1000: 100%|██████████| 1/1 [00:00<00:00, 252.84epoch/s, test_loss=13.6, train_loss=8.01]\n",
      "Epoch 528/1000: 100%|██████████| 1/1 [00:00<00:00, 266.59epoch/s, test_loss=13.6, train_loss=8]\n",
      "Epoch 529/1000: 100%|██████████| 1/1 [00:00<00:00, 246.94epoch/s, test_loss=13.6, train_loss=7.99]\n",
      "Epoch 530/1000: 100%|██████████| 1/1 [00:00<00:00, 87.79epoch/s, test_loss=13.6, train_loss=7.98]\n",
      "Epoch 531/1000: 100%|██████████| 1/1 [00:00<00:00, 252.11epoch/s, test_loss=13.6, train_loss=7.97]\n",
      "Epoch 532/1000: 100%|██████████| 1/1 [00:00<00:00, 120.57epoch/s, test_loss=13.6, train_loss=7.96]\n",
      "Epoch 533/1000: 100%|██████████| 1/1 [00:00<00:00, 206.81epoch/s, test_loss=13.6, train_loss=7.95]\n",
      "Epoch 534/1000: 100%|██████████| 1/1 [00:00<00:00, 240.04epoch/s, test_loss=13.6, train_loss=7.94]\n",
      "Epoch 535/1000: 100%|██████████| 1/1 [00:00<00:00, 230.66epoch/s, test_loss=13.6, train_loss=7.93]\n",
      "Epoch 536/1000: 100%|██████████| 1/1 [00:00<00:00, 194.69epoch/s, test_loss=13.6, train_loss=7.92]\n",
      "Epoch 537/1000: 100%|██████████| 1/1 [00:00<00:00, 239.37epoch/s, test_loss=13.6, train_loss=7.91]\n",
      "Epoch 538/1000: 100%|██████████| 1/1 [00:00<00:00, 167.13epoch/s, test_loss=13.6, train_loss=7.9]\n",
      "Epoch 539/1000: 100%|██████████| 1/1 [00:00<00:00, 232.46epoch/s, test_loss=13.6, train_loss=7.89]\n",
      "Epoch 540/1000: 100%|██████████| 1/1 [00:00<00:00, 158.63epoch/s, test_loss=13.6, train_loss=7.88]\n",
      "Epoch 541/1000: 100%|██████████| 1/1 [00:00<00:00, 230.36epoch/s, test_loss=13.6, train_loss=7.87]\n",
      "Epoch 542/1000: 100%|██████████| 1/1 [00:00<00:00, 235.91epoch/s, test_loss=13.6, train_loss=7.86]\n",
      "Epoch 543/1000: 100%|██████████| 1/1 [00:00<00:00, 258.14epoch/s, test_loss=13.6, train_loss=7.85]\n",
      "Epoch 544/1000: 100%|██████████| 1/1 [00:00<00:00, 240.02epoch/s, test_loss=13.6, train_loss=7.84]\n",
      "Epoch 545/1000: 100%|██████████| 1/1 [00:00<00:00, 240.02epoch/s, test_loss=13.6, train_loss=7.83]\n",
      "Epoch 546/1000: 100%|██████████| 1/1 [00:00<00:00, 242.17epoch/s, test_loss=13.6, train_loss=7.82]\n",
      "Epoch 547/1000: 100%|██████████| 1/1 [00:00<00:00, 230.63epoch/s, test_loss=13.6, train_loss=7.81]\n",
      "Epoch 548/1000: 100%|██████████| 1/1 [00:00<00:00, 270.44epoch/s, test_loss=13.6, train_loss=7.8]\n",
      "Epoch 549/1000: 100%|██████████| 1/1 [00:00<00:00, 248.11epoch/s, test_loss=13.6, train_loss=7.79]\n",
      "Epoch 550/1000: 100%|██████████| 1/1 [00:00<00:00, 253.36epoch/s, test_loss=13.6, train_loss=7.78]\n",
      "Epoch 551/1000: 100%|██████████| 1/1 [00:00<00:00, 240.31epoch/s, test_loss=13.6, train_loss=7.77]\n",
      "Epoch 552/1000: 100%|██████████| 1/1 [00:00<00:00, 238.50epoch/s, test_loss=13.6, train_loss=7.76]\n",
      "Epoch 553/1000: 100%|██████████| 1/1 [00:00<00:00, 251.41epoch/s, test_loss=13.6, train_loss=7.75]\n",
      "Epoch 554/1000: 100%|██████████| 1/1 [00:00<00:00, 244.41epoch/s, test_loss=13.6, train_loss=7.74]\n",
      "Epoch 555/1000: 100%|██████████| 1/1 [00:00<00:00, 238.25epoch/s, test_loss=13.6, train_loss=7.73]\n",
      "Epoch 556/1000: 100%|██████████| 1/1 [00:00<00:00, 192.89epoch/s, test_loss=13.6, train_loss=7.72]\n",
      "Epoch 557/1000: 100%|██████████| 1/1 [00:00<00:00, 263.84epoch/s, test_loss=13.6, train_loss=7.71]\n",
      "Epoch 558/1000: 100%|██████████| 1/1 [00:00<00:00, 251.64epoch/s, test_loss=13.6, train_loss=7.7]\n",
      "Epoch 559/1000: 100%|██████████| 1/1 [00:00<00:00, 217.42epoch/s, test_loss=13.6, train_loss=7.69]\n",
      "Epoch 560/1000: 100%|██████████| 1/1 [00:00<00:00, 236.78epoch/s, test_loss=13.6, train_loss=7.68]\n",
      "Epoch 561/1000: 100%|██████████| 1/1 [00:00<00:00, 236.49epoch/s, test_loss=13.6, train_loss=7.67]\n",
      "Epoch 562/1000: 100%|██████████| 1/1 [00:00<00:00, 248.58epoch/s, test_loss=13.6, train_loss=7.66]\n",
      "Epoch 563/1000: 100%|██████████| 1/1 [00:00<00:00, 138.45epoch/s, test_loss=13.6, train_loss=7.65]\n",
      "Epoch 564/1000: 100%|██████████| 1/1 [00:00<00:00, 164.51epoch/s, test_loss=13.6, train_loss=7.64]\n",
      "Epoch 565/1000: 100%|██████████| 1/1 [00:00<00:00, 165.07epoch/s, test_loss=13.6, train_loss=7.63]\n",
      "Epoch 566/1000: 100%|██████████| 1/1 [00:00<00:00, 215.50epoch/s, test_loss=13.6, train_loss=7.63]\n",
      "Epoch 567/1000: 100%|██████████| 1/1 [00:00<00:00, 210.44epoch/s, test_loss=13.6, train_loss=7.62]\n",
      "Epoch 568/1000: 100%|██████████| 1/1 [00:00<00:00, 183.53epoch/s, test_loss=13.6, train_loss=7.61]\n",
      "Epoch 569/1000: 100%|██████████| 1/1 [00:00<00:00, 193.15epoch/s, test_loss=13.6, train_loss=7.6]\n",
      "Epoch 570/1000: 100%|██████████| 1/1 [00:00<00:00, 210.43epoch/s, test_loss=13.6, train_loss=7.59]\n",
      "Epoch 571/1000: 100%|██████████| 1/1 [00:00<00:00, 193.89epoch/s, test_loss=13.6, train_loss=7.58]\n",
      "Epoch 572/1000: 100%|██████████| 1/1 [00:00<00:00, 415.07epoch/s, test_loss=13.6, train_loss=7.57]\n",
      "Epoch 573/1000: 100%|██████████| 1/1 [00:00<00:00, 147.12epoch/s, test_loss=13.6, train_loss=7.56]\n",
      "Epoch 574/1000: 100%|██████████| 1/1 [00:00<00:00, 203.93epoch/s, test_loss=13.6, train_loss=7.55]\n",
      "Epoch 575/1000: 100%|██████████| 1/1 [00:00<00:00, 238.68epoch/s, test_loss=13.6, train_loss=7.54]\n",
      "Epoch 576/1000: 100%|██████████| 1/1 [00:00<00:00, 150.38epoch/s, test_loss=13.6, train_loss=7.53]\n",
      "Epoch 577/1000: 100%|██████████| 1/1 [00:00<00:00, 244.01epoch/s, test_loss=13.6, train_loss=7.52]\n",
      "Epoch 578/1000: 100%|██████████| 1/1 [00:00<00:00, 251.97epoch/s, test_loss=13.6, train_loss=7.51]\n",
      "Epoch 579/1000: 100%|██████████| 1/1 [00:00<00:00, 103.91epoch/s, test_loss=13.6, train_loss=7.51]\n",
      "Epoch 580/1000: 100%|██████████| 1/1 [00:00<00:00, 243.32epoch/s, test_loss=13.6, train_loss=7.5]\n",
      "Epoch 581/1000: 100%|██████████| 1/1 [00:00<00:00, 237.18epoch/s, test_loss=13.6, train_loss=7.49]\n",
      "Epoch 582/1000: 100%|██████████| 1/1 [00:00<00:00, 231.23epoch/s, test_loss=13.6, train_loss=7.48]\n",
      "Epoch 583/1000: 100%|██████████| 1/1 [00:00<00:00, 277.00epoch/s, test_loss=13.6, train_loss=7.47]\n",
      "Epoch 584/1000: 100%|██████████| 1/1 [00:00<00:00, 241.79epoch/s, test_loss=13.6, train_loss=7.46]\n",
      "Epoch 585/1000: 100%|██████████| 1/1 [00:00<00:00, 210.58epoch/s, test_loss=13.6, train_loss=7.45]\n",
      "Epoch 586/1000: 100%|██████████| 1/1 [00:00<00:00, 154.97epoch/s, test_loss=13.6, train_loss=7.44]\n",
      "Epoch 587/1000: 100%|██████████| 1/1 [00:00<00:00, 251.40epoch/s, test_loss=13.6, train_loss=7.43]\n",
      "Epoch 588/1000: 100%|██████████| 1/1 [00:00<00:00, 247.00epoch/s, test_loss=13.5, train_loss=7.42]\n",
      "Epoch 589/1000: 100%|██████████| 1/1 [00:00<00:00, 164.84epoch/s, test_loss=13.5, train_loss=7.42]\n",
      "Epoch 590/1000: 100%|██████████| 1/1 [00:00<00:00, 240.49epoch/s, test_loss=13.5, train_loss=7.41]\n",
      "Epoch 591/1000: 100%|██████████| 1/1 [00:00<00:00, 237.81epoch/s, test_loss=13.5, train_loss=7.4]\n",
      "Epoch 592/1000: 100%|██████████| 1/1 [00:00<00:00, 236.94epoch/s, test_loss=13.5, train_loss=7.39]\n",
      "Epoch 593/1000: 100%|██████████| 1/1 [00:00<00:00, 167.97epoch/s, test_loss=13.5, train_loss=7.38]\n",
      "Epoch 594/1000: 100%|██████████| 1/1 [00:00<00:00, 226.60epoch/s, test_loss=13.5, train_loss=7.37]\n",
      "Epoch 595/1000: 100%|██████████| 1/1 [00:00<00:00, 217.73epoch/s, test_loss=13.5, train_loss=7.36]\n",
      "Epoch 596/1000: 100%|██████████| 1/1 [00:00<00:00, 161.80epoch/s, test_loss=13.5, train_loss=7.35]\n",
      "Epoch 597/1000: 100%|██████████| 1/1 [00:00<00:00, 253.69epoch/s, test_loss=13.5, train_loss=7.34]\n",
      "Epoch 598/1000: 100%|██████████| 1/1 [00:00<00:00, 243.91epoch/s, test_loss=13.5, train_loss=7.34]\n",
      "Epoch 599/1000: 100%|██████████| 1/1 [00:00<00:00, 239.63epoch/s, test_loss=13.5, train_loss=7.33]\n",
      "Epoch 600/1000: 100%|██████████| 1/1 [00:00<00:00, 245.37epoch/s, test_loss=13.5, train_loss=7.32]\n",
      "Epoch 601/1000: 100%|██████████| 1/1 [00:00<00:00, 233.59epoch/s, test_loss=13.5, train_loss=7.31]\n",
      "Epoch 602/1000: 100%|██████████| 1/1 [00:00<00:00, 245.81epoch/s, test_loss=13.5, train_loss=7.3]\n",
      "Epoch 603/1000: 100%|██████████| 1/1 [00:00<00:00, 189.79epoch/s, test_loss=13.5, train_loss=7.29]\n",
      "Epoch 604/1000: 100%|██████████| 1/1 [00:00<00:00, 203.86epoch/s, test_loss=13.5, train_loss=7.28]\n",
      "Epoch 605/1000: 100%|██████████| 1/1 [00:00<00:00, 202.47epoch/s, test_loss=13.5, train_loss=7.28]\n",
      "Epoch 606/1000: 100%|██████████| 1/1 [00:00<00:00, 236.74epoch/s, test_loss=13.5, train_loss=7.27]\n",
      "Epoch 607/1000: 100%|██████████| 1/1 [00:00<00:00, 231.21epoch/s, test_loss=13.5, train_loss=7.26]\n",
      "Epoch 608/1000: 100%|██████████| 1/1 [00:00<00:00, 250.57epoch/s, test_loss=13.5, train_loss=7.25]\n",
      "Epoch 609/1000: 100%|██████████| 1/1 [00:00<00:00, 143.56epoch/s, test_loss=13.5, train_loss=7.24]\n",
      "Epoch 610/1000: 100%|██████████| 1/1 [00:00<00:00, 145.38epoch/s, test_loss=13.5, train_loss=7.23]\n",
      "Epoch 611/1000: 100%|██████████| 1/1 [00:00<00:00, 149.26epoch/s, test_loss=13.5, train_loss=7.22]\n",
      "Epoch 612/1000: 100%|██████████| 1/1 [00:00<00:00, 240.11epoch/s, test_loss=13.5, train_loss=7.22]\n",
      "Epoch 613/1000: 100%|██████████| 1/1 [00:00<00:00, 242.96epoch/s, test_loss=13.5, train_loss=7.21]\n",
      "Epoch 614/1000: 100%|██████████| 1/1 [00:00<00:00, 250.26epoch/s, test_loss=13.5, train_loss=7.2]\n",
      "Epoch 615/1000: 100%|██████████| 1/1 [00:00<00:00, 165.19epoch/s, test_loss=13.5, train_loss=7.19]\n",
      "Epoch 616/1000: 100%|██████████| 1/1 [00:00<00:00, 242.88epoch/s, test_loss=13.5, train_loss=7.18]\n",
      "Epoch 617/1000: 100%|██████████| 1/1 [00:00<00:00, 235.01epoch/s, test_loss=13.5, train_loss=7.17]\n",
      "Epoch 618/1000: 100%|██████████| 1/1 [00:00<00:00, 214.21epoch/s, test_loss=13.5, train_loss=7.17]\n",
      "Epoch 619/1000: 100%|██████████| 1/1 [00:00<00:00, 97.74epoch/s, test_loss=13.5, train_loss=7.16]\n",
      "Epoch 620/1000: 100%|██████████| 1/1 [00:00<00:00, 231.58epoch/s, test_loss=13.5, train_loss=7.15]\n",
      "Epoch 621/1000: 100%|██████████| 1/1 [00:00<00:00, 116.05epoch/s, test_loss=13.5, train_loss=7.14]\n",
      "Epoch 622/1000: 100%|██████████| 1/1 [00:00<00:00, 234.32epoch/s, test_loss=13.5, train_loss=7.13]\n",
      "Epoch 623/1000: 100%|██████████| 1/1 [00:00<00:00, 146.69epoch/s, test_loss=13.5, train_loss=7.12]\n",
      "Epoch 624/1000: 100%|██████████| 1/1 [00:00<00:00, 232.56epoch/s, test_loss=13.5, train_loss=7.12]\n",
      "Epoch 625/1000: 100%|██████████| 1/1 [00:00<00:00, 149.98epoch/s, test_loss=13.5, train_loss=7.11]\n",
      "Epoch 626/1000: 100%|██████████| 1/1 [00:00<00:00, 248.29epoch/s, test_loss=13.5, train_loss=7.1]\n",
      "Epoch 627/1000: 100%|██████████| 1/1 [00:00<00:00, 242.70epoch/s, test_loss=13.5, train_loss=7.09]\n",
      "Epoch 628/1000: 100%|██████████| 1/1 [00:00<00:00, 235.20epoch/s, test_loss=13.5, train_loss=7.08]\n",
      "Epoch 629/1000: 100%|██████████| 1/1 [00:00<00:00, 225.68epoch/s, test_loss=13.5, train_loss=7.08]\n",
      "Epoch 630/1000: 100%|██████████| 1/1 [00:00<00:00, 216.66epoch/s, test_loss=13.5, train_loss=7.07]\n",
      "Epoch 631/1000: 100%|██████████| 1/1 [00:00<00:00, 216.80epoch/s, test_loss=13.5, train_loss=7.06]\n",
      "Epoch 632/1000: 100%|██████████| 1/1 [00:00<00:00, 247.70epoch/s, test_loss=13.5, train_loss=7.05]\n",
      "Epoch 633/1000: 100%|██████████| 1/1 [00:00<00:00, 228.95epoch/s, test_loss=13.5, train_loss=7.04]\n",
      "Epoch 634/1000: 100%|██████████| 1/1 [00:00<00:00, 237.02epoch/s, test_loss=13.5, train_loss=7.03]\n",
      "Epoch 635/1000: 100%|██████████| 1/1 [00:00<00:00, 158.11epoch/s, test_loss=13.5, train_loss=7.03]\n",
      "Epoch 636/1000: 100%|██████████| 1/1 [00:00<00:00, 244.57epoch/s, test_loss=13.5, train_loss=7.02]\n",
      "Epoch 637/1000: 100%|██████████| 1/1 [00:00<00:00, 246.30epoch/s, test_loss=13.5, train_loss=7.01]\n",
      "Epoch 638/1000: 100%|██████████| 1/1 [00:00<00:00, 239.48epoch/s, test_loss=13.5, train_loss=7]\n",
      "Epoch 639/1000: 100%|██████████| 1/1 [00:00<00:00, 249.47epoch/s, test_loss=13.5, train_loss=7]\n",
      "Epoch 640/1000: 100%|██████████| 1/1 [00:00<00:00, 234.79epoch/s, test_loss=13.5, train_loss=6.99]\n",
      "Epoch 641/1000: 100%|██████████| 1/1 [00:00<00:00, 252.88epoch/s, test_loss=13.5, train_loss=6.98]\n",
      "Epoch 642/1000: 100%|██████████| 1/1 [00:00<00:00, 249.20epoch/s, test_loss=13.5, train_loss=6.97]\n",
      "Epoch 643/1000: 100%|██████████| 1/1 [00:00<00:00, 259.47epoch/s, test_loss=13.5, train_loss=6.96]\n",
      "Epoch 644/1000: 100%|██████████| 1/1 [00:00<00:00, 260.60epoch/s, test_loss=13.5, train_loss=6.96]\n",
      "Epoch 645/1000: 100%|██████████| 1/1 [00:00<00:00, 240.20epoch/s, test_loss=13.5, train_loss=6.95]\n",
      "Epoch 646/1000: 100%|██████████| 1/1 [00:00<00:00, 240.93epoch/s, test_loss=13.5, train_loss=6.94]\n",
      "Epoch 647/1000: 100%|██████████| 1/1 [00:00<00:00, 254.80epoch/s, test_loss=13.5, train_loss=6.93]\n",
      "Epoch 648/1000: 100%|██████████| 1/1 [00:00<00:00, 229.01epoch/s, test_loss=13.5, train_loss=6.92]\n",
      "Epoch 649/1000: 100%|██████████| 1/1 [00:00<00:00, 149.33epoch/s, test_loss=13.5, train_loss=6.92]\n",
      "Epoch 650/1000: 100%|██████████| 1/1 [00:00<00:00, 227.42epoch/s, test_loss=13.5, train_loss=6.91]\n",
      "Epoch 651/1000: 100%|██████████| 1/1 [00:00<00:00, 249.47epoch/s, test_loss=13.5, train_loss=6.9]\n",
      "Epoch 652/1000: 100%|██████████| 1/1 [00:00<00:00, 247.10epoch/s, test_loss=13.5, train_loss=6.89]\n",
      "Epoch 653/1000: 100%|██████████| 1/1 [00:00<00:00, 248.18epoch/s, test_loss=13.5, train_loss=6.89]\n",
      "Epoch 654/1000: 100%|██████████| 1/1 [00:00<00:00, 254.28epoch/s, test_loss=13.5, train_loss=6.88]\n",
      "Epoch 655/1000: 100%|██████████| 1/1 [00:00<00:00, 252.84epoch/s, test_loss=13.5, train_loss=6.87]\n",
      "Epoch 656/1000: 100%|██████████| 1/1 [00:00<00:00, 242.25epoch/s, test_loss=13.5, train_loss=6.86]\n",
      "Epoch 657/1000: 100%|██████████| 1/1 [00:00<00:00, 259.31epoch/s, test_loss=13.5, train_loss=6.85]\n",
      "Epoch 658/1000: 100%|██████████| 1/1 [00:00<00:00, 244.81epoch/s, test_loss=13.5, train_loss=6.85]\n",
      "Epoch 659/1000: 100%|██████████| 1/1 [00:00<00:00, 226.34epoch/s, test_loss=13.5, train_loss=6.84]\n",
      "Epoch 660/1000: 100%|██████████| 1/1 [00:00<00:00, 221.18epoch/s, test_loss=13.5, train_loss=6.83]\n",
      "Epoch 661/1000: 100%|██████████| 1/1 [00:00<00:00, 177.66epoch/s, test_loss=13.5, train_loss=6.82]\n",
      "Epoch 662/1000: 100%|██████████| 1/1 [00:00<00:00, 318.23epoch/s, test_loss=13.5, train_loss=6.82]\n",
      "Epoch 663/1000: 100%|██████████| 1/1 [00:00<00:00, 204.84epoch/s, test_loss=13.5, train_loss=6.81]\n",
      "Epoch 664/1000: 100%|██████████| 1/1 [00:00<00:00, 220.31epoch/s, test_loss=13.5, train_loss=6.8]\n",
      "Epoch 665/1000: 100%|██████████| 1/1 [00:00<00:00, 163.66epoch/s, test_loss=13.5, train_loss=6.79]\n",
      "Epoch 666/1000: 100%|██████████| 1/1 [00:00<00:00, 256.14epoch/s, test_loss=13.5, train_loss=6.79]\n",
      "Epoch 667/1000: 100%|██████████| 1/1 [00:00<00:00, 244.39epoch/s, test_loss=13.5, train_loss=6.78]\n",
      "Epoch 668/1000: 100%|██████████| 1/1 [00:00<00:00, 226.25epoch/s, test_loss=13.5, train_loss=6.77]\n",
      "Epoch 669/1000: 100%|██████████| 1/1 [00:00<00:00, 250.35epoch/s, test_loss=13.5, train_loss=6.76]\n",
      "Epoch 670/1000: 100%|██████████| 1/1 [00:00<00:00, 120.86epoch/s, test_loss=13.5, train_loss=6.76]\n",
      "Epoch 671/1000: 100%|██████████| 1/1 [00:00<00:00, 226.79epoch/s, test_loss=13.5, train_loss=6.75]\n",
      "Epoch 672/1000: 100%|██████████| 1/1 [00:00<00:00, 239.47epoch/s, test_loss=13.5, train_loss=6.74]\n",
      "Epoch 673/1000: 100%|██████████| 1/1 [00:00<00:00, 242.39epoch/s, test_loss=13.5, train_loss=6.73]\n",
      "Epoch 674/1000: 100%|██████████| 1/1 [00:00<00:00, 246.20epoch/s, test_loss=13.5, train_loss=6.73]\n",
      "Epoch 675/1000: 100%|██████████| 1/1 [00:00<00:00, 244.94epoch/s, test_loss=13.5, train_loss=6.72]\n",
      "Epoch 676/1000: 100%|██████████| 1/1 [00:00<00:00, 247.44epoch/s, test_loss=13.5, train_loss=6.71]\n",
      "Epoch 677/1000: 100%|██████████| 1/1 [00:00<00:00, 241.96epoch/s, test_loss=13.5, train_loss=6.71]\n",
      "Epoch 678/1000: 100%|██████████| 1/1 [00:00<00:00, 249.72epoch/s, test_loss=13.5, train_loss=6.7]\n",
      "Epoch 679/1000: 100%|██████████| 1/1 [00:00<00:00, 243.36epoch/s, test_loss=13.5, train_loss=6.69]\n",
      "Epoch 680/1000: 100%|██████████| 1/1 [00:00<00:00, 247.39epoch/s, test_loss=13.5, train_loss=6.68]\n",
      "Epoch 681/1000: 100%|██████████| 1/1 [00:00<00:00, 272.69epoch/s, test_loss=13.5, train_loss=6.68]\n",
      "Epoch 682/1000: 100%|██████████| 1/1 [00:00<00:00, 244.52epoch/s, test_loss=13.5, train_loss=6.67]\n",
      "Epoch 683/1000: 100%|██████████| 1/1 [00:00<00:00, 259.13epoch/s, test_loss=13.5, train_loss=6.66]\n",
      "Epoch 684/1000: 100%|██████████| 1/1 [00:00<00:00, 230.65epoch/s, test_loss=13.5, train_loss=6.65]\n",
      "Epoch 685/1000: 100%|██████████| 1/1 [00:00<00:00, 138.15epoch/s, test_loss=13.5, train_loss=6.65]\n",
      "Epoch 686/1000: 100%|██████████| 1/1 [00:00<00:00, 257.21epoch/s, test_loss=13.5, train_loss=6.64]\n",
      "Epoch 687/1000: 100%|██████████| 1/1 [00:00<00:00, 263.46epoch/s, test_loss=13.5, train_loss=6.63]\n",
      "Epoch 688/1000: 100%|██████████| 1/1 [00:00<00:00, 245.77epoch/s, test_loss=13.5, train_loss=6.63]\n",
      "Epoch 689/1000: 100%|██████████| 1/1 [00:00<00:00, 259.13epoch/s, test_loss=13.5, train_loss=6.62]\n",
      "Epoch 690/1000: 100%|██████████| 1/1 [00:00<00:00, 144.14epoch/s, test_loss=13.5, train_loss=6.61]\n",
      "Epoch 691/1000: 100%|██████████| 1/1 [00:00<00:00, 159.80epoch/s, test_loss=13.5, train_loss=6.6]\n",
      "Epoch 692/1000: 100%|██████████| 1/1 [00:00<00:00, 179.17epoch/s, test_loss=13.5, train_loss=6.6]\n",
      "Epoch 693/1000: 100%|██████████| 1/1 [00:00<00:00, 254.03epoch/s, test_loss=13.5, train_loss=6.59]\n",
      "Epoch 694/1000: 100%|██████████| 1/1 [00:00<00:00, 264.31epoch/s, test_loss=13.5, train_loss=6.58]\n",
      "Epoch 695/1000: 100%|██████████| 1/1 [00:00<00:00, 188.20epoch/s, test_loss=13.5, train_loss=6.58]\n",
      "Epoch 696/1000: 100%|██████████| 1/1 [00:00<00:00, 250.50epoch/s, test_loss=13.5, train_loss=6.57]\n",
      "Epoch 697/1000: 100%|██████████| 1/1 [00:00<00:00, 236.79epoch/s, test_loss=13.5, train_loss=6.56]\n",
      "Epoch 698/1000: 100%|██████████| 1/1 [00:00<00:00, 252.55epoch/s, test_loss=13.5, train_loss=6.56]\n",
      "Epoch 699/1000: 100%|██████████| 1/1 [00:00<00:00, 254.49epoch/s, test_loss=13.5, train_loss=6.55]\n",
      "Epoch 700/1000: 100%|██████████| 1/1 [00:00<00:00, 261.31epoch/s, test_loss=13.5, train_loss=6.54]\n",
      "Epoch 701/1000: 100%|██████████| 1/1 [00:00<00:00, 250.48epoch/s, test_loss=13.5, train_loss=6.53]\n",
      "Epoch 702/1000: 100%|██████████| 1/1 [00:00<00:00, 243.49epoch/s, test_loss=13.5, train_loss=6.53]\n",
      "Epoch 703/1000: 100%|██████████| 1/1 [00:00<00:00, 243.94epoch/s, test_loss=13.5, train_loss=6.52]\n",
      "Epoch 704/1000: 100%|██████████| 1/1 [00:00<00:00, 251.35epoch/s, test_loss=13.5, train_loss=6.51]\n",
      "Epoch 705/1000: 100%|██████████| 1/1 [00:00<00:00, 202.45epoch/s, test_loss=13.5, train_loss=6.51]\n",
      "Epoch 706/1000: 100%|██████████| 1/1 [00:00<00:00, 134.77epoch/s, test_loss=13.5, train_loss=6.5]\n",
      "Epoch 707/1000: 100%|██████████| 1/1 [00:00<00:00, 168.21epoch/s, test_loss=13.5, train_loss=6.49]\n",
      "Epoch 708/1000: 100%|██████████| 1/1 [00:00<00:00, 276.80epoch/s, test_loss=13.5, train_loss=6.49]\n",
      "Epoch 709/1000: 100%|██████████| 1/1 [00:00<00:00, 150.69epoch/s, test_loss=13.5, train_loss=6.48]\n",
      "Epoch 710/1000: 100%|██████████| 1/1 [00:00<00:00, 250.74epoch/s, test_loss=13.5, train_loss=6.47]\n",
      "Epoch 711/1000: 100%|██████████| 1/1 [00:00<00:00, 250.39epoch/s, test_loss=13.5, train_loss=6.47]\n",
      "Epoch 712/1000: 100%|██████████| 1/1 [00:00<00:00, 237.27epoch/s, test_loss=13.5, train_loss=6.46]\n",
      "Epoch 713/1000: 100%|██████████| 1/1 [00:00<00:00, 179.05epoch/s, test_loss=13.5, train_loss=6.45]\n",
      "Epoch 714/1000: 100%|██████████| 1/1 [00:00<00:00, 250.96epoch/s, test_loss=13.5, train_loss=6.45]\n",
      "Epoch 715/1000: 100%|██████████| 1/1 [00:00<00:00, 248.12epoch/s, test_loss=13.5, train_loss=6.44]\n",
      "Epoch 716/1000: 100%|██████████| 1/1 [00:00<00:00, 248.29epoch/s, test_loss=13.5, train_loss=6.43]\n",
      "Epoch 717/1000: 100%|██████████| 1/1 [00:00<00:00, 239.87epoch/s, test_loss=13.5, train_loss=6.42]\n",
      "Epoch 718/1000: 100%|██████████| 1/1 [00:00<00:00, 250.92epoch/s, test_loss=13.5, train_loss=6.42]\n",
      "Epoch 719/1000: 100%|██████████| 1/1 [00:00<00:00, 271.25epoch/s, test_loss=13.5, train_loss=6.41]\n",
      "Epoch 720/1000: 100%|██████████| 1/1 [00:00<00:00, 250.47epoch/s, test_loss=13.5, train_loss=6.4]\n",
      "Epoch 721/1000: 100%|██████████| 1/1 [00:00<00:00, 242.31epoch/s, test_loss=13.5, train_loss=6.4]\n",
      "Epoch 722/1000: 100%|██████████| 1/1 [00:00<00:00, 259.34epoch/s, test_loss=13.5, train_loss=6.39]\n",
      "Epoch 723/1000: 100%|██████████| 1/1 [00:00<00:00, 234.31epoch/s, test_loss=13.5, train_loss=6.38]\n",
      "Epoch 724/1000: 100%|██████████| 1/1 [00:00<00:00, 234.50epoch/s, test_loss=13.5, train_loss=6.38]\n",
      "Epoch 725/1000: 100%|██████████| 1/1 [00:00<00:00, 242.46epoch/s, test_loss=13.5, train_loss=6.37]\n",
      "Epoch 726/1000: 100%|██████████| 1/1 [00:00<00:00, 255.44epoch/s, test_loss=13.5, train_loss=6.37]\n",
      "Epoch 727/1000: 100%|██████████| 1/1 [00:00<00:00, 268.30epoch/s, test_loss=13.5, train_loss=6.36]\n",
      "Epoch 728/1000: 100%|██████████| 1/1 [00:00<00:00, 248.86epoch/s, test_loss=13.5, train_loss=6.35]\n",
      "Epoch 729/1000: 100%|██████████| 1/1 [00:00<00:00, 267.29epoch/s, test_loss=13.5, train_loss=6.35]\n",
      "Epoch 730/1000: 100%|██████████| 1/1 [00:00<00:00, 243.35epoch/s, test_loss=13.5, train_loss=6.34]\n",
      "Epoch 731/1000: 100%|██████████| 1/1 [00:00<00:00, 245.97epoch/s, test_loss=13.5, train_loss=6.33]\n",
      "Epoch 732/1000: 100%|██████████| 1/1 [00:00<00:00, 240.29epoch/s, test_loss=13.5, train_loss=6.33]\n",
      "Epoch 733/1000: 100%|██████████| 1/1 [00:00<00:00, 262.72epoch/s, test_loss=13.5, train_loss=6.32]\n",
      "Epoch 734/1000: 100%|██████████| 1/1 [00:00<00:00, 239.70epoch/s, test_loss=13.5, train_loss=6.31]\n",
      "Epoch 735/1000: 100%|██████████| 1/1 [00:00<00:00, 244.44epoch/s, test_loss=13.5, train_loss=6.31]\n",
      "Epoch 736/1000: 100%|██████████| 1/1 [00:00<00:00, 236.89epoch/s, test_loss=13.5, train_loss=6.3]\n",
      "Epoch 737/1000: 100%|██████████| 1/1 [00:00<00:00, 247.90epoch/s, test_loss=13.5, train_loss=6.29]\n",
      "Epoch 738/1000: 100%|██████████| 1/1 [00:00<00:00, 236.22epoch/s, test_loss=13.5, train_loss=6.29]\n",
      "Epoch 739/1000: 100%|██████████| 1/1 [00:00<00:00, 272.20epoch/s, test_loss=13.5, train_loss=6.28]\n",
      "Epoch 740/1000: 100%|██████████| 1/1 [00:00<00:00, 247.99epoch/s, test_loss=13.5, train_loss=6.27]\n",
      "Epoch 741/1000: 100%|██████████| 1/1 [00:00<00:00, 255.45epoch/s, test_loss=13.5, train_loss=6.27]\n",
      "Epoch 742/1000: 100%|██████████| 1/1 [00:00<00:00, 241.01epoch/s, test_loss=13.5, train_loss=6.26]\n",
      "Epoch 743/1000: 100%|██████████| 1/1 [00:00<00:00, 181.46epoch/s, test_loss=13.5, train_loss=6.26]\n",
      "Epoch 744/1000: 100%|██████████| 1/1 [00:00<00:00, 255.95epoch/s, test_loss=13.5, train_loss=6.25]\n",
      "Epoch 745/1000: 100%|██████████| 1/1 [00:00<00:00, 253.28epoch/s, test_loss=13.5, train_loss=6.24]\n",
      "Epoch 746/1000: 100%|██████████| 1/1 [00:00<00:00, 236.54epoch/s, test_loss=13.5, train_loss=6.24]\n",
      "Epoch 747/1000: 100%|██████████| 1/1 [00:00<00:00, 249.47epoch/s, test_loss=13.5, train_loss=6.23]\n",
      "Epoch 748/1000: 100%|██████████| 1/1 [00:00<00:00, 246.43epoch/s, test_loss=13.5, train_loss=6.22]\n",
      "Epoch 749/1000: 100%|██████████| 1/1 [00:00<00:00, 241.16epoch/s, test_loss=13.5, train_loss=6.22]\n",
      "Epoch 750/1000: 100%|██████████| 1/1 [00:00<00:00, 235.17epoch/s, test_loss=13.5, train_loss=6.21]\n",
      "Epoch 751/1000: 100%|██████████| 1/1 [00:00<00:00, 265.28epoch/s, test_loss=13.5, train_loss=6.2]\n",
      "Epoch 752/1000: 100%|██████████| 1/1 [00:00<00:00, 267.22epoch/s, test_loss=13.5, train_loss=6.2]\n",
      "Epoch 753/1000: 100%|██████████| 1/1 [00:00<00:00, 245.78epoch/s, test_loss=13.5, train_loss=6.19]\n",
      "Epoch 754/1000: 100%|██████████| 1/1 [00:00<00:00, 240.51epoch/s, test_loss=13.5, train_loss=6.19]\n",
      "Epoch 755/1000: 100%|██████████| 1/1 [00:00<00:00, 162.25epoch/s, test_loss=13.5, train_loss=6.18]\n",
      "Epoch 756/1000: 100%|██████████| 1/1 [00:00<00:00, 238.11epoch/s, test_loss=13.5, train_loss=6.17]\n",
      "Epoch 757/1000: 100%|██████████| 1/1 [00:00<00:00, 252.59epoch/s, test_loss=13.5, train_loss=6.17]\n",
      "Epoch 758/1000: 100%|██████████| 1/1 [00:00<00:00, 130.71epoch/s, test_loss=13.5, train_loss=6.16]\n",
      "Epoch 759/1000: 100%|██████████| 1/1 [00:00<00:00, 238.03epoch/s, test_loss=13.5, train_loss=6.16]\n",
      "Epoch 760/1000: 100%|██████████| 1/1 [00:00<00:00, 243.11epoch/s, test_loss=13.5, train_loss=6.15]\n",
      "Epoch 761/1000: 100%|██████████| 1/1 [00:00<00:00, 250.30epoch/s, test_loss=13.5, train_loss=6.14]\n",
      "Epoch 762/1000: 100%|██████████| 1/1 [00:00<00:00, 192.15epoch/s, test_loss=13.5, train_loss=6.14]\n",
      "Epoch 763/1000: 100%|██████████| 1/1 [00:00<00:00, 244.01epoch/s, test_loss=13.5, train_loss=6.13]\n",
      "Epoch 764/1000: 100%|██████████| 1/1 [00:00<00:00, 244.87epoch/s, test_loss=13.5, train_loss=6.12]\n",
      "Epoch 765/1000: 100%|██████████| 1/1 [00:00<00:00, 251.62epoch/s, test_loss=13.5, train_loss=6.12]\n",
      "Epoch 766/1000: 100%|██████████| 1/1 [00:00<00:00, 263.46epoch/s, test_loss=13.5, train_loss=6.11]\n",
      "Epoch 767/1000: 100%|██████████| 1/1 [00:00<00:00, 266.10epoch/s, test_loss=13.5, train_loss=6.11]\n",
      "Epoch 768/1000: 100%|██████████| 1/1 [00:00<00:00, 251.88epoch/s, test_loss=13.5, train_loss=6.1]\n",
      "Epoch 769/1000: 100%|██████████| 1/1 [00:00<00:00, 232.62epoch/s, test_loss=13.5, train_loss=6.09]\n",
      "Epoch 770/1000: 100%|██████████| 1/1 [00:00<00:00, 235.49epoch/s, test_loss=13.5, train_loss=6.09]\n",
      "Epoch 771/1000: 100%|██████████| 1/1 [00:00<00:00, 274.01epoch/s, test_loss=13.5, train_loss=6.08]\n",
      "Epoch 772/1000: 100%|██████████| 1/1 [00:00<00:00, 244.11epoch/s, test_loss=13.5, train_loss=6.08]\n",
      "Epoch 773/1000: 100%|██████████| 1/1 [00:00<00:00, 250.78epoch/s, test_loss=13.5, train_loss=6.07]\n",
      "Epoch 774/1000: 100%|██████████| 1/1 [00:00<00:00, 260.99epoch/s, test_loss=13.5, train_loss=6.06]\n",
      "Epoch 775/1000: 100%|██████████| 1/1 [00:00<00:00, 242.42epoch/s, test_loss=13.5, train_loss=6.06]\n",
      "Epoch 776/1000: 100%|██████████| 1/1 [00:00<00:00, 264.51epoch/s, test_loss=13.5, train_loss=6.05]\n",
      "Epoch 777/1000: 100%|██████████| 1/1 [00:00<00:00, 259.04epoch/s, test_loss=13.5, train_loss=6.05]\n",
      "Epoch 778/1000: 100%|██████████| 1/1 [00:00<00:00, 253.77epoch/s, test_loss=13.5, train_loss=6.04]\n",
      "Epoch 779/1000: 100%|██████████| 1/1 [00:00<00:00, 242.31epoch/s, test_loss=13.5, train_loss=6.04]\n",
      "Epoch 780/1000: 100%|██████████| 1/1 [00:00<00:00, 246.93epoch/s, test_loss=13.5, train_loss=6.03]\n",
      "Epoch 781/1000: 100%|██████████| 1/1 [00:00<00:00, 129.96epoch/s, test_loss=13.5, train_loss=6.02]\n",
      "Epoch 782/1000: 100%|██████████| 1/1 [00:00<00:00, 293.86epoch/s, test_loss=13.5, train_loss=6.02]\n",
      "Epoch 783/1000: 100%|██████████| 1/1 [00:00<00:00, 469.69epoch/s, test_loss=13.5, train_loss=6.01]\n",
      "Epoch 784/1000: 100%|██████████| 1/1 [00:00<00:00, 126.83epoch/s, test_loss=13.5, train_loss=6.01]\n",
      "Epoch 785/1000: 100%|██████████| 1/1 [00:00<00:00, 267.90epoch/s, test_loss=13.5, train_loss=6]\n",
      "Epoch 786/1000: 100%|██████████| 1/1 [00:00<00:00, 255.11epoch/s, test_loss=13.5, train_loss=5.99]\n",
      "Epoch 787/1000: 100%|██████████| 1/1 [00:00<00:00, 249.10epoch/s, test_loss=13.5, train_loss=5.99]\n",
      "Epoch 788/1000: 100%|██████████| 1/1 [00:00<00:00, 217.37epoch/s, test_loss=13.5, train_loss=5.98]\n",
      "Epoch 789/1000: 100%|██████████| 1/1 [00:00<00:00, 198.57epoch/s, test_loss=13.5, train_loss=5.98]\n",
      "Epoch 790/1000: 100%|██████████| 1/1 [00:00<00:00, 205.88epoch/s, test_loss=13.5, train_loss=5.97]\n",
      "Epoch 791/1000: 100%|██████████| 1/1 [00:00<00:00, 159.21epoch/s, test_loss=13.5, train_loss=5.97]\n",
      "Epoch 792/1000: 100%|██████████| 1/1 [00:00<00:00, 237.69epoch/s, test_loss=13.5, train_loss=5.96]\n",
      "Epoch 793/1000: 100%|██████████| 1/1 [00:00<00:00, 268.78epoch/s, test_loss=13.5, train_loss=5.95]\n",
      "Epoch 794/1000: 100%|██████████| 1/1 [00:00<00:00, 240.73epoch/s, test_loss=13.5, train_loss=5.95]\n",
      "Epoch 795/1000: 100%|██████████| 1/1 [00:00<00:00, 167.75epoch/s, test_loss=13.5, train_loss=5.94]\n",
      "Epoch 796/1000: 100%|██████████| 1/1 [00:00<00:00, 203.66epoch/s, test_loss=13.5, train_loss=5.94]\n",
      "Epoch 797/1000: 100%|██████████| 1/1 [00:00<00:00, 214.96epoch/s, test_loss=13.5, train_loss=5.93]\n",
      "Epoch 798/1000: 100%|██████████| 1/1 [00:00<00:00, 162.14epoch/s, test_loss=13.5, train_loss=5.93]\n",
      "Epoch 799/1000: 100%|██████████| 1/1 [00:00<00:00, 209.65epoch/s, test_loss=13.5, train_loss=5.92]\n",
      "Epoch 800/1000: 100%|██████████| 1/1 [00:00<00:00, 224.35epoch/s, test_loss=13.5, train_loss=5.91]\n",
      "Epoch 801/1000: 100%|██████████| 1/1 [00:00<00:00, 254.86epoch/s, test_loss=13.5, train_loss=5.91]\n",
      "Epoch 802/1000: 100%|██████████| 1/1 [00:00<00:00, 246.06epoch/s, test_loss=13.5, train_loss=5.9]\n",
      "Epoch 803/1000: 100%|██████████| 1/1 [00:00<00:00, 237.22epoch/s, test_loss=13.5, train_loss=5.9]\n",
      "Epoch 804/1000: 100%|██████████| 1/1 [00:00<00:00, 158.34epoch/s, test_loss=13.5, train_loss=5.89]\n",
      "Epoch 805/1000: 100%|██████████| 1/1 [00:00<00:00, 261.82epoch/s, test_loss=13.5, train_loss=5.89]\n",
      "Epoch 806/1000: 100%|██████████| 1/1 [00:00<00:00, 224.69epoch/s, test_loss=13.5, train_loss=5.88]\n",
      "Epoch 807/1000: 100%|██████████| 1/1 [00:00<00:00, 169.05epoch/s, test_loss=13.5, train_loss=5.88]\n",
      "Epoch 808/1000: 100%|██████████| 1/1 [00:00<00:00, 236.62epoch/s, test_loss=13.5, train_loss=5.87]\n",
      "Epoch 809/1000: 100%|██████████| 1/1 [00:00<00:00, 233.00epoch/s, test_loss=13.5, train_loss=5.86]\n",
      "Epoch 810/1000: 100%|██████████| 1/1 [00:00<00:00, 245.02epoch/s, test_loss=13.5, train_loss=5.86]\n",
      "Epoch 811/1000: 100%|██████████| 1/1 [00:00<00:00, 251.28epoch/s, test_loss=13.5, train_loss=5.85]\n",
      "Epoch 812/1000: 100%|██████████| 1/1 [00:00<00:00, 132.91epoch/s, test_loss=13.5, train_loss=5.85]\n",
      "Epoch 813/1000: 100%|██████████| 1/1 [00:00<00:00, 166.49epoch/s, test_loss=13.5, train_loss=5.84]\n",
      "Epoch 814/1000: 100%|██████████| 1/1 [00:00<00:00, 238.27epoch/s, test_loss=13.5, train_loss=5.84]\n",
      "Epoch 815/1000: 100%|██████████| 1/1 [00:00<00:00, 140.81epoch/s, test_loss=13.5, train_loss=5.83]\n",
      "Epoch 816/1000: 100%|██████████| 1/1 [00:00<00:00, 264.27epoch/s, test_loss=13.5, train_loss=5.83]\n",
      "Epoch 817/1000: 100%|██████████| 1/1 [00:00<00:00, 124.64epoch/s, test_loss=13.5, train_loss=5.82]\n",
      "Epoch 818/1000: 100%|██████████| 1/1 [00:00<00:00, 159.38epoch/s, test_loss=13.5, train_loss=5.82]\n",
      "Epoch 819/1000: 100%|██████████| 1/1 [00:00<00:00, 426.81epoch/s, test_loss=13.5, train_loss=5.81]\n",
      "Epoch 820/1000: 100%|██████████| 1/1 [00:00<00:00, 233.39epoch/s, test_loss=13.5, train_loss=5.8]\n",
      "Epoch 821/1000: 100%|██████████| 1/1 [00:00<00:00, 158.62epoch/s, test_loss=13.5, train_loss=5.8]\n",
      "Epoch 822/1000: 100%|██████████| 1/1 [00:00<00:00, 247.07epoch/s, test_loss=13.5, train_loss=5.79]\n",
      "Epoch 823/1000: 100%|██████████| 1/1 [00:00<00:00, 254.34epoch/s, test_loss=13.5, train_loss=5.79]\n",
      "Epoch 824/1000: 100%|██████████| 1/1 [00:00<00:00, 251.20epoch/s, test_loss=13.5, train_loss=5.78]\n",
      "Epoch 825/1000: 100%|██████████| 1/1 [00:00<00:00, 256.33epoch/s, test_loss=13.5, train_loss=5.78]\n",
      "Epoch 826/1000: 100%|██████████| 1/1 [00:00<00:00, 236.97epoch/s, test_loss=13.5, train_loss=5.77]\n",
      "Epoch 827/1000: 100%|██████████| 1/1 [00:00<00:00, 242.64epoch/s, test_loss=13.5, train_loss=5.77]\n",
      "Epoch 828/1000: 100%|██████████| 1/1 [00:00<00:00, 237.48epoch/s, test_loss=13.5, train_loss=5.76]\n",
      "Epoch 829/1000: 100%|██████████| 1/1 [00:00<00:00, 117.95epoch/s, test_loss=13.5, train_loss=5.76]\n",
      "Epoch 830/1000: 100%|██████████| 1/1 [00:00<00:00, 222.44epoch/s, test_loss=13.5, train_loss=5.75]\n",
      "Epoch 831/1000: 100%|██████████| 1/1 [00:00<00:00, 247.69epoch/s, test_loss=13.5, train_loss=5.75]\n",
      "Epoch 832/1000: 100%|██████████| 1/1 [00:00<00:00, 240.10epoch/s, test_loss=13.5, train_loss=5.74]\n",
      "Epoch 833/1000: 100%|██████████| 1/1 [00:00<00:00, 191.67epoch/s, test_loss=13.5, train_loss=5.74]\n",
      "Epoch 834/1000: 100%|██████████| 1/1 [00:00<00:00, 235.46epoch/s, test_loss=13.5, train_loss=5.73]\n",
      "Epoch 835/1000: 100%|██████████| 1/1 [00:00<00:00, 163.72epoch/s, test_loss=13.5, train_loss=5.73]\n",
      "Epoch 836/1000: 100%|██████████| 1/1 [00:00<00:00, 248.96epoch/s, test_loss=13.5, train_loss=5.72]\n",
      "Epoch 837/1000: 100%|██████████| 1/1 [00:00<00:00, 249.63epoch/s, test_loss=13.5, train_loss=5.72]\n",
      "Epoch 838/1000: 100%|██████████| 1/1 [00:00<00:00, 228.92epoch/s, test_loss=13.5, train_loss=5.71]\n",
      "Epoch 839/1000: 100%|██████████| 1/1 [00:00<00:00, 249.47epoch/s, test_loss=13.5, train_loss=5.7]\n",
      "Epoch 840/1000: 100%|██████████| 1/1 [00:00<00:00, 220.59epoch/s, test_loss=13.5, train_loss=5.7]\n",
      "Epoch 841/1000: 100%|██████████| 1/1 [00:00<00:00, 249.78epoch/s, test_loss=13.5, train_loss=5.69]\n",
      "Epoch 842/1000: 100%|██████████| 1/1 [00:00<00:00, 251.20epoch/s, test_loss=13.5, train_loss=5.69]\n",
      "Epoch 843/1000: 100%|██████████| 1/1 [00:00<00:00, 238.18epoch/s, test_loss=13.5, train_loss=5.68]\n",
      "Epoch 844/1000: 100%|██████████| 1/1 [00:00<00:00, 241.08epoch/s, test_loss=13.5, train_loss=5.68]\n",
      "Epoch 845/1000: 100%|██████████| 1/1 [00:00<00:00, 248.57epoch/s, test_loss=13.5, train_loss=5.67]\n",
      "Epoch 846/1000: 100%|██████████| 1/1 [00:00<00:00, 236.25epoch/s, test_loss=13.5, train_loss=5.67]\n",
      "Epoch 847/1000: 100%|██████████| 1/1 [00:00<00:00, 214.72epoch/s, test_loss=13.5, train_loss=5.66]\n",
      "Epoch 848/1000: 100%|██████████| 1/1 [00:00<00:00, 201.47epoch/s, test_loss=13.5, train_loss=5.66]\n",
      "Epoch 849/1000: 100%|██████████| 1/1 [00:00<00:00, 206.89epoch/s, test_loss=13.5, train_loss=5.65]\n",
      "Epoch 850/1000: 100%|██████████| 1/1 [00:00<00:00, 329.15epoch/s, test_loss=13.5, train_loss=5.65]\n",
      "Epoch 851/1000: 100%|██████████| 1/1 [00:00<00:00, 232.51epoch/s, test_loss=13.5, train_loss=5.64]\n",
      "Epoch 852/1000: 100%|██████████| 1/1 [00:00<00:00, 316.79epoch/s, test_loss=13.5, train_loss=5.64]\n",
      "Epoch 853/1000: 100%|██████████| 1/1 [00:00<00:00, 165.94epoch/s, test_loss=13.5, train_loss=5.63]\n",
      "Epoch 854/1000: 100%|██████████| 1/1 [00:00<00:00, 235.69epoch/s, test_loss=13.5, train_loss=5.63]\n",
      "Epoch 855/1000: 100%|██████████| 1/1 [00:00<00:00, 248.63epoch/s, test_loss=13.5, train_loss=5.62]\n",
      "Epoch 856/1000: 100%|██████████| 1/1 [00:00<00:00, 234.17epoch/s, test_loss=13.5, train_loss=5.62]\n",
      "Epoch 857/1000: 100%|██████████| 1/1 [00:00<00:00, 237.30epoch/s, test_loss=13.5, train_loss=5.61]\n",
      "Epoch 858/1000: 100%|██████████| 1/1 [00:00<00:00, 227.40epoch/s, test_loss=13.5, train_loss=5.61]\n",
      "Epoch 859/1000: 100%|██████████| 1/1 [00:00<00:00, 132.75epoch/s, test_loss=13.5, train_loss=5.6]\n",
      "Epoch 860/1000: 100%|██████████| 1/1 [00:00<00:00, 161.50epoch/s, test_loss=13.5, train_loss=5.6]\n",
      "Epoch 861/1000: 100%|██████████| 1/1 [00:00<00:00, 255.02epoch/s, test_loss=13.5, train_loss=5.59]\n",
      "Epoch 862/1000: 100%|██████████| 1/1 [00:00<00:00, 272.06epoch/s, test_loss=13.5, train_loss=5.59]\n",
      "Epoch 863/1000: 100%|██████████| 1/1 [00:00<00:00, 258.49epoch/s, test_loss=13.5, train_loss=5.58]\n",
      "Epoch 864/1000: 100%|██████████| 1/1 [00:00<00:00, 249.25epoch/s, test_loss=13.5, train_loss=5.58]\n",
      "Epoch 865/1000: 100%|██████████| 1/1 [00:00<00:00, 252.23epoch/s, test_loss=13.5, train_loss=5.57]\n",
      "Epoch 866/1000: 100%|██████████| 1/1 [00:00<00:00, 237.77epoch/s, test_loss=13.5, train_loss=5.57]\n",
      "Epoch 867/1000: 100%|██████████| 1/1 [00:00<00:00, 200.22epoch/s, test_loss=13.5, train_loss=5.57]\n",
      "Epoch 868/1000: 100%|██████████| 1/1 [00:00<00:00, 243.16epoch/s, test_loss=13.5, train_loss=5.56]\n",
      "Epoch 869/1000: 100%|██████████| 1/1 [00:00<00:00, 185.24epoch/s, test_loss=13.5, train_loss=5.56]\n",
      "Epoch 870/1000: 100%|██████████| 1/1 [00:00<00:00, 185.19epoch/s, test_loss=13.5, train_loss=5.55]\n",
      "Epoch 871/1000: 100%|██████████| 1/1 [00:00<00:00, 242.60epoch/s, test_loss=13.5, train_loss=5.55]\n",
      "Epoch 872/1000: 100%|██████████| 1/1 [00:00<00:00, 240.00epoch/s, test_loss=13.5, train_loss=5.54]\n",
      "Epoch 873/1000: 100%|██████████| 1/1 [00:00<00:00, 274.95epoch/s, test_loss=13.5, train_loss=5.54]\n",
      "Epoch 874/1000: 100%|██████████| 1/1 [00:00<00:00, 240.54epoch/s, test_loss=13.5, train_loss=5.53]\n",
      "Epoch 875/1000: 100%|██████████| 1/1 [00:00<00:00, 230.71epoch/s, test_loss=13.5, train_loss=5.53]\n",
      "Epoch 876/1000: 100%|██████████| 1/1 [00:00<00:00, 243.36epoch/s, test_loss=13.5, train_loss=5.52]\n",
      "Epoch 877/1000: 100%|██████████| 1/1 [00:00<00:00, 247.26epoch/s, test_loss=13.5, train_loss=5.52]\n",
      "Epoch 878/1000: 100%|██████████| 1/1 [00:00<00:00, 240.68epoch/s, test_loss=13.5, train_loss=5.51]\n",
      "Epoch 879/1000: 100%|██████████| 1/1 [00:00<00:00, 259.16epoch/s, test_loss=13.5, train_loss=5.51]\n",
      "Epoch 880/1000: 100%|██████████| 1/1 [00:00<00:00, 247.39epoch/s, test_loss=13.5, train_loss=5.5]\n",
      "Epoch 881/1000: 100%|██████████| 1/1 [00:00<00:00, 250.20epoch/s, test_loss=13.5, train_loss=5.5]\n",
      "Epoch 882/1000: 100%|██████████| 1/1 [00:00<00:00, 238.11epoch/s, test_loss=13.5, train_loss=5.49]\n",
      "Epoch 883/1000: 100%|██████████| 1/1 [00:00<00:00, 263.20epoch/s, test_loss=13.5, train_loss=5.49]\n",
      "Epoch 884/1000: 100%|██████████| 1/1 [00:00<00:00, 234.24epoch/s, test_loss=13.5, train_loss=5.48]\n",
      "Epoch 885/1000: 100%|██████████| 1/1 [00:00<00:00, 245.81epoch/s, test_loss=13.5, train_loss=5.48]\n",
      "Epoch 886/1000: 100%|██████████| 1/1 [00:00<00:00, 254.93epoch/s, test_loss=13.5, train_loss=5.48]\n",
      "Epoch 887/1000: 100%|██████████| 1/1 [00:00<00:00, 253.48epoch/s, test_loss=13.5, train_loss=5.47]\n",
      "Epoch 888/1000: 100%|██████████| 1/1 [00:00<00:00, 187.62epoch/s, test_loss=13.5, train_loss=5.47]\n",
      "Epoch 889/1000: 100%|██████████| 1/1 [00:00<00:00, 252.32epoch/s, test_loss=13.5, train_loss=5.46]\n",
      "Epoch 890/1000: 100%|██████████| 1/1 [00:00<00:00, 258.83epoch/s, test_loss=13.5, train_loss=5.46]\n",
      "Epoch 891/1000: 100%|██████████| 1/1 [00:00<00:00, 258.35epoch/s, test_loss=13.5, train_loss=5.45]\n",
      "Epoch 892/1000: 100%|██████████| 1/1 [00:00<00:00, 245.54epoch/s, test_loss=13.5, train_loss=5.45]\n",
      "Epoch 893/1000: 100%|██████████| 1/1 [00:00<00:00, 245.88epoch/s, test_loss=13.5, train_loss=5.44]\n",
      "Epoch 894/1000: 100%|██████████| 1/1 [00:00<00:00, 239.92epoch/s, test_loss=13.5, train_loss=5.44]\n",
      "Epoch 895/1000: 100%|██████████| 1/1 [00:00<00:00, 169.01epoch/s, test_loss=13.5, train_loss=5.43]\n",
      "Epoch 896/1000: 100%|██████████| 1/1 [00:00<00:00, 265.45epoch/s, test_loss=13.5, train_loss=5.43]\n",
      "Epoch 897/1000: 100%|██████████| 1/1 [00:00<00:00, 268.81epoch/s, test_loss=13.5, train_loss=5.42]\n",
      "Epoch 898/1000: 100%|██████████| 1/1 [00:00<00:00, 119.88epoch/s, test_loss=13.5, train_loss=5.42]\n",
      "Epoch 899/1000: 100%|██████████| 1/1 [00:00<00:00, 123.51epoch/s, test_loss=13.5, train_loss=5.42]\n",
      "Epoch 900/1000: 100%|██████████| 1/1 [00:00<00:00, 253.80epoch/s, test_loss=13.5, train_loss=5.41]\n",
      "Epoch 901/1000: 100%|██████████| 1/1 [00:00<00:00, 277.11epoch/s, test_loss=13.5, train_loss=5.41]\n",
      "Epoch 902/1000: 100%|██████████| 1/1 [00:00<00:00, 232.87epoch/s, test_loss=13.5, train_loss=5.4]\n",
      "Epoch 903/1000: 100%|██████████| 1/1 [00:00<00:00, 250.86epoch/s, test_loss=13.5, train_loss=5.4]\n",
      "Epoch 904/1000: 100%|██████████| 1/1 [00:00<00:00, 244.89epoch/s, test_loss=13.5, train_loss=5.39]\n",
      "Epoch 905/1000: 100%|██████████| 1/1 [00:00<00:00, 254.77epoch/s, test_loss=13.5, train_loss=5.39]\n",
      "Epoch 906/1000: 100%|██████████| 1/1 [00:00<00:00, 254.51epoch/s, test_loss=13.5, train_loss=5.38]\n",
      "Epoch 907/1000: 100%|██████████| 1/1 [00:00<00:00, 264.34epoch/s, test_loss=13.5, train_loss=5.38]\n",
      "Epoch 908/1000: 100%|██████████| 1/1 [00:00<00:00, 248.04epoch/s, test_loss=13.5, train_loss=5.38]\n",
      "Epoch 909/1000: 100%|██████████| 1/1 [00:00<00:00, 247.00epoch/s, test_loss=13.5, train_loss=5.37]\n",
      "Epoch 910/1000: 100%|██████████| 1/1 [00:00<00:00, 219.92epoch/s, test_loss=13.5, train_loss=5.37]\n",
      "Epoch 911/1000: 100%|██████████| 1/1 [00:00<00:00, 242.85epoch/s, test_loss=13.5, train_loss=5.36]\n",
      "Epoch 912/1000: 100%|██████████| 1/1 [00:00<00:00, 242.36epoch/s, test_loss=13.5, train_loss=5.36]\n",
      "Epoch 913/1000: 100%|██████████| 1/1 [00:00<00:00, 239.31epoch/s, test_loss=13.5, train_loss=5.35]\n",
      "Epoch 914/1000: 100%|██████████| 1/1 [00:00<00:00, 224.65epoch/s, test_loss=13.5, train_loss=5.35]\n",
      "Epoch 915/1000: 100%|██████████| 1/1 [00:00<00:00, 255.38epoch/s, test_loss=13.5, train_loss=5.34]\n",
      "Epoch 916/1000: 100%|██████████| 1/1 [00:00<00:00, 220.40epoch/s, test_loss=13.5, train_loss=5.34]\n",
      "Epoch 917/1000: 100%|██████████| 1/1 [00:00<00:00, 221.13epoch/s, test_loss=13.5, train_loss=5.34]\n",
      "Epoch 918/1000: 100%|██████████| 1/1 [00:00<00:00, 228.78epoch/s, test_loss=13.5, train_loss=5.33]\n",
      "Epoch 919/1000: 100%|██████████| 1/1 [00:00<00:00, 242.81epoch/s, test_loss=13.5, train_loss=5.33]\n",
      "Epoch 920/1000: 100%|██████████| 1/1 [00:00<00:00, 268.52epoch/s, test_loss=13.5, train_loss=5.32]\n",
      "Epoch 921/1000: 100%|██████████| 1/1 [00:00<00:00, 110.76epoch/s, test_loss=13.5, train_loss=5.32]\n",
      "Epoch 922/1000: 100%|██████████| 1/1 [00:00<00:00, 133.42epoch/s, test_loss=13.5, train_loss=5.31]\n",
      "Epoch 923/1000: 100%|██████████| 1/1 [00:00<00:00, 412.87epoch/s, test_loss=13.5, train_loss=5.31]\n",
      "Epoch 924/1000: 100%|██████████| 1/1 [00:00<00:00, 243.88epoch/s, test_loss=13.5, train_loss=5.31]\n",
      "Epoch 925/1000: 100%|██████████| 1/1 [00:00<00:00, 258.29epoch/s, test_loss=13.5, train_loss=5.3]\n",
      "Epoch 926/1000: 100%|██████████| 1/1 [00:00<00:00, 258.46epoch/s, test_loss=13.5, train_loss=5.3]\n",
      "Epoch 927/1000: 100%|██████████| 1/1 [00:00<00:00, 240.62epoch/s, test_loss=13.5, train_loss=5.29]\n",
      "Epoch 928/1000: 100%|██████████| 1/1 [00:00<00:00, 111.53epoch/s, test_loss=13.5, train_loss=5.29]\n",
      "Epoch 929/1000: 100%|██████████| 1/1 [00:00<00:00, 157.86epoch/s, test_loss=13.5, train_loss=5.29]\n",
      "Epoch 930/1000: 100%|██████████| 1/1 [00:00<00:00, 256.66epoch/s, test_loss=13.5, train_loss=5.28]\n",
      "Epoch 931/1000: 100%|██████████| 1/1 [00:00<00:00, 112.93epoch/s, test_loss=13.5, train_loss=5.28]\n",
      "Epoch 932/1000: 100%|██████████| 1/1 [00:00<00:00, 242.07epoch/s, test_loss=13.5, train_loss=5.27]\n",
      "Epoch 933/1000: 100%|██████████| 1/1 [00:00<00:00, 216.97epoch/s, test_loss=13.5, train_loss=5.27]\n",
      "Epoch 934/1000: 100%|██████████| 1/1 [00:00<00:00, 234.92epoch/s, test_loss=13.5, train_loss=5.26]\n",
      "Epoch 935/1000: 100%|██████████| 1/1 [00:00<00:00, 207.20epoch/s, test_loss=13.5, train_loss=5.26]\n",
      "Epoch 936/1000: 100%|██████████| 1/1 [00:00<00:00, 378.92epoch/s, test_loss=13.5, train_loss=5.26]\n",
      "Epoch 937/1000: 100%|██████████| 1/1 [00:00<00:00, 247.06epoch/s, test_loss=13.5, train_loss=5.25]\n",
      "Epoch 938/1000: 100%|██████████| 1/1 [00:00<00:00, 240.75epoch/s, test_loss=13.5, train_loss=5.25]\n",
      "Epoch 939/1000: 100%|██████████| 1/1 [00:00<00:00, 252.58epoch/s, test_loss=13.5, train_loss=5.24]\n",
      "Epoch 940/1000: 100%|██████████| 1/1 [00:00<00:00, 228.01epoch/s, test_loss=13.5, train_loss=5.24]\n",
      "Epoch 941/1000: 100%|██████████| 1/1 [00:00<00:00, 190.24epoch/s, test_loss=13.5, train_loss=5.24]\n",
      "Epoch 942/1000: 100%|██████████| 1/1 [00:00<00:00, 246.00epoch/s, test_loss=13.5, train_loss=5.23]\n",
      "Epoch 943/1000: 100%|██████████| 1/1 [00:00<00:00, 128.98epoch/s, test_loss=13.5, train_loss=5.23]\n",
      "Epoch 944/1000: 100%|██████████| 1/1 [00:00<00:00, 161.05epoch/s, test_loss=13.5, train_loss=5.22]\n",
      "Epoch 945/1000: 100%|██████████| 1/1 [00:00<00:00, 172.20epoch/s, test_loss=13.6, train_loss=5.22]\n",
      "Epoch 946/1000: 100%|██████████| 1/1 [00:00<00:00, 248.12epoch/s, test_loss=13.6, train_loss=5.22]\n",
      "Epoch 947/1000: 100%|██████████| 1/1 [00:00<00:00, 245.09epoch/s, test_loss=13.6, train_loss=5.21]\n",
      "Epoch 948/1000: 100%|██████████| 1/1 [00:00<00:00, 263.48epoch/s, test_loss=13.6, train_loss=5.21]\n",
      "Epoch 949/1000: 100%|██████████| 1/1 [00:00<00:00, 243.53epoch/s, test_loss=13.6, train_loss=5.2]\n",
      "Epoch 950/1000: 100%|██████████| 1/1 [00:00<00:00, 424.22epoch/s, test_loss=13.6, train_loss=5.2]\n",
      "Epoch 951/1000: 100%|██████████| 1/1 [00:00<00:00, 244.47epoch/s, test_loss=13.6, train_loss=5.19]\n",
      "Epoch 952/1000: 100%|██████████| 1/1 [00:00<00:00, 115.03epoch/s, test_loss=13.6, train_loss=5.19]\n",
      "Epoch 953/1000: 100%|██████████| 1/1 [00:00<00:00, 199.69epoch/s, test_loss=13.6, train_loss=5.19]\n",
      "Epoch 954/1000: 100%|██████████| 1/1 [00:00<00:00, 200.36epoch/s, test_loss=13.6, train_loss=5.18]\n",
      "Epoch 955/1000: 100%|██████████| 1/1 [00:00<00:00, 205.84epoch/s, test_loss=13.6, train_loss=5.18]\n",
      "Epoch 956/1000: 100%|██████████| 1/1 [00:00<00:00, 180.80epoch/s, test_loss=13.6, train_loss=5.18]\n",
      "Epoch 957/1000: 100%|██████████| 1/1 [00:00<00:00, 157.32epoch/s, test_loss=13.6, train_loss=5.17]\n",
      "Epoch 958/1000: 100%|██████████| 1/1 [00:00<00:00, 245.15epoch/s, test_loss=13.6, train_loss=5.17]\n",
      "Epoch 959/1000: 100%|██████████| 1/1 [00:00<00:00, 249.35epoch/s, test_loss=13.6, train_loss=5.16]\n",
      "Epoch 960/1000: 100%|██████████| 1/1 [00:00<00:00, 258.99epoch/s, test_loss=13.6, train_loss=5.16]\n",
      "Epoch 961/1000: 100%|██████████| 1/1 [00:00<00:00, 270.44epoch/s, test_loss=13.6, train_loss=5.16]\n",
      "Epoch 962/1000: 100%|██████████| 1/1 [00:00<00:00, 158.28epoch/s, test_loss=13.6, train_loss=5.15]\n",
      "Epoch 963/1000: 100%|██████████| 1/1 [00:00<00:00, 226.06epoch/s, test_loss=13.6, train_loss=5.15]\n",
      "Epoch 964/1000: 100%|██████████| 1/1 [00:00<00:00, 253.52epoch/s, test_loss=13.6, train_loss=5.14]\n",
      "Epoch 965/1000: 100%|██████████| 1/1 [00:00<00:00, 253.69epoch/s, test_loss=13.6, train_loss=5.14]\n",
      "Epoch 966/1000: 100%|██████████| 1/1 [00:00<00:00, 254.65epoch/s, test_loss=13.6, train_loss=5.14]\n",
      "Epoch 967/1000: 100%|██████████| 1/1 [00:00<00:00, 257.07epoch/s, test_loss=13.6, train_loss=5.13]\n",
      "Epoch 968/1000: 100%|██████████| 1/1 [00:00<00:00, 251.91epoch/s, test_loss=13.6, train_loss=5.13]\n",
      "Epoch 969/1000: 100%|██████████| 1/1 [00:00<00:00, 126.10epoch/s, test_loss=13.6, train_loss=5.12]\n",
      "Epoch 970/1000: 100%|██████████| 1/1 [00:00<00:00, 248.85epoch/s, test_loss=13.6, train_loss=5.12]\n",
      "Epoch 971/1000: 100%|██████████| 1/1 [00:00<00:00, 275.05epoch/s, test_loss=13.6, train_loss=5.12]\n",
      "Epoch 972/1000: 100%|██████████| 1/1 [00:00<00:00, 260.69epoch/s, test_loss=13.6, train_loss=5.11]\n",
      "Epoch 973/1000: 100%|██████████| 1/1 [00:00<00:00, 169.56epoch/s, test_loss=13.6, train_loss=5.11]\n",
      "Epoch 974/1000: 100%|██████████| 1/1 [00:00<00:00, 256.77epoch/s, test_loss=13.6, train_loss=5.11]\n",
      "Epoch 975/1000: 100%|██████████| 1/1 [00:00<00:00, 138.45epoch/s, test_loss=13.6, train_loss=5.1]\n",
      "Epoch 976/1000: 100%|██████████| 1/1 [00:00<00:00, 255.22epoch/s, test_loss=13.6, train_loss=5.1]\n",
      "Epoch 977/1000: 100%|██████████| 1/1 [00:00<00:00, 256.45epoch/s, test_loss=13.6, train_loss=5.09]\n",
      "Epoch 978/1000: 100%|██████████| 1/1 [00:00<00:00, 255.59epoch/s, test_loss=13.6, train_loss=5.09]\n",
      "Epoch 979/1000: 100%|██████████| 1/1 [00:00<00:00, 276.63epoch/s, test_loss=13.6, train_loss=5.09]\n",
      "Epoch 980/1000: 100%|██████████| 1/1 [00:00<00:00, 259.07epoch/s, test_loss=13.6, train_loss=5.08]\n",
      "Epoch 981/1000: 100%|██████████| 1/1 [00:00<00:00, 247.09epoch/s, test_loss=13.6, train_loss=5.08]\n",
      "Epoch 982/1000: 100%|██████████| 1/1 [00:00<00:00, 245.22epoch/s, test_loss=13.6, train_loss=5.08]\n",
      "Epoch 983/1000: 100%|██████████| 1/1 [00:00<00:00, 248.45epoch/s, test_loss=13.6, train_loss=5.07]\n",
      "Epoch 984/1000: 100%|██████████| 1/1 [00:00<00:00, 259.44epoch/s, test_loss=13.6, train_loss=5.07]\n",
      "Epoch 985/1000: 100%|██████████| 1/1 [00:00<00:00, 261.98epoch/s, test_loss=13.6, train_loss=5.06]\n",
      "Epoch 986/1000: 100%|██████████| 1/1 [00:00<00:00, 261.10epoch/s, test_loss=13.6, train_loss=5.06]\n",
      "Epoch 987/1000: 100%|██████████| 1/1 [00:00<00:00, 273.28epoch/s, test_loss=13.6, train_loss=5.06]\n",
      "Epoch 988/1000: 100%|██████████| 1/1 [00:00<00:00, 250.17epoch/s, test_loss=13.6, train_loss=5.05]\n",
      "Epoch 989/1000: 100%|██████████| 1/1 [00:00<00:00, 264.08epoch/s, test_loss=13.6, train_loss=5.05]\n",
      "Epoch 990/1000: 100%|██████████| 1/1 [00:00<00:00, 272.53epoch/s, test_loss=13.6, train_loss=5.05]\n",
      "Epoch 991/1000: 100%|██████████| 1/1 [00:00<00:00, 254.02epoch/s, test_loss=13.6, train_loss=5.04]\n",
      "Epoch 992/1000: 100%|██████████| 1/1 [00:00<00:00, 121.49epoch/s, test_loss=13.6, train_loss=5.04]\n",
      "Epoch 993/1000: 100%|██████████| 1/1 [00:00<00:00, 253.62epoch/s, test_loss=13.6, train_loss=5.03]\n",
      "Epoch 994/1000: 100%|██████████| 1/1 [00:00<00:00, 138.18epoch/s, test_loss=13.6, train_loss=5.03]\n",
      "Epoch 995/1000: 100%|██████████| 1/1 [00:00<00:00, 186.60epoch/s, test_loss=13.6, train_loss=5.03]\n",
      "Epoch 996/1000: 100%|██████████| 1/1 [00:00<00:00, 223.01epoch/s, test_loss=13.6, train_loss=5.02]\n",
      "Epoch 997/1000: 100%|██████████| 1/1 [00:00<00:00, 258.14epoch/s, test_loss=13.6, train_loss=5.02]\n",
      "Epoch 998/1000: 100%|██████████| 1/1 [00:00<00:00, 258.54epoch/s, test_loss=13.6, train_loss=5.02]\n",
      "Epoch 999/1000: 100%|██████████| 1/1 [00:00<00:00, 160.50epoch/s, test_loss=13.6, train_loss=5.01]\n",
      "Epoch 1000/1000: 100%|██████████| 1/1 [00:00<00:00, 278.51epoch/s, test_loss=13.6, train_loss=5.01]\n"
     ]
    }
   ],
   "source": [
    "train_ratings, test_ratings = split_dataframe(ratings)\n",
    "train_data = build_rating_sparse_tensor(train_ratings, users_count, book_count)\n",
    "test_data = build_rating_sparse_tensor(test_ratings, users_count, book_count)\n",
    "\n",
    "# Initialize model\n",
    "gd_model = GDModel(users_count, book_count, embedding_dim)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(gd_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize loss variables\n",
    "losses = []\n",
    "best_loss = 10000\n",
    "best_weights = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss = sparse_mse_loss_func(\n",
    "        train_data,\n",
    "        gd_model.user_embeddings,\n",
    "        gd_model.book_embeddings)\n",
    "    grav_loss = gravity_loss_ratio * gravity_loss_func(\n",
    "        gd_model.book_embeddings.weight,\n",
    "        gd_model.user_embeddings.weight)\n",
    "    reg_loss = reg_loss_ratio * regularization_loss_func(\n",
    "        gd_model.book_embeddings.weight,\n",
    "        gd_model.user_embeddings.weight)\n",
    "\n",
    "    loss = train_loss + reg_loss + grav_loss\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    test_loss = sparse_mse_loss_func(\n",
    "        test_data,\n",
    "        gd_model.user_embeddings,\n",
    "        gd_model.book_embeddings)\n",
    "\n",
    "    # Save best test weights\n",
    "    if test_loss < best_loss:\n",
    "        best_epoch = epoch\n",
    "        train_loss_at_best = train_loss.item()\n",
    "        best_loss = test_loss.item()\n",
    "\n",
    "        del best_weights\n",
    "        best_weights = copy.deepcopy(gd_model.state_dict())\n",
    "\n",
    "    losses.append((train_loss.item(), test_loss.item()))\n",
    "\n",
    "    # Visualize training progress\n",
    "    with tqdm(total=1, desc=f'Epoch {epoch+1}/{epoch_count}', unit='epoch') as t:\n",
    "        t.set_description(f'Epoch {epoch+1}/{epoch_count}')\n",
    "        t.set_postfix(train_loss=train_loss.item(),\n",
    "                      test_loss=test_loss.item())\n",
    "        t.update()\n",
    "\n",
    "    del train_loss\n",
    "    del test_loss\n",
    "    del loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G8EvpMF9FmTV",
    "outputId": "741e5fdf-2524-4cac-cc81-082f7e2a6cd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 718 had lowest TEST LOSS: 13.494 with TRAIN LOSS: 6.411\n"
     ]
    }
   ],
   "source": [
    "best_model = GDModel(users_count, book_count, embedding_dim)\n",
    "best_model.load_state_dict(best_weights)\n",
    "print(f\"EPOCH {best_epoch} had lowest TEST LOSS: {round(best_loss, 3)}\\\n",
    " with TRAIN LOSS: {round(train_loss_at_best, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU-lf7Nm6jSe"
   },
   "source": [
    "### Save book embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UkoW6CrmEmOf"
   },
   "outputs": [],
   "source": [
    "embeddings = gd_model.book_embeddings.weight.detach().numpy()\n",
    "with open(embeddings_save_path, 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs5OMoqZ6gIb"
   },
   "source": [
    "### Plot loss progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "-mezY5YyE8Yp",
    "outputId": "aa6b3d26-c6db-41ca-f2ed-1fc9567d8f5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"0cb96cd1-e448-4f8f-887f-9cd182c026ba\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0cb96cd1-e448-4f8f-887f-9cd182c026ba\")) {                    Plotly.newPlot(                        \"0cb96cd1-e448-4f8f-887f-9cd182c026ba\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[18.802949905395508,18.77288055419922,18.742937088012695,18.713117599487305,18.683427810668945,18.653865814208984,18.624427795410156,18.595121383666992,18.565942764282227,18.53689193725586,18.507970809936523,18.47918128967285,18.45052146911621,18.421993255615234,18.39359474182129,18.365325927734375,18.33719253540039,18.30918312072754,18.281307220458984,18.253562927246094,18.22594451904297,18.198463439941406,18.171104431152344,18.143878936767578,18.116777420043945,18.08980941772461,18.06296730041504,18.036251068115234,18.009662628173828,17.983200073242188,17.95686149597168,17.930646896362305,17.90455436706543,17.878582000732422,17.852731704711914,17.82699966430664,17.801389694213867,17.775894165039062,17.750516891479492,17.725252151489258,17.700101852416992,17.675064086914062,17.6501407623291,17.62532615661621,17.60062026977539,17.576021194458008,17.551532745361328,17.52714729309082,17.50286293029785,17.478683471679688,17.454605102539062,17.430625915527344,17.406747817993164,17.382966995239258,17.35927963256836,17.3356876373291,17.31218910217285,17.28878402709961,17.26546859741211,17.24224281311035,17.219104766845703,17.19605255126953,17.17308807373047,17.150205612182617,17.12740707397461,17.10468864440918,17.082050323486328,17.059494018554688,17.03701400756836,17.01460838317871,16.992280960083008,16.97002410888672,16.947843551635742,16.925731658935547,16.903690338134766,16.881717681884766,16.859811782836914,16.83797264099121,16.816200256347656,16.79448890686035,16.772842407226562,16.751256942749023,16.729732513427734,16.708267211914062,16.686859130859375,16.66550636291504,16.644208908081055,16.622966766357422,16.601778030395508,16.58064079284668,16.559555053710938,16.53851890563965,16.51753044128418,16.4965877532959,16.47569465637207,16.45484161376953,16.434038162231445,16.413272857666016,16.39255142211914,16.371868133544922,16.351224899291992,16.33062171936035,16.310054779052734,16.289522171020508,16.269027709960938,16.24856185913086,16.228132247924805,16.207735061645508,16.187366485595703,16.167028427124023,16.146718978881836,16.126432418823242,16.10617446899414,16.085941314697266,16.065732955932617,16.045547485351562,16.025381088256836,16.005237579345703,15.985114097595215,15.965006828308105,15.944917678833008,15.924845695495605,15.904787063598633,15.884745597839355,15.864715576171875,15.844696044921875,15.824689865112305,15.80469036102295,15.784703254699707,15.76472282409668,15.744749069213867,15.72477912902832,15.704817771911621,15.684855461120605,15.664896965026855,15.644941329956055,15.62498664855957,15.605030059814453,15.585073471069336,15.565112113952637,15.545147895812988,15.525177955627441,15.505203247070312,15.485222816467285,15.465232849121094,15.445234298706055,15.425226211547852,15.405207633972168,15.385176658630371,15.365134239196777,15.345076560974121,15.325004577636719,15.30491828918457,15.284814834594727,15.264690399169922,15.244548797607422,15.224390029907227,15.204208374023438,15.184004783630371,15.163780212402344,15.143531799316406,15.12325668334961,15.102959632873535,15.082633972167969,15.062280654907227,15.041900634765625,15.021490097045898,15.001051902770996,14.980579376220703,14.960078239440918,14.939543724060059,14.918973922729492,14.898371696472168,14.877732276916504,14.857060432434082,14.836347579956055,14.815598487854004,14.794811248779297,14.773984909057617,14.753118515014648,14.732210159301758,14.711260795593262,14.690267562866211,14.669231414794922,14.648151397705078,14.627029418945312,14.605859756469727,14.584643363952637,14.563380241394043,14.542069435119629,14.52071475982666,14.499306678771973,14.477849960327148,14.456343650817871,14.434788703918457,14.413179397583008,14.391521453857422,14.369810104370117,14.348044395446777,14.326227188110352,14.30435562133789,14.282432556152344,14.26045036315918,14.238417625427246,14.216326713562012,14.19417953491211,14.171979904174805,14.14971923828125,14.127406120300293,14.105033874511719,14.082602500915527,14.0601167678833,14.037571907043457,14.014967918395996,13.992307662963867,13.969589233398438,13.946810722351074,13.92397403717041,13.901081085205078,13.878127098083496,13.855113983154297,13.832045555114746,13.808916091918945,13.785728454589844,13.762482643127441,13.739179611206055,13.715816497802734,13.692397117614746,13.668919563293457,13.645384788513184,13.621792793273926,13.598142623901367,13.57443904876709,13.550678253173828,13.526860237121582,13.502989768981934,13.4790620803833,13.455080032348633,13.431045532226562,13.406956672668457,13.382813453674316,13.358622550964355,13.33437728881836,13.31008243560791,13.285737037658691,13.261341094970703,13.236897468566895,13.212408065795898,13.187870979309082,13.163287162780762,13.138654708862305,13.113983154296875,13.08926773071289,13.064509391784668,13.039708137512207,13.01486873626709,12.989988327026367,12.965070724487305,12.940117835998535,12.915127754211426,12.89010238647461,12.865043640136719,12.839953422546387,12.81483268737793,12.789680480957031,12.764501571655273,12.739294052124023,12.714061737060547,12.688806533813477,12.66352653503418,12.638223648071289,12.61290454864502,12.587564468383789,12.56220817565918,12.536834716796875,12.511449813842773,12.48604965209961,12.460639953613281,12.435219764709473,12.409790992736816,12.384358406066895,12.358920097351074,12.333477973937988,12.30803394317627,12.282590866088867,12.257149696350098,12.231712341308594,12.206277847290039,12.180852890014648,12.155433654785156,12.130027770996094,12.104630470275879,12.079248428344727,12.053879737854004,12.028529167175293,12.00319766998291,11.977884292602539,11.952593803405762,11.927328109741211,11.90208625793457,11.876873016357422,11.8516845703125,11.826529502868652,11.801403999328613,11.776312828063965,11.751256942749023,11.726238250732422,11.70125675201416,11.676316261291504,11.651415824890137,11.62656021118164,11.601746559143066,11.576980590820312,11.552261352539062,11.527593612670898,11.502974510192871,11.47840690612793,11.45389461517334,11.429435729980469,11.405034065246582,11.380690574645996,11.356405258178711,11.33218002319336,11.30801773071289,11.283918380737305,11.259882926940918,11.235913276672363,11.212010383605957,11.188175201416016,11.164410591125488,11.140715599060059,11.117090225219727,11.093539237976074,11.070061683654785,11.046658515930176,11.023330688476562,11.000082015991211,10.976908683776855,10.953812599182129,10.930798530578613,10.907864570617676,10.885010719299316,10.862239837646484,10.839550971984863,10.816946029663086,10.794425964355469,10.771989822387695,10.749640464782715,10.727376937866211,10.7052001953125,10.683110237121582,10.661109924316406,10.639196395874023,10.617374420166016,10.595640182495117,10.573997497558594,10.552443504333496,10.53098201751709,10.509611129760742,10.488332748413086,10.467143058776855,10.44604778289795,10.425045013427734,10.404135704040527,10.383316993713379,10.362593650817871,10.341962814331055,10.321426391601562,10.300981521606445,10.280631065368652,10.260372161865234,10.240209579467773,10.220141410827637,10.200165748596191,10.18028450012207,10.16049575805664,10.140800476074219,10.121199607849121,10.101692199707031,10.08227825164795,10.062956809997559,10.043728828430176,10.024593353271484,10.005550384521484,9.986600875854492,9.967741012573242,9.948976516723633,9.930301666259766,9.911718368530273,9.893226623535156,9.874825477600098,9.856514930725098,9.838294982910156,9.820162773132324,9.802123069763184,9.784170150756836,9.76630687713623,9.748533248901367,9.730844497680664,9.71324634552002,9.695733070373535,9.678308486938477,9.660968780517578,9.643714904785156,9.626546859741211,9.60946273803711,9.592463493347168,9.57554817199707,9.558716773986816,9.541967391967773,9.525299072265625,9.508715629577637,9.492212295532227,9.475790023803711,9.459447860717773,9.443185806274414,9.427001953125,9.410898208618164,9.394871711730957,9.378923416137695,9.363051414489746,9.347257614135742,9.331537246704102,9.315896034240723,9.300326347351074,9.284832954406738,9.269412994384766,9.25406551361084,9.238791465759277,9.223589897155762,9.20845890045166,9.193399429321289,9.178409576416016,9.163491249084473,9.148639678955078,9.133858680725098,9.119145393371582,9.104499816894531,9.089921951293945,9.075409889221191,9.060964584350586,9.046584129333496,9.032269477844238,9.018016815185547,9.003829002380371,8.989706039428711,8.9756441116333,8.961645126342773,8.947707176208496,8.933830261230469,8.920014381408691,8.906257629394531,8.892561912536621,8.878923416137695,8.865345001220703,8.851823806762695,8.838359832763672,8.82495403289795,8.811602592468262,8.798307418823242,8.785069465637207,8.77188491821289,8.75875473022461,8.745680809020996,8.732657432556152,8.719687461853027,8.706770896911621,8.6939058303833,8.681093215942383,8.668330192565918,8.655618667602539,8.642956733703613,8.630345344543457,8.617783546447754,8.605270385742188,8.592804908752441,8.580389022827148,8.568017959594727,8.555695533752441,8.54341983795166,8.531189918518066,8.519006729125977,8.506868362426758,8.49477481842041,8.482725143432617,8.470721244812012,8.458760261535645,8.446844100952148,8.434969902038574,8.423137664794922,8.411347389221191,8.399602890014648,8.387895584106445,8.37623119354248,8.364607810974121,8.353023529052734,8.34148120880127,8.329977989196777,8.318514823913574,8.307089805603027,8.295703887939453,8.284356117248535,8.273046493530273,8.261775016784668,8.250540733337402,8.239343643188477,8.228182792663574,8.217059135437012,8.205970764160156,8.194920539855957,8.183903694152832,8.17292308807373,8.16197681427002,8.151065826416016,8.140190124511719,8.12934684753418,8.118539810180664,8.10776424407959,8.09702205657959,8.08631420135498,8.075637817382812,8.064994812011719,8.054384231567383,8.043806076049805,8.033259391784668,8.022743225097656,8.012259483337402,8.001806259155273,7.991384029388428,7.980992317199707,7.970631122589111,7.960300922393799,7.9499993324279785,7.939727783203125,7.929486274719238,7.919273853302002,7.909090042114258,7.898935317993164,7.8888092041015625,7.878711700439453,7.868642330169678,7.858600616455078,7.848587512969971,7.838601589202881,7.828643321990967,7.818711757659912,7.808808326721191,7.7989301681518555,7.789080619812012,7.779256343841553,7.769458770751953,7.759687900543213,7.749942779541016,7.740222454071045,7.730529308319092,7.720860958099365,7.711218357086182,7.701600074768066,7.692008018493652,7.682439804077148,7.672896385192871,7.663379192352295,7.6538848876953125,7.644414901733398,7.634969234466553,7.625547409057617,7.616150379180908,7.606776714324951,7.5974249839782715,7.588099479675293,7.578795433044434,7.569515228271484,7.560257434844971,7.551023960113525,7.541811943054199,7.532622814178467,7.523456573486328,7.514312744140625,7.505190372467041,7.496091842651367,7.487014293670654,7.477959156036377,7.468925476074219,7.459914207458496,7.450924396514893,7.441954612731934,7.433007717132568,7.424081325531006,7.415177822113037,7.406294345855713,7.39743185043335,7.388590335845947,7.379769325256348,7.370969772338867,7.362190246582031,7.353431224822998,7.344692707061768,7.33597469329834,7.327276706695557,7.318599224090576,7.309941291809082,7.301303863525391,7.292685508728027,7.284087657928467,7.275509834289551,7.266950607299805,7.258411884307861,7.24989128112793,7.241392135620117,7.232911109924316,7.224449634552002,7.216007232666016,7.207584381103516,7.199179172515869,7.190793991088867,7.182427883148193,7.174079895019531,7.1657514572143555,7.15744161605835,7.149148941040039,7.140875339508057,7.132620811462402,7.124385833740234,7.116168022155762,7.107968330383301,7.099786281585693,7.091622829437256,7.08347749710083,7.075349807739258,7.067240238189697,7.059149265289307,7.0510759353637695,7.0430192947387695,7.034982204437256,7.026960849761963,7.01895809173584,7.010972023010254,7.00300407409668,6.995053291320801,6.987121105194092,6.979205131530762,6.971307277679443,6.963425159454346,6.955561637878418,6.947714328765869,6.939884662628174,6.932071685791016,6.924275875091553,6.916496276855469,6.90873384475708,6.900988578796387,6.893259525299072,6.885548114776611,6.877852439880371,6.870173454284668,6.862511157989502,6.854866027832031,6.8472371101379395,6.839623928070068,6.832028388977051,6.824448585510254,6.816884994506836,6.809337139129639,6.801806926727295,6.794291973114014,6.786793231964111,6.7793097496032715,6.771844387054443,6.764393329620361,6.756958484649658,6.749540328979492,6.742137432098389,6.734751224517822,6.727380275726318,6.720024108886719,6.712686061859131,6.705361366271973,6.69805383682251,6.690761566162109,6.6834845542907715,6.676223278045654,6.6689772605896,6.661746501922607,6.654531955718994,6.647332191467285,6.640148639678955,6.6329803466796875,6.625826835632324,6.618689060211182,6.611565589904785,6.604458332061768,6.597365379333496,6.5902886390686035,6.583227634429932,6.576179504394531,6.569148540496826,6.562130928039551,6.555128574371338,6.5481414794921875,6.541169166564941,6.534212589263916,6.5272698402404785,6.5203423500061035,6.513429164886475,6.506531715393066,6.499648094177246,6.4927802085876465,6.485926151275635,6.479086875915527,6.472262382507324,6.465452194213867,6.4586567878723145,6.451875686645508,6.4451093673706055,6.438357353210449,6.4316205978393555,6.424896717071533,6.418188095092773,6.411493301391602,6.40481424331665,6.398148536682129,6.391496181488037,6.384859085083008,6.378235816955566,6.371626853942871,6.365032196044922,6.358452320098877,6.351884841918945,6.345332622528076,6.338795185089111,6.332269191741943,6.325760364532471,6.319262981414795,6.312780857086182,6.306312084197998,6.2998576164245605,6.293416500091553,6.286990165710449,6.280576229095459,6.274176597595215,6.267791748046875,6.261420249938965,6.255062103271484,6.248717784881592,6.242386341094971,6.236069679260254,6.229765892028809,6.223474979400635,6.217199325561523,6.210936069488525,6.204686641693115,6.198450565338135,6.1922287940979,6.186018943786621,6.179823398590088,6.173640727996826,6.1674723625183105,6.161316394805908,6.155174255371094,6.149045467376709,6.142928600311279,6.136826515197754,6.1307373046875,6.124660491943359,6.118598461151123,6.112548351287842,6.10651159286499,6.10048770904541,6.094477653503418,6.088479042053223,6.082494258880615,6.076522350311279,6.070563793182373,6.0646185874938965,6.058685302734375,6.052765369415283,6.046858310699463,6.040964603424072,6.035082817077637,6.029213905334473,6.0233588218688965,6.017515182495117,6.011684894561768,6.005867004394531,6.000062465667725,5.994270324707031,5.988490581512451,5.982723236083984,5.976969242095947,5.971226692199707,5.965497016906738,5.959780693054199,5.954076290130615,5.9483842849731445,5.942704677581787,5.937037467956543,5.93138313293457,5.925741195678711,5.920111179351807,5.914493560791016,5.90888786315918,5.903295516967773,5.897714614868164,5.892146110534668,5.886590003967285,5.881045818328857,5.875514984130859,5.869994163513184,5.864487171173096,5.858992099761963,5.853508472442627,5.848036766052246,5.8425774574279785,5.837130546569824,5.831694602966309,5.8262715339660645,5.820860385894775,5.815461158752441,5.810073375701904,5.8046979904174805,5.799334526062012,5.79398250579834,5.788641929626465,5.783314228057861,5.7779974937438965,5.7726922035217285,5.767399787902832,5.762117862701416,5.756848335266113,5.751590251922607,5.746343612670898,5.7411088943481445,5.735886096954346,5.7306742668151855,5.7254743576049805,5.720285892486572,5.715108394622803,5.7099432945251465,5.704789638519287,5.699646949768066,5.694516181945801,5.689395904541016,5.6842875480651855,5.679190158843994,5.674104690551758,5.66903018951416,5.663967132568359,5.658915042877197,5.653874397277832,5.648845195770264,5.643827438354492,5.638820171356201,5.633824348449707,5.628839492797852,5.623866558074951,5.618903160095215,5.613952159881592,5.609011173248291,5.604081630706787,5.599164009094238,5.594256401062012,5.589360237121582,5.584474086761475,5.579599857330322,5.57473611831665,5.569883346557617,5.565041542053223,5.560210227966309,5.555390357971191,5.5505805015563965,5.545782089233398,5.540994167327881,5.5362162590026855,5.5314507484436035,5.526693820953369,5.521949291229248,5.517214298248291,5.512490749359131,5.507777690887451,5.503074645996094,5.498382568359375,5.4937005043029785,5.489029407501221,5.484368801116943,5.4797186851501465,5.47507905960083,5.470449924468994,5.465830326080322,5.461222171783447,5.456624507904053,5.452036380767822,5.4474592208862305,5.442891597747803,5.438334941864014,5.433788299560547,5.4292521476745605,5.424725532531738,5.420209884643555,5.415704250335693,5.411208152770996,5.4067230224609375,5.402247428894043,5.397781848907471,5.393326759338379,5.388881683349609,5.384446144104004,5.380021095275879,5.375606536865234,5.371201038360596,5.366805553436279,5.362420558929443,5.3580451011657715,5.353679180145264,5.349323272705078,5.344977855682373,5.340641975402832,5.336316108703613,5.3319993019104,5.32769250869751,5.323395729064941,5.319108963012695,5.314830780029297,5.310563564300537,5.306305408477783,5.302056789398193,5.297817230224609,5.293587684631348,5.289368629455566,5.285158634185791,5.2809576988220215,5.276766777038574,5.272585391998291,5.2684125900268555,5.264249801635742,5.260097026824951,5.255953311920166,5.2518181800842285,5.2476935386657715,5.243577480316162,5.239470958709717,5.2353739738464355,5.231286525726318,5.227207660675049,5.223138332366943,5.219078540802002,5.215027809143066,5.210986137390137,5.206954002380371,5.202930450439453,5.198915958404541,5.194911003112793,5.190914630889893,5.1869282722473145,5.182950019836426,5.178980827331543,5.175021648406982,5.171070575714111,5.167129039764404,5.163195610046387,5.159271717071533,5.1553568840026855,5.1514506340026855,5.147552967071533,5.143664360046387,5.139785289764404,5.135914325714111,5.132051944732666,5.128199100494385,5.124354362487793,5.120518684387207,5.116691589355469,5.112873077392578,5.109064102172852,5.105262279510498,5.101469993591309,5.097686767578125,5.093911170959473,5.090144634246826,5.086386203765869,5.082637310028076,5.078896522521973,5.075163841247559,5.071439743041992,5.067724704742432,5.064017295837402,5.060318946838379,5.056628227233887,5.0529465675354,5.0492730140686035,5.045608043670654,5.0419511795043945,5.038302898406982,5.034662246704102,5.031030654907227,5.027407169342041,5.023791790008545,5.02018404006958,5.016585350036621,5.012994766235352,5.0094122886657715],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[18.521560668945312,18.51140785217285,18.501310348510742,18.491260528564453,18.48125648498535,18.47130584716797,18.46141242980957,18.451583862304688,18.441816329956055,18.432117462158203,18.4224910736084,18.41292953491211,18.403438568115234,18.39402198791504,18.38467788696289,18.375408172607422,18.366212844848633,18.357091903686523,18.34804916381836,18.339082717895508,18.330188751220703,18.32137107849121,18.31262969970703,18.3039608001709,18.295368194580078,18.286848068237305,18.278400421142578,18.2700252532959,18.261720657348633,18.253488540649414,18.245323181152344,18.23723030090332,18.229204177856445,18.221242904663086,18.213350296020508,18.205524444580078,18.19776153564453,18.1900634765625,18.18242645263672,18.17485237121582,18.167343139648438,18.159889221191406,18.152490615844727,18.14515495300293,18.13787269592285,18.130645751953125,18.12347412109375,18.11635398864746,18.10928726196289,18.102270126342773,18.095304489135742,18.088388442993164,18.08152198791504,18.074703216552734,18.067930221557617,18.061203002929688,18.05452537536621,18.047887802124023,18.041296005249023,18.034748077392578,18.02823829650879,18.02177619934082,18.015350341796875,18.00896644592285,18.002620697021484,17.99631690979004,17.99004364013672,17.98381233215332,17.97761344909668,17.971450805664062,17.965320587158203,17.959226608276367,17.95316505432129,17.947132110595703,17.941133499145508,17.93515968322754,17.929222106933594,17.923307418823242,17.917423248291016,17.911563873291016,17.905733108520508,17.89992332458496,17.894140243530273,17.888378143310547,17.882640838623047,17.876922607421875,17.871225357055664,17.86554718017578,17.859888076782227,17.854248046875,17.848623275756836,17.843013763427734,17.837421417236328,17.831838607788086,17.826269149780273,17.82071876525879,17.81517219543457,17.80963897705078,17.80411148071289,17.79859733581543,17.793087005615234,17.78758430480957,17.782085418701172,17.77659034729004,17.771100997924805,17.76561164855957,17.7601261138916,17.754638671875,17.7491512298584,17.743661880493164,17.73817253112793,17.732675552368164,17.7271785736084,17.721670150756836,17.716161727905273,17.71063995361328,17.705114364624023,17.699575424194336,17.694028854370117,17.6884708404541,17.682897567749023,17.67731475830078,17.671714782714844,17.66609764099121,17.66046714782715,17.654817581176758,17.64914894104004,17.64345932006836,17.637752532958984,17.632022857666016,17.62626838684082,17.620492935180664,17.61469078063965,17.608863830566406,17.603012084960938,17.597131729125977,17.59122085571289,17.585277557373047,17.579309463500977,17.573305130004883,17.5672664642334,17.561199188232422,17.555091857910156,17.548952102661133,17.542774200439453,17.536556243896484,17.530298233032227,17.524003982543945,17.51766586303711,17.511287689208984,17.504863739013672,17.49839210510254,17.491880416870117,17.485319137573242,17.47871208190918,17.472055435180664,17.465349197387695,17.45859146118164,17.451784133911133,17.444923400878906,17.438003540039062,17.431034088134766,17.42400550842285,17.41692352294922,17.409780502319336,17.402578353881836,17.39531707763672,17.38799476623535,17.3806095123291,17.37316131591797,17.365650177001953,17.358070373535156,17.35042953491211,17.34271812438965,17.334938049316406,17.32708740234375,17.319171905517578,17.311180114746094,17.303119659423828,17.29498291015625,17.286773681640625,17.278491973876953,17.270130157470703,17.261693954467773,17.25318145751953,17.244586944580078,17.23591423034668,17.22716522216797,17.21833038330078,17.20941734313965,17.200416564941406,17.191335678100586,17.182170867919922,17.17292022705078,17.16358184814453,17.15416145324707,17.144649505615234,17.135051727294922,17.125364303588867,17.11558723449707,17.105722427368164,17.095762252807617,17.08571434020996,17.07557487487793,17.06534194946289,17.05501937866211,17.04459571838379,17.034086227416992,17.023479461669922,17.01277732849121,17.001977920532227,16.9910888671875,16.980100631713867,16.969017028808594,16.95783805847168,16.94655990600586,16.935184478759766,16.9237117767334,16.91214370727539,16.90047836303711,16.888713836669922,16.876853942871094,16.864892959594727,16.85283660888672,16.840682983398438,16.828428268432617,16.81608009338379,16.803634643554688,16.79109001159668,16.7784481048584,16.76571273803711,16.752878189086914,16.739948272705078,16.726919174194336,16.71379852294922,16.700584411621094,16.687273025512695,16.67386817932129,16.660369873046875,16.646778106689453,16.63309669494629,16.61932373046875,16.605457305908203,16.591501235961914,16.577455520629883,16.563323974609375,16.549100875854492,16.534793853759766,16.520401000976562,16.50592613220215,16.491363525390625,16.476720809936523,16.461994171142578,16.447189331054688,16.43230438232422,16.417341232299805,16.402299880981445,16.38718605041504,16.371997833251953,16.356733322143555,16.341400146484375,16.32599639892578,16.31052589416504,16.294986724853516,16.279382705688477,16.263715744018555,16.247983932495117,16.232194900512695,16.21634292602539,16.200435638427734,16.184471130371094,16.168455123901367,16.152385711669922,16.136266708374023,16.120098114013672,16.103883743286133,16.087623596191406,16.071321487426758,16.054977416992188,16.038597106933594,16.022174835205078,16.005722045898438,15.98923397064209,15.972713470458984,15.95616626739502,15.93958854675293,15.922988891601562,15.906364440917969,15.889718055725098,15.873056411743164,15.856375694274902,15.839680671691895,15.82297134399414,15.806253433227539,15.78952407836914,15.772790908813477,15.756052017211914,15.739312171936035,15.722573280334473,15.705832481384277,15.689098358154297,15.672369956970215,15.655648231506348,15.63893985748291,15.622241020202637,15.605560302734375,15.588892936706543,15.572246551513672,15.555619239807129,15.539013862609863,15.52243423461914,15.505881309509277,15.48935604095459,15.472860336303711,15.456398963928223,15.439970016479492,15.423579216003418,15.407227516174316,15.390913963317871,15.374641418457031,15.358413696289062,15.342231750488281,15.32609748840332,15.31001091003418,15.293973922729492,15.277992248535156,15.26206111907959,15.246187210083008,15.23037338256836,15.214613914489746,15.198917388916016,15.183280944824219,15.167708396911621,15.152201652526855,15.136759757995605,15.12138557434082,15.106080055236816,15.090845108032227,15.07568073272705,15.060592651367188,15.045575141906738,15.030634880065918,15.015769958496094,15.000982284545898,14.986271858215332,14.971643447875977,14.957093238830566,14.942628860473633,14.928241729736328,14.913941383361816,14.899723052978516,14.88559341430664,14.871546745300293,14.857587814331055,14.843716621398926,14.829935073852539,14.816240310668945,14.80263614654541,14.789119720458984,14.775697708129883,14.76236629486084,14.749125480651855,14.735979080200195,14.72292423248291,14.7099609375,14.69709587097168,14.684317588806152,14.671638488769531,14.659053802490234,14.646564483642578,14.634169578552246,14.621870040893555,14.609663963317871,14.597556114196777,14.585543632507324,14.573625564575195,14.561806678771973,14.550082206726074,14.5384521484375,14.526920318603516,14.515480995178223,14.504142761230469,14.492898941040039,14.481749534606934,14.470697402954102,14.45974063873291,14.448878288269043,14.438112258911133,14.427441596984863,14.416865348815918,14.406383514404297,14.39599609375,14.38570499420166,14.375504493713379,14.365400314331055,14.355386734008789,14.345468521118164,14.335640907287598,14.325907707214355,14.316265106201172,14.306714057922363,14.29725456237793,14.287885665893555,14.278607368469238,14.269414901733398,14.260316848754883,14.251306533813477,14.242383003234863,14.233550071716309,14.224801063537598,14.21613883972168,14.207564353942871,14.199076652526855,14.190674781799316,14.182356834411621,14.17412281036377,14.165973663330078,14.157906532287598,14.149920463562012,14.142020225524902,14.134197235107422,14.126457214355469,14.11879825592041,14.111220359802246,14.103716850280762,14.096295356750488,14.088953971862793,14.081684112548828,14.074494361877441,14.067380905151367,14.060343742370605,14.053378105163574,14.046490669250488,14.03967571258545,14.03293228149414,14.026263236999512,14.019664764404297,14.013137817382812,14.006682395935059,14.000297546386719,13.993979454040527,13.987732887268066,13.981552124023438,13.975439071655273,13.969396591186523,13.963418006896973,13.95750617980957,13.951655387878418,13.945874214172363,13.940154075622559,13.934497833251953,13.928905487060547,13.923375129699707,13.9179048538208,13.912498474121094,13.907149314880371,13.901861190795898,13.896634101867676,13.891462326049805,13.88634967803955,13.881293296813965,13.87629508972168,13.871355056762695,13.866470336914062,13.861639022827148,13.856863975524902,13.852140426635742,13.84747314453125,13.84285831451416,13.838294982910156,13.833784103393555,13.829324722290039,13.82491683959961,13.820556640625,13.816247940063477,13.811989784240723,13.807779312133789,13.803614616394043,13.799501419067383,13.795433044433594,13.791414260864258,13.787440299987793,13.7835111618042,13.779626846313477,13.775788307189941,13.771995544433594,13.768244743347168,13.764538764953613,13.760873794555664,13.757251739501953,13.75367259979248,13.750133514404297,13.746635437011719,13.743180274963379,13.739763259887695,13.7363862991333,13.733049392700195,13.729751586914062,13.72649097442627,13.72326946258545,13.720084190368652,13.716937065124512,13.713827133178711,13.710755348205566,13.707716941833496,13.704712867736816,13.701748847961426,13.698816299438477,13.695919036865234,13.693053245544434,13.690225601196289,13.68742847442627,13.684664726257324,13.681933403015137,13.679235458374023,13.676569938659668,13.673933029174805,13.671329498291016,13.668756484985352,13.666215896606445,13.663703918457031,13.66122055053711,13.658767700195312,13.656344413757324,13.653949737548828,13.651583671569824,13.649246215820312,13.646937370300293,13.644655227661133,13.642400741577148,13.64017391204834,13.637971878051758,13.635798454284668,13.633649826049805,13.6315279006958,13.629430770874023,13.627358436584473,13.625313758850098,13.623291015625,13.621294021606445,13.619322776794434,13.6173734664917,13.615448951721191,13.613546371459961,13.611669540405273,13.609814643859863,13.607980728149414,13.606170654296875,13.604384422302246,13.602619171142578,13.600873947143555,13.599152565002441,13.597451210021973,13.595771789550781,13.594111442565918,13.592473983764648,13.59085464477539,13.589258193969727,13.587679862976074,13.586122512817383,13.584583282470703,13.583063125610352,13.581563949584961,13.580082893371582,13.578619956970215,13.577176094055176,13.575750350952148,13.57434368133545,13.572954177856445,13.571581840515137,13.57022762298584,13.568891525268555,13.567573547363281,13.56627082824707,13.564984321594238,13.563714981079102,13.562463760375977,13.561227798461914,13.56000804901123,13.558804512023926,13.557616233825684,13.55644416809082,13.55528736114502,13.554146766662598,13.553020477294922,13.551909446716309,13.550812721252441,13.549731254577637,13.548665046691895,13.547612190246582,13.546573638916016,13.545547485351562,13.544538497924805,13.54354190826416,13.542560577392578,13.541592597961426,13.540635108947754,13.539693832397461,13.538765907287598,13.537849426269531,13.536945343017578,13.536056518554688,13.535177230834961,13.534313201904297,13.53346061706543,13.53261947631836,13.531790733337402,13.530976295471191,13.530170440673828,13.529377937316895,13.528596878051758,13.527827262878418,13.527070045471191,13.526323318481445,13.525588035583496,13.524862289428711,13.524149894714355,13.52344799041748,13.522756576538086,13.522074699401855,13.521405220031738,13.520743370056152,13.520094871520996,13.519455909729004,13.51882553100586,13.518206596374512,13.517597198486328,13.516998291015625,13.51640796661377,13.515829086303711,13.515257835388184,13.514698028564453,13.51414680480957,13.513604164123535,13.51307201385498,13.512547492980957,13.512033462524414,13.511526107788086,13.511030197143555,13.510540962219238,13.510061264038086,13.509591102600098,13.50912857055664,13.508676528930664,13.50822925567627,13.507791519165039,13.507362365722656,13.506941795349121,13.506529808044434,13.506123542785645,13.505727767944336,13.505337715148926,13.50495719909668,13.504583358764648,13.504218101501465,13.503857612609863,13.503506660461426,13.503164291381836,13.502826690673828,13.502497673034668,13.502175331115723,13.501860618591309,13.501551628112793,13.501250267028809,13.500957489013672,13.500670433044434,13.500389099121094,13.500117301940918,13.499849319458008,13.499588012695312,13.499335289001465,13.4990873336792,13.498845100402832,13.498612403869629,13.498384475708008,13.498162269592285,13.497945785522461,13.497734069824219,13.49753189086914,13.497334480285645,13.497142791748047,13.496955871582031,13.49677562713623,13.496603012084961,13.49643325805664,13.496269226074219,13.496112823486328,13.495962142944336,13.495814323425293,13.495675086975098,13.495538711547852,13.495410919189453,13.495285034179688,13.49516487121582,13.495051383972168,13.494942665100098,13.494837760925293,13.494739532470703,13.494645118713379,13.494556427001953,13.494474411010742,13.494394302368164,13.494319915771484,13.494250297546387,13.494185447692871,13.494126319885254,13.494071006774902,13.494020462036133,13.493974685668945,13.493931770324707,13.493895530700684,13.49386215209961,13.493834495544434,13.49381160736084,13.493792533874512,13.493776321411133,13.493764877319336,13.493759155273438,13.493757247924805,13.493757247924805,13.493762969970703,13.493773460388184,13.493786811828613,13.493804931640625,13.493826866149902,13.493853569030762,13.493881225585938,13.493915557861328,13.493951797485352,13.493992805480957,13.494037628173828,13.494085311889648,13.49413776397705,13.494193077087402,13.49425220489502,13.494314193725586,13.494380950927734,13.494450569152832,13.494523048400879,13.494600296020508,13.494680404663086,13.49476432800293,13.494850158691406,13.494940757751465,13.49503231048584,13.495129585266113,13.495227813720703,13.495332717895508,13.495438575744629,13.4955472946167,13.495658874511719,13.49577522277832,13.495894432067871,13.496014595031738,13.496139526367188,13.496265411376953,13.4963960647583,13.496527671813965,13.496664047241211,13.496803283691406,13.49694538116455,13.497087478637695,13.497235298156738,13.497383117675781,13.497537612915039,13.497692108154297,13.49785041809082,13.49800968170166,13.498173713684082,13.498337745666504,13.498506546020508,13.498677253723145,13.498847961425781,13.499025344848633,13.4992036819458,13.499383926391602,13.499565124511719,13.499750137329102,13.49993896484375,13.500127792358398,13.500319480895996,13.500513076782227,13.500710487365723,13.500909805297852,13.501111030578613,13.501314163208008,13.501518249511719,13.501727104187012,13.501935005187988,13.502147674560547,13.502361297607422,13.502577781677246,13.502795219421387,13.503015518188477,13.5032377243042,13.503461837768555,13.503686904907227,13.503914833068848,13.504145622253418,13.504375457763672,13.504609107971191,13.504844665527344,13.505082130432129,13.505321502685547,13.505560874938965,13.505804061889648,13.506048202514648,13.506294250488281,13.506543159484863,13.506791114807129,13.50704288482666,13.507295608520508,13.507550239562988,13.507807731628418,13.508064270019531,13.508323669433594,13.508583068847656,13.5088472366333,13.509111404418945,13.509376525878906,13.5096435546875,13.509913444519043,13.510183334350586,13.510453224182129,13.510726928710938,13.511001586914062,13.511277198791504,13.511553764343262,13.511832237243652,13.512113571166992,13.512393951416016,13.512676239013672,13.512960433959961,13.513246536254883,13.513531684875488,13.51382064819336,13.51410961151123,13.514399528503418,13.514692306518555,13.514984130859375,13.515278816223145,13.51557445526123,13.515870094299316,13.516168594360352,13.516468048095703,13.516766548156738,13.517066955566406,13.51736831665039,13.517672538757324,13.517975807189941,13.518281936645508,13.518588066101074,13.518895149230957,13.519204139709473,13.519513130187988,13.519824028015137,13.520135879516602,13.520448684692383,13.520760536193848,13.521075248718262,13.521389961242676,13.521706581115723,13.522024154663086,13.52234172821045,13.522661209106445,13.522981643676758,13.523300170898438,13.523622512817383,13.523944854736328,13.52426815032959,13.524592399597168,13.524917602539062,13.525242805480957,13.525568962097168,13.525895118713379,13.526223182678223,13.526552200317383,13.526881217956543,13.527212142944336,13.527543067932129,13.527874946594238,13.528205871582031,13.528539657592773,13.5288724899292,13.529206275939941,13.529542922973633,13.529877662658691,13.530216217041016,13.530550956726074,13.530888557434082,13.531227111816406,13.531564712524414,13.531904220581055,13.532244682312012,13.532584190368652,13.532926559448242,13.5332670211792,13.533609390258789,13.533951759338379,13.534296035766602,13.534638404846191,13.534981727600098,13.53532886505127,13.535672187805176,13.536019325256348,13.53636360168457,13.536710739135742,13.537056922912598,13.537405014038086,13.537753105163574,13.538100242614746,13.53844928741455,13.538797378540039,13.539148330688477,13.539498329162598,13.539848327636719,13.540200233459473,13.540550231933594,13.540901184082031,13.541253089904785,13.541605949401855,13.54195785522461,13.54231071472168,13.542664527893066,13.543018341064453,13.54337215423584,13.54372501373291,13.544078826904297,13.54443359375,13.544791221618652,13.545145034790039,13.545498847961426,13.545855522155762,13.546210289001465,13.546567916870117,13.546924591064453,13.547279357910156,13.547637939453125,13.547994613647461,13.548351287841797,13.548709869384766,13.549066543579102,13.54942512512207,13.549781799316406,13.550139427185059,13.550498008728027,13.550856590270996,13.551216125488281,13.551575660705566,13.551935195922852,13.552291870117188,13.552653312683105,13.55301284790039,13.553372383117676,13.553732872009277,13.55409049987793,13.554450035095215,13.554810523986816,13.555171012878418,13.555530548095703,13.555891036987305,13.556251525878906,13.556612968444824,13.556971549987793,13.557332038879395,13.557693481445312,13.55805492401123,13.558415412902832,13.558774948120117,13.559136390686035,13.559497833251953,13.559857368469238,13.560216903686523,13.560578346252441,13.560937881469727,13.561299324035645,13.561659812927246,13.562021255493164,13.562382698059082,13.562744140625,13.563103675842285,13.563464164733887,13.563825607299805,13.56418514251709,13.564544677734375,13.564906120300293,13.565267562866211,13.565627098083496,13.565986633300781,13.5663480758667,13.566707611083984,13.56706714630127,13.567428588867188,13.567788124084473,13.568147659301758,13.568506240844727,13.568866729736328,13.569225311279297,13.569584846496582,13.569944381713867],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\",\"width\":5},\"mode\":\"lines\",\"name\":\"Averaged Train Loss\",\"x\":[19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[18.52456979751587,18.49571952819824,18.46699867248535,18.438407039642335,18.40994510650635,18.3816125869751,18.35340976715088,18.325336742401124,18.297393226623537,18.269579219818116,18.24189462661743,18.21433916091919,18.186912441253664,18.159614086151127,18.132443523406984,18.105400371551518,18.07848405838013,18.0516939163208,18.025029468536378,17.998489952087404,17.972074413299563,17.945782279968263,17.919612312316897,17.89356412887573,17.867636489868165,17.841828632354737,17.816139221191403,17.79056749343872,17.765112304687502,17.739772319793705,17.714546489715577,17.689433670043947,17.6644326210022,17.639542293548583,17.61476154327393,17.59008893966675,17.565523338317874,17.54106330871582,17.516707801818846,17.49245538711548,17.468304920196537,17.44425506591797,17.420304489135745,17.39645185470581,17.37269582748413,17.34903516769409,17.32546854019165,17.3019944190979,17.278611755371095,17.25531930923462,17.23211555480957,17.208999347686767,17.185969257354735,17.163024044036867,17.14016227722168,17.117382812499997,17.094684314727782,17.072065448760984,17.049524879455564,17.02706146240234,17.00467376708984,16.982360649108884,16.96012086868286,16.937953090667726,16.915856170654298,16.893828773498537,16.87186965942383,16.849977588653566,16.828151226043705,16.80638942718506,16.78469104766846,16.763054752349856,16.7414794921875,16.719963836669923,16.69850664138794,16.677106857299805,16.655763053894045,16.63447437286377,16.61323938369751,16.592056941986083,16.570925903320315,16.549845027923585,16.52881326675415,16.5078293800354,16.48689212799072,16.4660005569458,16.445153331756593,16.424349498748782,16.403587913513185,16.382867336273193,16.36218671798706,16.341544914245606,16.320940589904787,16.300372791290282,16.279840469360355,16.25934238433838,16.23887767791748,16.21844482421875,16.198043060302734,16.177671194076538,16.157328128814697,16.13701276779175,16.11672396659851,16.096460580825806,16.07622175216675,16.056006145477298,16.035812854766846,16.01564073562622,15.995488500595092,15.975355339050294,15.955240058898926,15.93514156341553,15.915058898925784,15.894991064071657,15.874936771392825,15.854894971847536,15.83486466407776,15.814844942092897,15.794834566116334,15.77483253479004,15.754837799072266,15.734849309921264,15.714865922927855,15.69488673210144,15.674910593032836,15.654936456680296,15.634963369369506,15.614990186691283,15.595016050338744,15.575039720535278,15.555060291290285,15.535076665878298,15.515087938308715,15.495092964172363,15.47509093284607,15.455080604553224,15.435060977935793,15.415031147003177,15.394990062713626,15.374936628341677,15.354870033264163,15.334789228439332,15.31469316482544,15.294580984115601,15.274451541900635,15.254303932189943,15.234137248992923,15.213950443267823,15.193742656707764,15.173512792587282,15.153259992599489,15.132983350753785,15.112681818008422,15.092354488372802,15.072000360488891,15.0516188621521,15.031208801269534,15.010769224166873,14.990299367904665,14.969798374176028,14.949265289306643,14.92869920730591,14.908099412918093,14.887464809417725,14.86679468154907,14.846088218688964,14.825344657897949,14.80456314086914,14.783742713928223,14.76288275718689,14.741982316970825,14.721040868759157,14.700057506561281,14.67903141975403,14.657961988449099,14.636848402023318,14.615689992904665,14.594486141204834,14.573236083984376,14.551939058303834,14.53059449195862,14.509201765060425,14.48776035308838,14.46626949310303,14.444728803634645,14.42313756942749,14.401495075225831,14.379801082611085,14.358054876327516,14.336256170272828,14.314404392242432,14.292498779296874,14.270539283752441,14.248525381088259,14.226456594467164,14.204332542419435,14.182153034210208,14.15991749763489,14.137625694274904,14.115277528762821,14.092872524261477,14.070410442352296,14.047891092300418,14.025314378738406,14.002679920196533,13.979987716674804,13.9572377204895,13.934429550170897,13.911563444137572,13.888639116287232,13.865656661987307,13.842616176605224,13.819517469406131,13.796360826492313,13.773146343231204,13.74987397193909,13.726543998718265,13.703156566619876,13.679711866378785,13.656210088729859,13.632651567459108,13.609036540985107,13.585365390777588,13.561638450622558,13.537856149673464,13.514018869400028,13.490126943588258,13.466180992126466,13.442181539535524,13.418129110336304,13.394024229049682,13.369867324829102,13.345659351348878,13.321400785446167,13.29709234237671,13.272734737396242,13.2483286857605,13.223874998092652,13.199374532699585,13.174828147888183,13.150236701965332,13.125601148605346,13.100922203063964,13.076201009750367,13.051438522338868,13.026635694503785,13.001793718338014,12.97691354751587,12.951996231079102,12.927043008804322,12.902054977416993,12.877033424377442,12.851979494094849,12.826894330978394,12.80177927017212,12.776635599136352,12.751464653015137,12.7262677192688,12.701046180725097,12.675801277160646,12.650534439086913,12.625247240066528,12.599941062927247,12.574617290496827,12.549277353286742,12.523922872543334,12.498555278778076,12.473176193237306,12.44778699874878,12.42238931655884,12.396984672546388,12.371574878692629,12.34616117477417,12.320745372772219,12.29532895088196,12.269913673400877,12.244501066207885,12.219092798233032,12.193690490722657,12.168295907974244,12.142910671234132,12.117536401748657,12.092174625396726,12.06682720184326,12.041495704650877,12.016181802749633,11.99088716506958,11.965613460540771,11.940362405776977,11.91513557434082,11.88993468284607,11.864761304855346,11.839617109298707,11.814503717422486,11.789422798156737,11.764376020431518,11.739364862442017,11.714390993118286,11.689456033706666,11.664561414718628,11.639708805084227,11.614899682998656,11.590135717391966,11.565418243408201,11.540748929977417,11.516129207611085,11.49156050682068,11.467044258117676,11.442581939697266,11.41817488670349,11.39382462501526,11.369532394409179,11.345299577713012,11.321127510070799,11.297017526626586,11.272970771789552,11.248988580703736,11.2250723361969,11.201223039627077,11.177441883087159,11.15373010635376,11.130088806152346,11.106519079208375,11.083022069931031,11.05959873199463,11.036250114440918,11.012977266311646,10.989781093597413,10.966662597656251,10.943622684478761,10.920662164688112,10.897781896591189,10.874982881546021,10.852265739440918,10.829631376266478,10.807080459594726,10.784613800048827,10.76223187446594,10.739935541152953,10.717725467681884,10.695602178573608,10.673566102981567,10.6516179561615,10.629758214950561,10.607987451553345,10.58630599975586,10.564714384078979,10.543213033676146,10.521802330017088,10.500482559204102,10.47925410270691,10.458117198944093,10.43707218170166,10.416119432449342,10.39525899887085,10.374491214752197,10.3538161277771,10.333233976364134,10.312744855880736,10.29234890937805,10.272046184539796,10.25183687210083,10.231720924377441,10.21169834136963,10.19176907539368,10.171933269500734,10.1521906375885,10.13254132270813,10.112985086441041,10.093521928787231,10.074151706695558,10.054874372482299,10.035689640045167,10.016597318649293,9.997597169876098,9.978689098358155,9.959872817993165,9.941148138046266,9.922514820098879,9.90397243499756,9.88552083969116,9.86715965270996,9.848888635635376,9.83070740699768,9.812615633010864,9.794612932205201,9.776699018478395,9.75887336730957,9.741135692596435,9.723485612869263,9.705922651290894,9.688446331024172,9.671056365966798,9.653752231597903,9.636533594131471,9.619399833679202,9.60235061645508,9.585385370254519,9.568503618240358,9.55170497894287,9.534988832473754,9.518354749679565,9.501802206039429,9.485330629348756,9.468939685821535,9.452628660202027,9.43639717102051,9.420244646072387,9.404170513153076,9.3881742477417,9.3722553730011,9.3564133644104,9.340647554397583,9.324957418441773,9.30934247970581,9.293802070617675,9.27833571434021,9.262942886352539,9.247622966766357,9.232375478744506,9.21719980239868,9.202095460891723,9.187061786651613,9.17209839820862,9.157204437255862,9.142379570007325,9.127623224258421,9.112934780120849,9.098313760757447,9.083759546279909,9.069271564483644,9.054849338531495,9.040492248535157,9.026199865341187,9.011971473693848,8.99780673980713,8.98370499610901,8.969665718078613,8.955688428878785,8.9417724609375,8.927917337417602,8.914122581481934,8.900387620925903,8.886711883544923,8.873095083236695,8.859536504745485,8.8460355758667,8.832591915130617,8.819204950332644,8.805874252319336,8.792599248886109,8.7793794631958,8.766214418411256,8.753103590011598,8.7400465965271,8.727042865753175,8.71409192085266,8.701193380355836,8.688346576690675,8.675551223754882,8.662806844711303,8.650112867355347,8.637468957901001,8.624874639511109,8.612329339981079,8.599832725524903,8.587384414672853,8.574983882904052,8.562630796432495,8.550324630737306,8.538065004348756,8.525851440429689,8.513683748245239,8.50156126022339,8.489483642578126,8.477450513839722,8.465461444854737,8.453516054153441,8.441614055633545,8.429755020141602,8.41793851852417,8.40616421699524,8.394431686401367,8.382740592956543,8.371090602874755,8.359481382369996,8.34791250228882,8.336383628845216,8.324894380569459,8.313444423675538,8.30203356742859,8.290661382675172,8.279327392578125,8.268031454086303,8.256773185729982,8.24555230140686,8.234368467330933,8.223221397399904,8.212110710144044,8.201036071777343,8.189997291564943,8.17899398803711,8.16802592277527,8.157092809677124,8.146194362640381,8.135330295562744,8.124500274658203,8.113704109191895,8.102941465377809,8.092212128639222,8.08151571750641,8.070852088928223,8.060220980644226,8.049622106552125,8.03905520439148,8.028520011901856,8.018016362190247,8.007543873786927,7.997102427482606,7.986691784858704,7.976311659812927,7.965961885452272,7.955642175674439,7.945352339744568,7.935092115402222,7.924861311912537,7.914659738540649,7.904487180709839,7.894343376159668,7.884228205680848,7.8741414070129405,7.864082789421083,7.854052138328554,7.8440493106842055,7.834074044227601,7.824126195907594,7.814205551147461,7.804311966896057,7.794445204734802,7.784605145454407,7.774791550636292,7.765004253387452,7.755243182182312,7.745508050918579,7.735798716545105,7.726115012168885,7.71645679473877,7.7068238973617555,7.6972162246704094,7.687633442878722,7.67807559967041,7.668542432785035,7.659033799171447,7.649549531936645,7.640089607238769,7.630653738975525,7.62124183177948,7.611853742599487,7.6024893760681165,7.593148493766785,7.583831095695495,7.574536991119385,7.5652659893035885,7.556018018722534,7.546792984008788,7.537590742111206,7.528411102294922,7.519253969192505,7.510119199752808,7.501006841659546,7.491916584968567,7.482848405838013,7.473802161216736,7.4647777557373045,7.455775046348571,7.446793961524963,7.43783438205719,7.428896188735961,7.419979286193848,7.411083602905274,7.402208971977235,7.393355321884156,7.384522557258607,7.375710558891297,7.366919231414795,7.358148503303527,7.349398303031922,7.340668511390686,7.331959009170532,7.323269724845886,7.314600563049316,7.305951452255249,7.297322297096251,7.28871304988861,7.280123519897461,7.271553707122803,7.263003540039064,7.254472899436952,7.2459617376327525,7.237469983100892,7.228997468948365,7.220544171333314,7.212110018730165,7.2036950349807745,7.19529905319214,7.1869219779968265,7.178563761711121,7.170224308967591,7.161903619766235,7.153601503372193,7.145317959785461,7.137052941322326,7.128806376457214,7.120578122138977,7.112368273735046,7.104176616668702,7.096003127098084,7.087847733497619,7.079710364341735,7.071590948104857,7.06348955631256,7.055406045913696,7.047340369224549,7.039292335510255,7.031262016296387,7.023249316215517,7.015254235267641,7.007276678085328,6.999316596984864,6.991373920440674,6.983448600769043,6.975540566444396,6.967649745941162,6.959776186943055,6.951919698715211,6.944080328941347,6.936257982254029,6.928452682495117,6.9206643342971805,6.912892866134644,6.905138230323793,6.8974004030227665,6.889679288864136,6.881974887847901,6.874287152290345,6.866616034507752,6.858961462974549,6.851323366165162,6.843701791763306,6.836096644401551,6.82850787639618,6.820935463905334,6.8133793592453005,6.8058395147323605,6.798315906524658,6.79080843925476,6.783317184448243,6.775841951370239,6.768382787704468,6.76093966960907,6.753512477874756,6.746101212501526,6.738705825805664,6.731326293945313,6.7239625453948975,6.716614556312562,6.709282326698305,6.701965856552126,6.694664978981019,6.68737976551056,6.680110120773317,6.672856020927431,6.665617418289186,6.658394289016725,6.6511866569519045,6.643994426727295,6.63681755065918,6.62965602874756,6.622509765625002,6.615378761291506,6.608262991905215,6.601162457466127,6.59407708644867,6.587006878852844,6.579951739311218,6.572911715507508,6.565886688232423,6.5588766813278205,6.551881647109986,6.544901537895203,6.537936377525329,6.5309860706329355,6.524050641059876,6.517129993438721,6.510224080085755,6.50333297252655,6.496456575393676,6.489594864845275,6.482747840881348,6.47591543197632,6.469097685813904,6.462294483184815,6.455505800247193,6.448731637001037,6.4419719696044915,6.435226726531981,6.428495931625366,6.421779537200928,6.415077471733094,6.408389759063721,6.401716399192811,6.395057249069215,6.388412427902222,6.381781792640687,6.375165367126466,6.368563103675843,6.3619749546051025,6.355400943756104,6.348841047286987,6.34229519367218,6.335763311386108,6.329245471954345,6.322741675376892,6.316251826286316,6.309775924682618,6.303313899040223,6.29686577320099,6.290431451797487,6.284010958671572,6.277604293823243,6.271211338043213,6.264832210540772,6.258466720581056,6.252115011215211,6.245776915550232,6.239452481269836,6.23314163684845,6.226844429969788,6.220560741424561,6.2142906427383435,6.208034086227418,6.201790928840638,6.195561242103578,6.189345002174378,6.183142137527467,6.176952743530274,6.170776677131654,6.164613962173462,6.158464598655701,6.152328515052796,6.146205663681031,6.140096044540407,6.133999633789064,6.127916383743287,6.121846365928651,6.115789461135864,6.1097456932067855,6.1037149906158445,6.097697401046753,6.09169282913208,6.0857012510299695,6.07972276210785,6.073757195472718,6.067804574966431,6.061864900588989,6.055938100814819,6.0500241994857795,6.044123148918153,6.038234925270081,6.032359504699708,6.026496887207032,6.020647025108338,6.014809942245484,6.008985567092896,6.003173851966859,5.99737482070923,5.991588425636293,5.985814666748047,5.980053496360779,5.974304914474487,5.968568897247315,5.962845349311829,5.957134366035462,5.951435852050782,5.945749807357789,5.940076184272766,5.934414958953857,5.928766179084778,5.923129725456238,5.917505621910096,5.911893892288209,5.906294465065002,5.900707268714905,5.895132327079773,5.889569640159607,5.884019136428833,5.878480839729309,5.872954702377319,5.867440700531006,5.86193881034851,5.8564490318298335,5.850971364974976,5.845505714416504,5.840052080154418,5.834610486030579,5.829180860519409,5.823763179779053,5.818357419967651,5.812963604927062,5.807581663131714,5.802211570739746,5.79685332775116,5.791506934165955,5.7861723661422735,5.7808495521545415,5.775538539886475,5.7702392578125,5.7649516582489015,5.759675765037537,5.754411578178406,5.749159026145936,5.743918108940125,5.73868877887726,5.733471059799196,5.728264856338502,5.723070216178894,5.717887115478516,5.7127154827117925,5.707555341720581,5.702406644821167,5.69726939201355,5.692143583297729,5.687029147148132,5.681926059722901,5.676834321022034,5.671753931045534,5.666684794425965,5.6616269826889045,5.656580376625061,5.651544976234437,5.646520829200745,5.641507840156556,5.636506056785585,5.6315153837204,5.626535868644716,5.621567440032959,5.616610097885132,5.611663818359375,5.606728577613831,5.601804375648499,5.596891140937807,5.5919888734817516,5.587097573280335,5.582217168807984,5.577347731590271,5.572489094734192,5.567641401290894,5.562804508209228,5.557978487014771,5.553163290023804,5.548358821868897,5.543565130233764,5.538782143592834,5.534009909629821,5.529248356819153,5.524497485160827,5.519757270812988,5.515027689933778,5.510308694839478,5.5056002855300905,5.500902485847474,5.496215200424194,5.491538453102112,5.486872220039368,5.482216429710388,5.477571153640747,5.472936296463012,5.468311858177185,5.463697814941406,5.459094142913817,5.454500818252564,5.449917840957642,5.445345187187195,5.440782809257507,5.436230707168579,5.4316888570785515,5.42715721130371,5.422635769844055,5.4181245803833,5.4136235237121575,5.409132575988769,5.40465178489685,5.400181078910828,5.395720458030701,5.391269874572753,5.386829352378845,5.382398843765259,5.377978372573853,5.373567843437196,5.369167256355286,5.364776635169984,5.360395932197571,5.356025099754334,5.351664185523987,5.347313117980957,5.3429718732833855,5.338640427589415,5.33431875705719,5.330006861686707,5.325704741477967,5.321412348747255,5.317129659652711,5.312856674194337,5.308593344688416,5.304339671134949,5.300095629692079,5.2958611965179445,5.291636300086975,5.287421011924745,5.283215260505677,5.279019021987915,5.274832272529602,5.2706550598144535,5.266487264633179,5.2623289108276365,5.258179998397828,5.254040527343751,5.24991044998169,5.24578971862793,5.241678309440614,5.237576222419739,5.23348343372345,5.22939989566803,5.225325679779052,5.221260690689086,5.217204880714416,5.213158297538756,5.209120917320251,5.2050926923751835,5.201073598861695,5.197063636779786,5.193062782287598,5.189070987701416,5.18508825302124,5.181114554405213,5.177149891853334,5.173194217681886,5.169247508049013,5.165309762954713,5.161380958557129,5.157461094856263,5.153550124168397,5.149648046493531,5.145754837989807,5.141870450973511,5.137994909286499,5.134128165245056,5.130270195007324,5.126420974731445,5.12258050441742,5.118748784065247,5.114925765991211,5.111111426353455,5.107305765151978,5.10350878238678,5.099720382690429,5.0959406137466425,5.0921694278717045,5.088406801223755,5.084652733802796,5.080907201766967,5.077170181274415,5.073441672325135,5.069721579551697,5.066009998321533,5.06230685710907,5.058612108230591,5.054925751686096,5.051247787475585,5.047578215599059,5.043916964530943],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"dash\":\"dash\",\"width\":5},\"mode\":\"lines\",\"name\":\"Averaged Test Loss\",\"x\":[19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[18.428421783447266,18.418853187561037,18.409351348876953,18.399917316436767,18.39055233001709,18.381257915496825,18.372035026550293,18.362884426116945,18.353806495666504,18.344801712036137,18.335870265960693,18.32701187133789,18.318226909637453,18.309515190124515,18.300876235961915,18.292309856414793,18.283815670013432,18.275393104553228,18.26704168319702,18.25876054763794,18.250549030303958,18.24240674972534,18.234332656860353,18.226325702667236,18.21838541030884,18.210510635375975,18.202700519561766,18.194954204559327,18.187270641326908,18.17964897155762,18.172088050842287,18.164587116241456,18.15714502334595,18.14976091384888,18.14243392944336,18.135162925720216,18.127946853637695,18.12078504562378,18.113676261901862,18.106619739532473,18.099614524841307,18.092659282684327,18.085753631591796,18.078896617889402,18.0720871925354,18.065324592590333,18.05860815048218,18.051936626434326,18.04530954360962,18.03872585296631,18.032184886932374,18.0256856918335,18.019227600097658,18.01280975341797,18.00643119812012,18.000091361999516,17.993789196014408,17.987524032592773,17.981295013427737,17.975101375579836,17.96894216537476,17.962816905975345,17.95672426223755,17.95066375732422,17.944634342193602,17.938635349273678,17.932665634155274,17.92672472000122,17.920811462402344,17.914925193786623,17.909065055847172,17.9032301902771,17.89741954803467,17.891632366180424,17.88586769104004,17.880124473571776,17.87440242767334,17.86869993209839,17.863016510009768,17.85735092163086,17.851702594757082,17.846070289611816,17.840453338623046,17.83485059738159,17.829261207580565,17.823684215545654,17.81811866760254,17.812563705444337,17.807018280029297,17.801481437683105,17.79595212936401,17.79042959213257,17.784912681579588,17.779400539398193,17.77389211654663,17.768386745452883,17.762882804870607,17.757379913330077,17.751876735687254,17.74637260437012,17.740866279602052,17.73535680770874,17.7298433303833,17.724324798583986,17.718800163269044,17.713268470764163,17.70772876739502,17.70217990875244,17.69662094116211,17.69105100631714,17.685469055175783,17.679873847961428,17.67426471710205,17.668640327453616,17.663000011444094,17.657342529296876,17.65166711807251,17.645972442626956,17.64025754928589,17.634521579742433,17.628763294219972,17.62298173904419,17.617175960540774,17.611344814300537,17.605487537384032,17.59960289001465,17.593689823150637,17.587747287750243,17.581774520874024,17.57577018737793,17.56973342895508,17.56366319656372,17.557558155059816,17.55141763687134,17.54524040222168,17.539025402069093,17.532771587371826,17.52647800445557,17.520143699646,17.513767433166507,17.507348346710206,17.50088520050049,17.494376945495606,17.48782262802124,17.481221199035645,17.474571514129636,17.467872619628906,17.461123561859132,17.454323101043702,17.447470283508302,17.440563964843747,17.433603286743164,17.426587200164796,17.419514656066895,17.412384605407713,17.405195903778075,17.397947502136226,17.390638637542725,17.383268070220947,17.375834846496584,17.368337821960452,17.36077632904053,17.353149223327637,17.34545545578003,17.337693977355958,17.329864025115967,17.321964454650878,17.31399431228638,17.30595283508301,17.29783887863159,17.289651679992676,17.28138999938965,17.27305326461792,17.26464033126831,17.25615043640137,17.247582626342776,17.23893632888794,17.23021020889282,17.221403789520263,17.21251602172852,17.203546237945556,17.194493675231936,17.185357189178468,17.176136398315432,17.166830444335936,17.157438468933105,17.14796009063721,17.138394165039063,17.128740215301516,17.118997669219972,17.10916566848755,17.099243736267088,17.089231395721434,17.079127883911134,17.068932723999023,17.058645534515385,17.048265457153324,17.037792205810543,17.02722520828247,17.016564178466798,17.0058087348938,16.99495830535889,16.98401288986206,16.9729718208313,16.96183490753174,16.950601959228518,16.93927240371704,16.92784662246704,16.916324043273928,16.904704570770264,16.89298810958862,16.881174850463864,16.86926431655884,16.857256698608396,16.845151805877688,16.832949829101565,16.820651054382328,16.80825548171997,16.795763301849362,16.783174610137937,16.770489597320555,16.757708740234374,16.744832229614257,16.731860446929932,16.718793678283692,16.705632305145265,16.6923770904541,16.679028129577638,16.665586090087892,16.652051639556884,16.638425540924075,16.62470808029175,16.610900211334233,16.597002506256107,16.583016014099123,16.568941307067874,16.554779148101808,16.540530490875245,16.52619638442993,16.511777782440188,16.49727554321289,16.482690715789794,16.468024349212644,16.453277778625488,16.43845205307007,16.423548412323,16.40856800079346,16.39351215362549,16.378382205963135,16.363179302215578,16.347904777526857,16.33256015777588,16.317146873474123,16.301666450500488,16.286120319366454,16.270510005950925,16.254837131500242,16.239103317260742,16.22331008911133,16.20745906829834,16.19155225753784,16.175590991973877,16.159577274322512,16.143512678146365,16.127399015426636,16.111238193511962,16.095031833648683,16.078782081604004,16.06249055862427,16.046159315109254,16.029790353775024,16.013385581970216,15.99694685935974,15.980476140975954,15.963975477218629,15.947446775436404,15.93089213371277,15.914313554763796,15.89771308898926,15.881092882156373,15.864454650878907,15.847800827026367,15.831133222579957,15.81445393562317,15.797765254974365,15.781068992614747,15.764367580413818,15.747662782669067,15.730956888198852,15.714251947402953,15.697549819946287,15.680852746963499,15.664162778854369,15.647482013702392,15.6308123588562,15.614156103134155,15.597515058517455,15.580891418457032,15.564287185668945,15.547704219818117,15.531144666671755,15.514610433578492,15.498103523254395,15.481625986099244,15.465179538726808,15.44876618385315,15.43238778114319,15.416046190261842,15.39974322319031,15.383480930328371,15.367260932922363,15.351085090637207,15.334955072402954,15.318872690200806,15.302839756011963,15.286857795715333,15.270928573608398,15.255053615570068,15.239234495162965,15.223472833633423,15.207770395278933,15.192128467559815,15.176548624038697,15.161032247543336,15.145580816268922,15.130195713043216,15.114878273010255,15.099629878997803,15.084451961517333,15.069345378875733,15.054311752319338,15.039352035522462,15.024467658996583,15.009659576416016,14.994928884506226,14.980276727676392,14.965704202651976,14.951212215423583,14.936801767349241,14.922473716735839,14.908228969573974,14.89406852722168,14.879993057250978,14.866003513336182,14.852100610733032,14.838285064697265,14.82455768585205,14.81091890335083,14.797369384765625,14.783909988403321,14.77054114341736,14.757263469696046,14.744077301025392,14.730983161926272,14.717981576919556,14.705072927474976,14.692257452011109,14.67953577041626,14.666908073425295,14.65437469482422,14.6419358253479,14.62959156036377,14.617342424392701,14.605188417434695,14.593129682540896,14.581166505813602,14.569298744201662,14.557526779174808,14.545850467681886,14.534269857406615,14.522784900665282,14.511395597457886,14.500101900100708,14.488903951644899,14.47780137062073,14.466794204711913,14.455882263183593,14.445065355300901,14.434343290328977,14.42371606826782,14.413183307647703,14.402744960784911,14.392400550842286,14.382149887084962,14.371992778778075,14.361928653717042,14.351957464218142,14.342078876495364,14.332292413711551,14.322597837448122,14.312994623184204,14.303482389450073,14.294060802459716,14.284729385375977,14.275487899780275,14.266335725784304,14.257272529602051,14.248297786712648,14.239411067962646,14.23061170578003,14.221899461746217,14.21327362060547,14.204733753204346,14.19627938270569,14.187910032272338,14.179625129699707,14.171424055099488,14.163306427001952,14.155271482467652,14.14731869697571,14.139447689056398,14.131657934188844,14.123948621749879,14.116319322586062,14.108769369125367,14.101298141479493,14.09390516281128,14.086589717864992,14.079351282119752,14.072189378738406,14.065103244781495,14.058092355728151,14.051156139373782,14.044293832778934,14.037504768371583,14.03078875541687,14.024144887924194,14.017572498321535,14.011071062088014,14.00464005470276,13.99827871322632,13.991986417770386,13.985762786865235,13.979607009887697,13.973518466949464,13.967496776580811,13.961541080474854,13.955650901794435,13.949825716018678,13.944064712524414,13.938367319107057,13.932733011245729,13.927161121368409,13.921651268005371,13.916202831268313,13.910814952850341,13.905487251281738,13.900218963623047,13.895009851455688,13.889859056472778,13.884766101837158,13.879730415344238,13.874751377105712,13.869828462600708,13.864961051940918,13.86014852523804,13.855390548706058,13.850686454772951,13.84603548049927,13.841437435150148,13.83689160346985,13.832397651672364,13.82795491218567,13.823562717437744,13.819220542907715,13.814928007125854,13.81068458557129,13.806489801406862,13.80234308242798,13.798243856430053,13.794191694259643,13.790186119079591,13.786226558685303,13.782312488555908,13.778443670272827,13.774619436264038,13.770839262008666,13.767102766036988,13.76340961456299,13.759759092330933,13.756150913238526,13.752584409713746,13.749059247970584,13.74557504653931,13.742131471633913,13.73872790336609,13.73536376953125,13.732038974761963,13.728752851486206,13.725505113601685,13.722295188903809,13.719122838974,13.715987586975098,13.712889051437378,13.709826707839966,13.706800317764284,13.703809499740602,13.700853681564332,13.69793257713318,13.695045852661135,13.692193174362185,13.689374160766603,13.686588335037232,13.683835363388063,13.681114816665652,13.678426456451417,13.675769996643067,13.673144865036011,13.670550918579103,13.667987728118899,13.665455102920534,13.662952518463136,13.66047968864441,13.658036375045777,13.65562219619751,13.653236818313601,13.650879859924316,13.6485511302948,13.646250343322754,13.643977069854737,13.641730976104737,13.639511919021608,13.637319564819338,13.635153627395631,13.633013725280765,13.630899715423586,13.628811264038086,13.626747989654541,13.62470965385437,13.622696113586427,13.620707035064697,13.618742036819459,13.616801071166995,13.614883708953858,13.612989807128908,13.611118984222413,13.609271144866943,13.607445955276487,13.60564317703247,13.603862619400024,13.602104043960571,13.600367069244385,13.598651552200316,13.596957302093506,13.595284128189085,13.593631649017333,13.5919997215271,13.590388202667238,13.588796854019167,13.587225341796877,13.585673475265505,13.584141159057618,13.58262810707092,13.581134223937989,13.579659175872804,13.578202819824218,13.576764869689942,13.575345325469971,13.57394380569458,13.572560214996338,13.571194314956664,13.569845962524415,13.568515014648437,13.567201185226441,13.565904378890991,13.564624404907226,13.563361072540284,13.562114191055297,13.560883569717408,13.559669113159181,13.558470630645754,13.557287931442263,13.556120729446413,13.55496897697449,13.553832530975344,13.55271134376526,13.551605224609375,13.550513792037963,13.54943709373474,13.54837498664856,13.547327232360841,13.546293687820436,13.545274305343629,13.544268798828124,13.543277120590211,13.542299127578735,13.541334629058838,13.540383529663087,13.539445781707766,13.53852105140686,13.537609338760376,13.536710500717163,13.535824489593507,13.534951066970828,13.53409013748169,13.533241510391237,13.532404994964601,13.53158073425293,13.530768442153931,13.529967975616456,13.529179239273072,13.528402233123781,13.527636575698855,13.526882457733157,13.52613959312439,13.525407838821412,13.524687194824219,13.523977518081667,13.523278617858889,13.522590494155885,13.521913051605226,13.521246099472048,13.52058963775635,13.519943475723268,13.519307518005375,13.518681716918948,13.518065977096558,13.517460155487061,13.516864061355593,13.516277742385865,13.515701055526733,13.51513385772705,13.514576244354249,13.514027929306032,13.513488960266116,13.512959146499636,13.51243839263916,13.511926651000978,13.511423826217655,13.510929918289188,13.510444641113283,13.509968137741089,13.509500122070314,13.509040641784667,13.508589601516723,13.508146905899046,13.507712411880492,13.507286071777344,13.506867980957031,13.506457805633547,13.506055641174317,13.505661344528198,13.505274820327758,13.504895973205565,13.504524660110473,13.504161071777345,13.503805017471315,13.503456354141234,13.503115129470824,13.502781105041503,13.502454328536986,13.502134704589842,13.501822185516357,13.501516580581665,13.501218032836913,13.50092635154724,13.500641584396364,13.500363540649415,13.500092029571533,13.4998272895813,13.49956912994385,13.499317502975467,13.499072265625001,13.498833465576173,13.498601102828978,13.498374891281127,13.498154830932616,13.497941017150879,13.49773325920105,13.497531509399414,13.497335863113404,13.497146034240723,13.496962213516236,13.496784210205078,13.496611833572388,13.496445178985596,13.496284198760987,13.49612879753113,13.495979070663454,13.495834732055666,13.495695829391481,13.495562410354616,13.495434331893923,13.495311546325684,13.495193910598756,13.495081520080566,13.494974374771118,13.494872283935546,13.494775199890135,13.494683218002319,13.4945960521698,13.494513893127444,13.49443645477295,13.494363927841189,13.494296264648439,13.494233322143554,13.494175004959105,13.494121360778808,13.494072341918946,13.494027948379518,13.493987989425658,13.493952417373658,13.493921375274658,13.493894720077515,13.493872451782227,13.493854522705078,13.493840885162355,13.493831396102907,13.493826150894167,13.493825006484986,13.493828058242798,13.493835163116454,13.493846321105954,13.493861484527587,13.493880558013915,13.493903541564938,13.493930435180662,13.493961238861083,13.493995809555052,13.494034099578858,13.494076251983643,13.494122123718263,13.494171667099002,13.494224834442141,13.494281625747684,13.49434189796448,13.494405698776246,13.494473028182984,13.494543886184692,13.494618225097655,13.494695949554444,13.494777011871337,13.494861507415774,13.494949340820314,13.495040416717531,13.49513478279114,13.495232343673708,13.495333099365235,13.495436954498292,13.495544004440308,13.495654153823853,13.495767402648926,13.495883560180664,13.496002817153931,13.496124935150146,13.496250200271605,13.496378326416014,13.496509456634522,13.496643304824829,13.496780061721802,13.496919584274293,13.497061967849733,13.497207069396973,13.497354745864868,13.497505283355713,13.497658491134645,13.497814416885378,13.497972869873047,13.498133993148805,13.49829773902893,13.49846396446228,13.498632669448853,13.498803949356079,13.498977708816529,13.499154043197633,13.499332714080811,13.499513816833497,13.499697208404543,13.49988307952881,13.500071144104005,13.500261640548707,13.500454378128053,13.500649404525756,13.500846767425537,13.501046276092529,13.50124797821045,13.501451873779299,13.501657962799074,13.501866197586061,13.502076530456545,13.502288913726808,13.502503395080568,13.502719974517822,13.502938556671143,13.503159141540529,13.503381633758545,13.503606128692628,13.503832626342774,13.504060983657839,13.504291391372682,13.504523563385012,13.504757642745973,13.504993534088136,13.505231285095217,13.505470895767214,13.50571222305298,13.50595531463623,13.506200122833253,13.506446743011477,13.506695032119753,13.506945085525516,13.50719680786133,13.507450246810915,13.507705307006837,13.507961893081667,13.508220195770264,13.508480072021483,13.508741521835328,13.509004497528077,13.509268951416018,13.509535074234012,13.509802627563479,13.510071659088137,13.510342168807986,13.51061410903931,13.510887479782108,13.511162328720095,13.511438655853274,13.51171627044678,13.51199531555176,13.512275695800783,13.512557458877565,13.512840509414673,13.513124847412108,13.51341061592102,13.51369767189026,13.513985919952393,13.514275407791137,13.514566135406493,13.514858150482178,13.515151262283327,13.515445661544803,13.515741252899172,13.516037988662719,13.51633586883545,13.516634941101074,13.516935110092163,13.517236423492433,13.517538881301881,13.517842292785646,13.51814684867859,13.518452405929567,13.51875901222229,13.519066715240479,13.519375371932984,13.519685029983522,13.519995784759523,13.520307445526123,13.520620155334473,13.520933771133423,13.521248388290406,13.521563911437989,13.521880388259888,13.522197771072388,13.522516012191772,13.522835111618042,13.523155069351196,13.523475885391235,13.523797512054445,13.52412009239197,13.52444348335266,13.524767732620239,13.525092697143554,13.525418472290038,13.525745010375976,13.526072263717651,13.526400327682495,13.52672920227051,13.52705888748169,13.527389192581177,13.527720212936403,13.528051948547365,13.528384304046632,13.528717374801637,13.529051160812378,13.52938561439514,13.529720783233643,13.530056524276732,13.530392932891845,13.530729913711548,13.531067562103273,13.53140573501587,13.531744527816773,13.532083988189699,13.532423973083498,13.532764625549317,13.533105659484866,13.533447313308718,13.533789348602296,13.534132051467896,13.534475278854371,13.534818935394288,13.535163164138796,13.535507822036745,13.535853004455568,13.536198711395267,13.53654479980469,13.536891460418703,13.537238502502442,13.537585973739624,13.537933826446535,13.538282203674319,13.538631010055543,13.538980102539064,13.539329719543456,13.53967967033386,13.540030097961424,13.540380811691284,13.54073190689087,13.541083335876467,13.54143524169922,13.541787481307985,13.542139959335326,13.542492866516112,13.542845964431763,13.54319944381714,13.543553256988528,13.543907213211062,13.544261598587038,13.544616270065308,13.54497117996216,13.545326375961304,13.54568181037903,13.54603753089905,13.546393394470215,13.546749448776248,13.547105741500856,13.54746232032776,13.547819185256959,13.548176288604736,13.548533487319947,13.548890829086304,13.549248552322387,13.549606418609619,13.549964523315431,13.550322771072388,13.55068106651306,13.551039600372317,13.551398229598998,13.551757049560548,13.552116012573242,13.552475070953367,13.552834320068358,13.553193712234496,13.553553199768066,13.553912830352784,13.554272603988649,13.55463252067566,13.554992485046387,13.555352449417116,13.555712509155274,13.556072807312013,13.55643301010132,13.556793212890627,13.557153511047364,13.557513761520386,13.557874202728273,13.558234691619873,13.558595228195191,13.558955812454224,13.559316492080688,13.559677124023438,13.560037755966187,13.560398387908936,13.5607590675354,13.56111969947815,13.5614803314209,13.561840963363647,13.562201547622681,13.562562131881714,13.562922716140747,13.563283205032349,13.56364369392395,13.564004278182983,13.564364767074585,13.564725255966186,13.56508560180664,13.565445947647095,13.565806150436401,13.566166257858278,13.566526269912721],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average batch mean loss during training from dataset with 792 users and 657 books\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('0cb96cd1-e448-4f8f-887f-9cd182c026ba');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = [item[0] for item in losses]\n",
    "test_loss = [item[1] for item in losses]\n",
    "\n",
    "window_size = 20\n",
    "train_loss_avg = np.convolve(\n",
    "    train_loss,\n",
    "    np.ones(window_size) / window_size,\n",
    "    mode='valid')\n",
    "test_loss_avg = np.convolve(\n",
    "    test_loss,\n",
    "    np.ones(window_size) / window_size,\n",
    "    mode='valid')\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=list(range(len(losses))),\n",
    "                         y=train_loss, mode='lines', name='Train Loss'))\n",
    "fig.add_trace(go.Scatter(x=list(range(len(losses))),\n",
    "                         y=test_loss, mode='lines', name='Test Loss'))\n",
    "fig.add_trace(go.Scatter(x=list(range(window_size-1, len(losses))),\n",
    "                         y=train_loss_avg,\n",
    "                         mode='lines', name='Averaged Train Loss',\n",
    "                         line=dict(dash='dash', width=5, color='blue')))\n",
    "fig.add_trace(go.Scatter(x=list(range(window_size-1, len(losses))),\n",
    "                         y=test_loss_avg,\n",
    "                         mode='lines', name='Averaged Test Loss',\n",
    "                         line=dict(dash='dash', width=5, color='red')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Average batch mean loss during training\\\n",
    " from dataset with {users_count} users and {book_count} books',\n",
    "    xaxis_title='Epoch',\n",
    "    yaxis_title='Loss'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zQTli3Fn0pO"
   },
   "source": [
    "## Small recommendation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs5W5tG5n0Bz",
    "outputId": "8cd1de24-9908-4fee-fe09-284bd3839c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(657, 35)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(embeddings_save_path, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "loaded_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "26otDUqQT0gn",
    "outputId": "851f109b-fd69-4872-ac1d-7e3567008b67"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c45bf60a-ca81-4914-8582-46368ade8788\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book-Embedding-ID</th>\n",
       "      <th>Author-Embedding-ID</th>\n",
       "      <th>Year-Embedding-ID</th>\n",
       "      <th>Book-ID</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Year-Of-Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image-URL-M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>34</td>\n",
       "      <td>10383</td>\n",
       "      <td>11</td>\n",
       "      <td>2062</td>\n",
       "      <td>0590353403</td>\n",
       "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
       "      <td>J. K. Rowling</td>\n",
       "      <td>1998</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>http://images.amazon.com/images/P/0590353403.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c45bf60a-ca81-4914-8582-46368ade8788')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c45bf60a-ca81-4914-8582-46368ade8788 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c45bf60a-ca81-4914-8582-46368ade8788');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1b9c56e1-b067-4987-a190-338b0c1227f8\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1b9c56e1-b067-4987-a190-338b0c1227f8')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1b9c56e1-b067-4987-a190-338b0c1227f8 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     Book-Embedding-ID  Author-Embedding-ID  Year-Embedding-ID  Book-ID  \\\n",
       "129                 34                10383                 11     2062   \n",
       "\n",
       "           ISBN                             Book-Title    Book-Author  \\\n",
       "129  0590353403  Harry Potter and the Sorcerer's Stone  J. K. Rowling   \n",
       "\n",
       "     Year-Of-Publication   Publisher  \\\n",
       "129                 1998  Scholastic   \n",
       "\n",
       "                                           Image-URL-M  \n",
       "129  http://images.amazon.com/images/P/0590353403.0...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "harry_potter_sorc_stone_emb_id = books[\"Book-Embedding-ID\"][\n",
    "    books[\"Book-Title\"].str.contains(\n",
    "        \"Harry Potter and the Sorcerer's Stone\")]\n",
    "\n",
    "books[\n",
    "    books[\"Book-Title\"].str.contains(\n",
    "        \"Harry Potter and the Sorcerer's Stone\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gxju2deZ2rvU"
   },
   "outputs": [],
   "source": [
    "def get_k_nearest_neighbours_model(embeddings, metric=\"cosine\"):\n",
    "    knn_model = NearestNeighbors(metric=metric, n_jobs=-1)\n",
    "    knn_model.fit(embeddings)\n",
    "    return knn_model\n",
    "\n",
    "def get_k_neighbours_for_vector(vector, knn_model, k=5):\n",
    "    _, cos_indices = knn_model.kneighbors(\n",
    "        vector, n_neighbors=k)\n",
    "    return cos_indices\n",
    "\n",
    "def convert_emb_ids_to_book_ids(ratings, emb_ids):\n",
    "  recommended_book_ids = ratings[\n",
    "      ratings[\"Book-Embedding-ID\"].isin(emb_ids[0])]\n",
    "\n",
    "  sorted_recommended_book_ids = recommended_book_ids.sort_values(\n",
    "      by=[\"Book-Embedding-ID\"],\n",
    "      key=lambda x: x.map(\n",
    "          {v: i for i, v in enumerate(emb_ids[0])}))\n",
    "\n",
    "  sorted_recommended_book_ids = sorted_recommended_book_ids[\"Book-ID\"].unique()\n",
    "  return sorted_recommended_book_ids\n",
    "\n",
    "def get_book_titles_from_book_ids(books_metadata, book_ids):\n",
    "  recommended_books = books_metadata[\n",
    "      books_metadata['Book-ID'].isin(book_ids)].sort_values(\n",
    "          by=[\"Book-ID\"], key=lambda x: x.map(\n",
    "              {v: i for i, v in enumerate(book_ids)}))\n",
    "  return recommended_books['Book-Title'].unique()\n",
    "\n",
    "def get_book_recommendations(\n",
    "    emb_id, embeddings,\n",
    "    book_ratings, book_metadata,\n",
    "    number_of_recommendations):\n",
    "  book_emb = embeddings[emb_id].reshape(1,-1)\n",
    "\n",
    "  knn_model = get_k_nearest_neighbours_model(\n",
    "      embeddings, metric=\"cosine\")\n",
    "  recommended_book_emb_ids = get_k_neighbours_for_vector(\n",
    "      book_emb, knn_model, k=number_of_recommendations)\n",
    "  recommended_book_ids = convert_emb_ids_to_book_ids(\n",
    "      book_ratings, recommended_book_emb_ids)\n",
    "  recommended_book_titles = get_book_titles_from_book_ids(\n",
    "      book_metadata, recommended_book_ids)\n",
    "\n",
    "  return recommended_book_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ditFrKO28Ap5",
    "outputId": "72ddf391-b613-48e7-aef5-3216f52d49de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Harry Potter and the Sorcerer's Stone\",\n",
       "       'Harry Potter and the Chamber of Secrets',\n",
       "       'Harry Potter and the Prisoner of Azkaban', 'Jane Eyre',\n",
       "       'Harry Potter and the Goblet of Fire'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_book_recommendations(\n",
    "    harry_potter_sorc_stone_emb_id,\n",
    "    loaded_embeddings, ratings, books, 5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d-vr50w37uEy",
    "EApXaoFh6bSL",
    "iU-lf7Nm6jSe",
    "Rs5OMoqZ6gIb"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
