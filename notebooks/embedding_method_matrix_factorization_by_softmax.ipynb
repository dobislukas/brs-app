{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "P13Dk13EFPDI",
        "FP6szSnCGx3H",
        "-j_0nkyh63_v",
        "bzYuZZmvPubr",
        "fcm89wXNmjs4",
        "fShkqQiPCkki"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix factorization using SoftMax model"
      ],
      "metadata": {
        "id": "NjI72VL2E-Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution inspired from https://developers.google.com/machine-learning/recommendation/overview."
      ],
      "metadata": {
        "id": "HFtumGGbOVXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paths"
      ],
      "metadata": {
        "id": "apIM6z9am4sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_path = \"/content/drive/MyDrive/Colab Notebooks/Codesentics/notebooks/\"\n",
        "bx_preprocessed_dataset_path = drive_path + \"bx_data/preprocessed_dataset/\"\n",
        "\n",
        "ratings_path = bx_preprocessed_dataset_path + \"preprocessed_ratings_data.pkl\"\n",
        "book_metadata_path = bx_preprocessed_dataset_path + \"preprocessed_book_metadata.pkl\"\n",
        "\n",
        "embeddings_save_path = drive_path + \"book_embeddings/softmax_book_embeddings.pkl\""
      ],
      "metadata": {
        "id": "xdkUI2F65DV-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "m-3O_Ir50NKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import copy\n",
        "\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors"
      ],
      "metadata": {
        "id": "oyazcIE9zf9G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and prepare data"
      ],
      "metadata": {
        "id": "on8u_qdj0RpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_pickle(ratings_path)\n",
        "books = pd.read_pickle(book_metadata_path)"
      ],
      "metadata": {
        "id": "BVm2Ue8cC55Y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary: user -> [list of his highly rated books]\n",
        "high_ratings = ratings[ratings[\"Book-Rating\"] > 6]\n",
        "user_to_books_df = high_ratings[[\"User-Embedding-ID\", \"Book-Embedding-ID\"]] \\\n",
        "    .groupby(\"User-Embedding-ID\", as_index=False) \\\n",
        "    .aggregate(lambda x: list(x))"
      ],
      "metadata": {
        "id": "B6qS2lPKaGJn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary for book embedding ID to year embedding ID conversion\n",
        "book_to_year_dict = {\n",
        "    book: year for book, year in zip(\n",
        "        books[\"Book-Embedding-ID\"],\n",
        "        books[\"Year-Embedding-ID\"])\n",
        "}\n",
        "# Create dictionary for book embedding ID to author embedding ID conversion\n",
        "book_to_author_dict = {\n",
        "    book: author\n",
        "    for book, author in zip(\n",
        "        books[\"Book-Embedding-ID\"],\n",
        "        books[\"Author-Embedding-ID\"])\n",
        "}"
      ],
      "metadata": {
        "id": "oaYoMoc1WAcw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define dataset implementation"
      ],
      "metadata": {
        "id": "V_7g-nDp0YA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMaxDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, user_to_books_df, book_to_year_dict, book_to_author_dict):\n",
        "        super(SoftMaxDataset, self).__init__()\n",
        "        self.user_to_books_df = user_to_books_df\n",
        "        self.book_to_year_dict = book_to_year_dict\n",
        "        self.book_to_author_dict = book_to_author_dict\n",
        "\n",
        "    # Length is defined by number of unique users that rated books\n",
        "    def __len__(self):\n",
        "        return len(self.user_to_books_df)\n",
        "\n",
        "    # Generate following tuple: (book, book-author, book-year) -> similar_book\n",
        "    def __getitem__(self, idx=None):\n",
        "\n",
        "        # Get random user books\n",
        "        user_book_ids = self.user_to_books_df[\"Book-Embedding-ID\"].sample().values[0]\n",
        "\n",
        "        # Sample his books and recommend another\n",
        "        input_book_id = random.choice(user_book_ids)\n",
        "        year_id = self.book_to_year_dict[input_book_id]\n",
        "        author_id = self.book_to_author_dict[input_book_id]\n",
        "        recommend_book_id = random.choice(user_book_ids)\n",
        "\n",
        "        return torch.tensor([\n",
        "            input_book_id, year_id, author_id,\n",
        "            recommend_book_id], dtype=torch.long)\n",
        "\n",
        "# Divide data into train/test parts and return their dataloaders\n",
        "def get_dataloaders(\n",
        "    data, years_dict, author_dict,\n",
        "    train_batch_size, train_ratio=0.8):\n",
        "\n",
        "    train_data = data.sample(frac=train_ratio)\n",
        "    test_data = data.drop(train_data.index)\n",
        "\n",
        "    sm_train_dataset = SoftMaxDataset(train_data, years_dict, author_dict)\n",
        "    sm_test_dataset = SoftMaxDataset(test_data, years_dict, author_dict)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        sm_train_dataset, batch_size=train_batch_size,\n",
        "        shuffle=True)\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        sm_test_dataset, batch_size=len(sm_test_dataset),\n",
        "        shuffle=False)\n",
        "\n",
        "    return train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "qx1CoB3QmTgA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model implementation"
      ],
      "metadata": {
        "id": "1sOWLJXa8eYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftMaxModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pub_year_count, authors_count, book_ids_count,\n",
        "                 in_embed_dims=(3,5,27), init_stddev=0.5):\n",
        "        super(SoftMaxModel, self).__init__()\n",
        "\n",
        "        embedding_dim = sum(in_embed_dims)\n",
        "\n",
        "        # Layers used to embed book for which are made recommendations\n",
        "        self.year_embedding_layer = nn.Embedding(\n",
        "            pub_year_count, in_embed_dims[0])\n",
        "        self.author_embedding_layer = nn.Embedding(\n",
        "            authors_count, in_embed_dims[1])\n",
        "        self.input_book_embedding_layer = nn.Embedding(\n",
        "            book_ids_count, in_embed_dims[2])\n",
        "\n",
        "        # Layers used for computing batch of user embeddings\n",
        "        self.linear_1_layer = nn.Linear(embedding_dim, embedding_dim // 2)\n",
        "        self.bn1 = nn.BatchNorm1d(embedding_dim // 2)\n",
        "        self.linear_2_layer = nn.Linear(embedding_dim // 2, embedding_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Layer used to store book embeddings for recommendation\n",
        "        self.book_embeddings = torch.nn.Embedding(book_ids_count, embedding_dim)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.initialize_weights(init_stddev)\n",
        "\n",
        "    # Normal distribution initialization\n",
        "    def initialize_weights(self, init_stddev):\n",
        "        self.year_embedding_layer.weight.data.normal_(std=init_stddev)\n",
        "        self.author_embedding_layer.weight.data.normal_(std=init_stddev)\n",
        "        self.input_book_embedding_layer.weight.data.normal_(std=init_stddev)\n",
        "        self.linear_1_layer.weight.data.normal_(std=init_stddev)\n",
        "        self.linear_2_layer.weight.data.normal_(std=init_stddev)\n",
        "        self.book_embeddings.weight.data.normal_(std=init_stddev)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        # Embed input book\n",
        "        year_emb = self.year_embedding_layer(input_ids[:,1])\n",
        "        author_emb = self.author_embedding_layer(input_ids[:,2])\n",
        "        book_emb = self.input_book_embedding_layer(input_ids[:,0])\n",
        "\n",
        "        # Concatenate input book embeddings\n",
        "        x = torch.cat((book_emb, author_emb, year_emb), dim=1)\n",
        "\n",
        "        # Compute user embedding\n",
        "        x = self.relu(self.bn1(self.linear_1_layer(x)))\n",
        "        x = self.bn2(self.linear_2_layer(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Compute loss with goal of having user embedding\n",
        "# contain his other highly rated books (stored in labels)\n",
        "def softmax_loss(user_embs, book_embs, labels):\n",
        "    logits = torch.matmul(user_embs, book_embs.T)\n",
        "    return F.cross_entropy(logits, labels)"
      ],
      "metadata": {
        "id": "y21E3sW0_IRU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop"
      ],
      "metadata": {
        "id": "2mI2zLOk8kHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of embedded items for model embedding layers\n",
        "book_ids_count = max(book_to_year_dict.keys()) + 1\n",
        "year_ids_count = max(book_to_year_dict.values()) + 1\n",
        "author_ids_count = max(book_to_author_dict.values()) + 1"
      ],
      "metadata": {
        "id": "EXooNrZfvmzJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters\n",
        "train_data_ratio = 0.8\n",
        "train_batch_size = 30\n",
        "epochs = 1000\n",
        "learning_rate = 0.1\n",
        "in_embed_dims = (3,5,27)  # (Year, Author, Input book) embedding dimensions"
      ],
      "metadata": {
        "id": "wGTkatfwpha5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataloaders\n",
        "train_dataloader, test_dataloader = get_dataloaders(\n",
        "    user_to_books_df,\n",
        "    book_to_year_dict, book_to_author_dict,\n",
        "    train_batch_size, train_data_ratio)\n",
        "\n",
        "# Initialize model\n",
        "model = SoftMaxModel(\n",
        "    year_ids_count, author_ids_count,\n",
        "    book_ids_count, in_embed_dims)\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize loss tracking variables\n",
        "epoch_losses = []\n",
        "best_loss = 100000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_train_loss = 0\n",
        "    epoch_test_loss = 0\n",
        "\n",
        "    # Training phase\n",
        "    for batch in train_dataloader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_data = batch[:,:-1]\n",
        "        labels = batch[:,-1]\n",
        "\n",
        "        if batch_data.shape[0] < 2:\n",
        "            break\n",
        "\n",
        "        batch_of_user_embeddings = model(batch_data)\n",
        "\n",
        "        train_loss = softmax_loss(\n",
        "            batch_of_user_embeddings,\n",
        "            model.book_embeddings.weight,\n",
        "            labels)\n",
        "        train_loss.backward()\n",
        "        epoch_train_loss += train_loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test phase\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            batch_data = batch[:,:-1]\n",
        "            labels = batch[:,-1]\n",
        "\n",
        "            batch_of_user_embeddings = model(batch_data)\n",
        "            test_loss = softmax_loss(\n",
        "                batch_of_user_embeddings,\n",
        "                model.book_embeddings.weight,\n",
        "                labels)\n",
        "            epoch_test_loss += test_loss.item()\n",
        "\n",
        "        # Save best test weights\n",
        "        if epoch_test_loss < best_loss:\n",
        "            best_epoch = epoch + 1\n",
        "            train_loss_at_best = train_loss.item()\n",
        "            best_loss = test_loss.item()\n",
        "            best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # Save epoch average of mean batch loss\n",
        "    avg_batch_train_loss = epoch_train_loss / len(train_dataloader)\n",
        "    avg_batch_test_loss = epoch_test_loss / len(test_dataloader)\n",
        "    epoch_losses.append((avg_batch_train_loss, avg_batch_test_loss))\n",
        "\n",
        "    # Visualize training progress\n",
        "    with tqdm(total=1, desc=f'Epoch {epoch+1}/{epochs}', unit='epoch') as t:\n",
        "        t.set_description(f'Epoch {epoch+1}/{epochs}')\n",
        "        t.set_postfix(train_loss=avg_batch_train_loss,\n",
        "                      test_loss=avg_batch_test_loss)\n",
        "        t.update()"
      ],
      "metadata": {
        "id": "SbwXFtWXulbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de4ec75-2572-4d86-bd42-c26211f4ac40"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1000: 100%|██████████| 1/1 [00:00<00:00, 121.46epoch/s, test_loss=7.88, train_loss=8.85]\n",
            "Epoch 2/1000: 100%|██████████| 1/1 [00:00<00:00, 93.25epoch/s, test_loss=7.09, train_loss=7.28]\n",
            "Epoch 3/1000: 100%|██████████| 1/1 [00:00<00:00, 149.17epoch/s, test_loss=6.73, train_loss=6.6]\n",
            "Epoch 4/1000: 100%|██████████| 1/1 [00:00<00:00, 186.45epoch/s, test_loss=6.62, train_loss=6.32]\n",
            "Epoch 5/1000: 100%|██████████| 1/1 [00:00<00:00, 146.74epoch/s, test_loss=6.52, train_loss=6.25]\n",
            "Epoch 6/1000: 100%|██████████| 1/1 [00:00<00:00, 105.71epoch/s, test_loss=6.52, train_loss=6.2]\n",
            "Epoch 7/1000: 100%|██████████| 1/1 [00:00<00:00, 185.02epoch/s, test_loss=6.45, train_loss=6.19]\n",
            "Epoch 8/1000: 100%|██████████| 1/1 [00:00<00:00, 116.13epoch/s, test_loss=6.52, train_loss=6.15]\n",
            "Epoch 9/1000: 100%|██████████| 1/1 [00:00<00:00, 189.11epoch/s, test_loss=6.53, train_loss=6.17]\n",
            "Epoch 10/1000: 100%|██████████| 1/1 [00:00<00:00, 91.67epoch/s, test_loss=6.5, train_loss=6.16]\n",
            "Epoch 11/1000: 100%|██████████| 1/1 [00:00<00:00, 103.59epoch/s, test_loss=6.5, train_loss=6.17]\n",
            "Epoch 12/1000: 100%|██████████| 1/1 [00:00<00:00, 137.27epoch/s, test_loss=6.54, train_loss=6.16]\n",
            "Epoch 13/1000: 100%|██████████| 1/1 [00:00<00:00, 163.53epoch/s, test_loss=6.53, train_loss=6.18]\n",
            "Epoch 14/1000: 100%|██████████| 1/1 [00:00<00:00, 110.06epoch/s, test_loss=6.52, train_loss=6.16]\n",
            "Epoch 15/1000: 100%|██████████| 1/1 [00:00<00:00, 83.15epoch/s, test_loss=6.53, train_loss=6.17]\n",
            "Epoch 16/1000: 100%|██████████| 1/1 [00:00<00:00, 148.82epoch/s, test_loss=6.55, train_loss=6.16]\n",
            "Epoch 17/1000: 100%|██████████| 1/1 [00:00<00:00, 203.46epoch/s, test_loss=6.49, train_loss=6.18]\n",
            "Epoch 18/1000: 100%|██████████| 1/1 [00:00<00:00, 139.94epoch/s, test_loss=6.48, train_loss=6.16]\n",
            "Epoch 19/1000: 100%|██████████| 1/1 [00:00<00:00, 175.99epoch/s, test_loss=6.47, train_loss=6.15]\n",
            "Epoch 20/1000: 100%|██████████| 1/1 [00:00<00:00, 128.35epoch/s, test_loss=6.49, train_loss=6.16]\n",
            "Epoch 21/1000: 100%|██████████| 1/1 [00:00<00:00, 191.83epoch/s, test_loss=6.53, train_loss=6.17]\n",
            "Epoch 22/1000: 100%|██████████| 1/1 [00:00<00:00, 189.55epoch/s, test_loss=6.5, train_loss=6.16]\n",
            "Epoch 23/1000: 100%|██████████| 1/1 [00:00<00:00, 196.43epoch/s, test_loss=6.5, train_loss=6.17]\n",
            "Epoch 24/1000: 100%|██████████| 1/1 [00:00<00:00, 118.60epoch/s, test_loss=6.52, train_loss=6.15]\n",
            "Epoch 25/1000: 100%|██████████| 1/1 [00:00<00:00, 214.12epoch/s, test_loss=6.52, train_loss=6.16]\n",
            "Epoch 26/1000: 100%|██████████| 1/1 [00:00<00:00, 130.86epoch/s, test_loss=6.49, train_loss=6.17]\n",
            "Epoch 27/1000: 100%|██████████| 1/1 [00:00<00:00, 137.53epoch/s, test_loss=6.49, train_loss=6.17]\n",
            "Epoch 28/1000: 100%|██████████| 1/1 [00:00<00:00, 137.01epoch/s, test_loss=6.56, train_loss=6.16]\n",
            "Epoch 29/1000: 100%|██████████| 1/1 [00:00<00:00, 179.80epoch/s, test_loss=6.46, train_loss=6.15]\n",
            "Epoch 30/1000: 100%|██████████| 1/1 [00:00<00:00, 159.18epoch/s, test_loss=6.49, train_loss=6.16]\n",
            "Epoch 31/1000: 100%|██████████| 1/1 [00:00<00:00, 155.84epoch/s, test_loss=6.49, train_loss=6.15]\n",
            "Epoch 32/1000: 100%|██████████| 1/1 [00:00<00:00, 193.06epoch/s, test_loss=6.53, train_loss=6.12]\n",
            "Epoch 33/1000: 100%|██████████| 1/1 [00:00<00:00, 190.09epoch/s, test_loss=6.53, train_loss=6.14]\n",
            "Epoch 34/1000: 100%|██████████| 1/1 [00:00<00:00, 181.84epoch/s, test_loss=6.47, train_loss=6.17]\n",
            "Epoch 35/1000: 100%|██████████| 1/1 [00:00<00:00, 279.30epoch/s, test_loss=6.54, train_loss=6.17]\n",
            "Epoch 36/1000: 100%|██████████| 1/1 [00:00<00:00, 118.47epoch/s, test_loss=6.48, train_loss=6.18]\n",
            "Epoch 37/1000: 100%|██████████| 1/1 [00:00<00:00, 150.35epoch/s, test_loss=6.49, train_loss=6.17]\n",
            "Epoch 38/1000: 100%|██████████| 1/1 [00:00<00:00, 85.59epoch/s, test_loss=6.46, train_loss=6.17]\n",
            "Epoch 39/1000: 100%|██████████| 1/1 [00:00<00:00, 114.58epoch/s, test_loss=6.49, train_loss=6.15]\n",
            "Epoch 40/1000: 100%|██████████| 1/1 [00:00<00:00, 160.54epoch/s, test_loss=6.51, train_loss=6.16]\n",
            "Epoch 41/1000: 100%|██████████| 1/1 [00:00<00:00, 96.49epoch/s, test_loss=6.49, train_loss=6.12]\n",
            "Epoch 42/1000: 100%|██████████| 1/1 [00:00<00:00, 105.27epoch/s, test_loss=6.46, train_loss=6.14]\n",
            "Epoch 43/1000: 100%|██████████| 1/1 [00:00<00:00, 237.25epoch/s, test_loss=6.51, train_loss=6.16]\n",
            "Epoch 44/1000: 100%|██████████| 1/1 [00:00<00:00, 101.37epoch/s, test_loss=6.48, train_loss=6.16]\n",
            "Epoch 45/1000: 100%|██████████| 1/1 [00:00<00:00, 289.68epoch/s, test_loss=6.51, train_loss=6.13]\n",
            "Epoch 46/1000: 100%|██████████| 1/1 [00:00<00:00, 136.59epoch/s, test_loss=6.5, train_loss=6.17]\n",
            "Epoch 47/1000: 100%|██████████| 1/1 [00:00<00:00, 199.98epoch/s, test_loss=6.5, train_loss=6.16]\n",
            "Epoch 48/1000: 100%|██████████| 1/1 [00:00<00:00, 265.19epoch/s, test_loss=6.5, train_loss=6.16]\n",
            "Epoch 49/1000: 100%|██████████| 1/1 [00:00<00:00, 370.23epoch/s, test_loss=6.49, train_loss=6.13]\n",
            "Epoch 50/1000: 100%|██████████| 1/1 [00:00<00:00, 255.78epoch/s, test_loss=6.53, train_loss=6.16]\n",
            "Epoch 51/1000: 100%|██████████| 1/1 [00:00<00:00, 192.62epoch/s, test_loss=6.52, train_loss=6.16]\n",
            "Epoch 52/1000: 100%|██████████| 1/1 [00:00<00:00, 187.68epoch/s, test_loss=6.49, train_loss=6.14]\n",
            "Epoch 53/1000: 100%|██████████| 1/1 [00:00<00:00, 234.61epoch/s, test_loss=6.46, train_loss=6.15]\n",
            "Epoch 54/1000: 100%|██████████| 1/1 [00:00<00:00, 177.90epoch/s, test_loss=6.49, train_loss=6.11]\n",
            "Epoch 55/1000: 100%|██████████| 1/1 [00:00<00:00, 136.15epoch/s, test_loss=6.54, train_loss=6.16]\n",
            "Epoch 56/1000: 100%|██████████| 1/1 [00:00<00:00, 201.54epoch/s, test_loss=6.47, train_loss=6.12]\n",
            "Epoch 57/1000: 100%|██████████| 1/1 [00:00<00:00, 201.38epoch/s, test_loss=6.47, train_loss=6.14]\n",
            "Epoch 58/1000: 100%|██████████| 1/1 [00:00<00:00, 165.58epoch/s, test_loss=6.5, train_loss=6.15]\n",
            "Epoch 59/1000: 100%|██████████| 1/1 [00:00<00:00, 108.37epoch/s, test_loss=6.47, train_loss=6.14]\n",
            "Epoch 60/1000: 100%|██████████| 1/1 [00:00<00:00, 376.85epoch/s, test_loss=6.45, train_loss=6.13]\n",
            "Epoch 61/1000: 100%|██████████| 1/1 [00:00<00:00, 107.60epoch/s, test_loss=6.49, train_loss=6.15]\n",
            "Epoch 62/1000: 100%|██████████| 1/1 [00:00<00:00, 209.82epoch/s, test_loss=6.46, train_loss=6.15]\n",
            "Epoch 63/1000: 100%|██████████| 1/1 [00:00<00:00, 162.92epoch/s, test_loss=6.54, train_loss=6.14]\n",
            "Epoch 64/1000: 100%|██████████| 1/1 [00:00<00:00, 127.59epoch/s, test_loss=6.54, train_loss=6.15]\n",
            "Epoch 65/1000: 100%|██████████| 1/1 [00:00<00:00, 319.64epoch/s, test_loss=6.52, train_loss=6.15]\n",
            "Epoch 66/1000: 100%|██████████| 1/1 [00:00<00:00, 213.97epoch/s, test_loss=6.54, train_loss=6.16]\n",
            "Epoch 67/1000: 100%|██████████| 1/1 [00:00<00:00, 257.32epoch/s, test_loss=6.48, train_loss=6.14]\n",
            "Epoch 68/1000: 100%|██████████| 1/1 [00:00<00:00, 148.53epoch/s, test_loss=6.54, train_loss=6.11]\n",
            "Epoch 69/1000: 100%|██████████| 1/1 [00:00<00:00, 145.14epoch/s, test_loss=6.47, train_loss=6.13]\n",
            "Epoch 70/1000: 100%|██████████| 1/1 [00:00<00:00, 112.27epoch/s, test_loss=6.49, train_loss=6.14]\n",
            "Epoch 71/1000: 100%|██████████| 1/1 [00:00<00:00, 205.68epoch/s, test_loss=6.47, train_loss=6.1]\n",
            "Epoch 72/1000: 100%|██████████| 1/1 [00:00<00:00, 143.70epoch/s, test_loss=6.56, train_loss=6.14]\n",
            "Epoch 73/1000: 100%|██████████| 1/1 [00:00<00:00, 119.00epoch/s, test_loss=6.51, train_loss=6.11]\n",
            "Epoch 74/1000: 100%|██████████| 1/1 [00:00<00:00, 135.11epoch/s, test_loss=6.51, train_loss=6.16]\n",
            "Epoch 75/1000: 100%|██████████| 1/1 [00:00<00:00, 73.77epoch/s, test_loss=6.41, train_loss=6.13]\n",
            "Epoch 76/1000: 100%|██████████| 1/1 [00:00<00:00, 243.46epoch/s, test_loss=6.54, train_loss=6.13]\n",
            "Epoch 77/1000: 100%|██████████| 1/1 [00:00<00:00, 110.18epoch/s, test_loss=6.43, train_loss=6.14]\n",
            "Epoch 78/1000: 100%|██████████| 1/1 [00:00<00:00, 209.16epoch/s, test_loss=6.52, train_loss=6.11]\n",
            "Epoch 79/1000: 100%|██████████| 1/1 [00:00<00:00, 117.26epoch/s, test_loss=6.55, train_loss=6.09]\n",
            "Epoch 80/1000: 100%|██████████| 1/1 [00:00<00:00, 146.90epoch/s, test_loss=6.51, train_loss=6.09]\n",
            "Epoch 81/1000: 100%|██████████| 1/1 [00:00<00:00, 113.32epoch/s, test_loss=6.52, train_loss=6.12]\n",
            "Epoch 82/1000: 100%|██████████| 1/1 [00:00<00:00, 132.98epoch/s, test_loss=6.54, train_loss=6.09]\n",
            "Epoch 83/1000: 100%|██████████| 1/1 [00:00<00:00, 96.00epoch/s, test_loss=6.45, train_loss=6.09]\n",
            "Epoch 84/1000: 100%|██████████| 1/1 [00:00<00:00, 175.73epoch/s, test_loss=6.49, train_loss=6.13]\n",
            "Epoch 85/1000: 100%|██████████| 1/1 [00:00<00:00, 184.21epoch/s, test_loss=6.5, train_loss=6.11]\n",
            "Epoch 86/1000: 100%|██████████| 1/1 [00:00<00:00, 230.15epoch/s, test_loss=6.48, train_loss=6.11]\n",
            "Epoch 87/1000: 100%|██████████| 1/1 [00:00<00:00, 105.76epoch/s, test_loss=6.52, train_loss=6.13]\n",
            "Epoch 88/1000: 100%|██████████| 1/1 [00:00<00:00, 152.06epoch/s, test_loss=6.54, train_loss=6.11]\n",
            "Epoch 89/1000: 100%|██████████| 1/1 [00:00<00:00, 195.28epoch/s, test_loss=6.52, train_loss=6.1]\n",
            "Epoch 90/1000: 100%|██████████| 1/1 [00:00<00:00, 122.62epoch/s, test_loss=6.47, train_loss=6.11]\n",
            "Epoch 91/1000: 100%|██████████| 1/1 [00:00<00:00, 145.84epoch/s, test_loss=6.49, train_loss=6.11]\n",
            "Epoch 92/1000: 100%|██████████| 1/1 [00:00<00:00, 133.79epoch/s, test_loss=6.43, train_loss=6.11]\n",
            "Epoch 93/1000: 100%|██████████| 1/1 [00:00<00:00, 347.10epoch/s, test_loss=6.53, train_loss=6.1]\n",
            "Epoch 94/1000: 100%|██████████| 1/1 [00:00<00:00, 245.25epoch/s, test_loss=6.58, train_loss=6.09]\n",
            "Epoch 95/1000: 100%|██████████| 1/1 [00:00<00:00, 212.11epoch/s, test_loss=6.46, train_loss=6.13]\n",
            "Epoch 96/1000: 100%|██████████| 1/1 [00:00<00:00, 110.42epoch/s, test_loss=6.48, train_loss=6.09]\n",
            "Epoch 97/1000: 100%|██████████| 1/1 [00:00<00:00, 196.22epoch/s, test_loss=6.59, train_loss=6.12]\n",
            "Epoch 98/1000: 100%|██████████| 1/1 [00:00<00:00, 208.75epoch/s, test_loss=6.55, train_loss=6.14]\n",
            "Epoch 99/1000: 100%|██████████| 1/1 [00:00<00:00, 235.44epoch/s, test_loss=6.5, train_loss=6.07]\n",
            "Epoch 100/1000: 100%|██████████| 1/1 [00:00<00:00, 210.54epoch/s, test_loss=6.44, train_loss=6.11]\n",
            "Epoch 101/1000: 100%|██████████| 1/1 [00:00<00:00, 180.92epoch/s, test_loss=6.5, train_loss=6.09]\n",
            "Epoch 102/1000: 100%|██████████| 1/1 [00:00<00:00, 130.06epoch/s, test_loss=6.57, train_loss=6.1]\n",
            "Epoch 103/1000: 100%|██████████| 1/1 [00:00<00:00, 207.65epoch/s, test_loss=6.49, train_loss=6.09]\n",
            "Epoch 104/1000: 100%|██████████| 1/1 [00:00<00:00, 214.89epoch/s, test_loss=6.46, train_loss=6.11]\n",
            "Epoch 105/1000: 100%|██████████| 1/1 [00:00<00:00, 168.01epoch/s, test_loss=6.55, train_loss=6.08]\n",
            "Epoch 106/1000: 100%|██████████| 1/1 [00:00<00:00, 153.87epoch/s, test_loss=6.53, train_loss=6.14]\n",
            "Epoch 107/1000: 100%|██████████| 1/1 [00:00<00:00, 144.05epoch/s, test_loss=6.34, train_loss=6.11]\n",
            "Epoch 108/1000: 100%|██████████| 1/1 [00:00<00:00, 155.66epoch/s, test_loss=6.5, train_loss=6.07]\n",
            "Epoch 109/1000: 100%|██████████| 1/1 [00:00<00:00, 217.51epoch/s, test_loss=6.48, train_loss=6.09]\n",
            "Epoch 110/1000: 100%|██████████| 1/1 [00:00<00:00, 195.53epoch/s, test_loss=6.45, train_loss=6.12]\n",
            "Epoch 111/1000: 100%|██████████| 1/1 [00:00<00:00, 94.46epoch/s, test_loss=6.46, train_loss=6.09]\n",
            "Epoch 112/1000: 100%|██████████| 1/1 [00:00<00:00, 226.73epoch/s, test_loss=6.49, train_loss=6.07]\n",
            "Epoch 113/1000: 100%|██████████| 1/1 [00:00<00:00, 119.15epoch/s, test_loss=6.49, train_loss=6.09]\n",
            "Epoch 114/1000: 100%|██████████| 1/1 [00:00<00:00, 245.84epoch/s, test_loss=6.49, train_loss=6.09]\n",
            "Epoch 115/1000: 100%|██████████| 1/1 [00:00<00:00, 247.10epoch/s, test_loss=6.57, train_loss=6.06]\n",
            "Epoch 116/1000: 100%|██████████| 1/1 [00:00<00:00, 206.29epoch/s, test_loss=6.45, train_loss=6.04]\n",
            "Epoch 117/1000: 100%|██████████| 1/1 [00:00<00:00, 66.50epoch/s, test_loss=6.45, train_loss=6.06]\n",
            "Epoch 118/1000: 100%|██████████| 1/1 [00:00<00:00, 195.80epoch/s, test_loss=6.51, train_loss=6.08]\n",
            "Epoch 119/1000: 100%|██████████| 1/1 [00:00<00:00, 373.03epoch/s, test_loss=6.5, train_loss=6.08]\n",
            "Epoch 120/1000: 100%|██████████| 1/1 [00:00<00:00, 98.76epoch/s, test_loss=6.6, train_loss=6.11]\n",
            "Epoch 121/1000: 100%|██████████| 1/1 [00:00<00:00, 186.07epoch/s, test_loss=6.58, train_loss=6.06]\n",
            "Epoch 122/1000: 100%|██████████| 1/1 [00:00<00:00, 278.34epoch/s, test_loss=6.43, train_loss=6.09]\n",
            "Epoch 123/1000: 100%|██████████| 1/1 [00:00<00:00, 162.17epoch/s, test_loss=6.55, train_loss=6.12]\n",
            "Epoch 124/1000: 100%|██████████| 1/1 [00:00<00:00, 200.94epoch/s, test_loss=6.44, train_loss=6.02]\n",
            "Epoch 125/1000: 100%|██████████| 1/1 [00:00<00:00, 204.76epoch/s, test_loss=6.5, train_loss=6.06]\n",
            "Epoch 126/1000: 100%|██████████| 1/1 [00:00<00:00, 106.24epoch/s, test_loss=6.44, train_loss=6.07]\n",
            "Epoch 127/1000: 100%|██████████| 1/1 [00:00<00:00, 92.20epoch/s, test_loss=6.51, train_loss=6.07]\n",
            "Epoch 128/1000: 100%|██████████| 1/1 [00:00<00:00, 213.02epoch/s, test_loss=6.53, train_loss=6.05]\n",
            "Epoch 129/1000: 100%|██████████| 1/1 [00:00<00:00, 250.74epoch/s, test_loss=6.57, train_loss=6.08]\n",
            "Epoch 130/1000: 100%|██████████| 1/1 [00:00<00:00, 107.36epoch/s, test_loss=6.44, train_loss=6.06]\n",
            "Epoch 131/1000: 100%|██████████| 1/1 [00:00<00:00, 246.62epoch/s, test_loss=6.49, train_loss=6.04]\n",
            "Epoch 132/1000: 100%|██████████| 1/1 [00:00<00:00, 351.78epoch/s, test_loss=6.5, train_loss=6.06]\n",
            "Epoch 133/1000: 100%|██████████| 1/1 [00:00<00:00, 111.82epoch/s, test_loss=6.45, train_loss=6.06]\n",
            "Epoch 134/1000: 100%|██████████| 1/1 [00:00<00:00, 135.34epoch/s, test_loss=6.59, train_loss=6.04]\n",
            "Epoch 135/1000: 100%|██████████| 1/1 [00:00<00:00, 100.26epoch/s, test_loss=6.53, train_loss=6.05]\n",
            "Epoch 136/1000: 100%|██████████| 1/1 [00:00<00:00, 308.00epoch/s, test_loss=6.52, train_loss=6.05]\n",
            "Epoch 137/1000: 100%|██████████| 1/1 [00:00<00:00, 150.63epoch/s, test_loss=6.46, train_loss=6.04]\n",
            "Epoch 138/1000: 100%|██████████| 1/1 [00:00<00:00, 228.71epoch/s, test_loss=6.61, train_loss=6.07]\n",
            "Epoch 139/1000: 100%|██████████| 1/1 [00:00<00:00, 78.03epoch/s, test_loss=6.5, train_loss=6.04]\n",
            "Epoch 140/1000: 100%|██████████| 1/1 [00:00<00:00, 102.77epoch/s, test_loss=6.48, train_loss=6.07]\n",
            "Epoch 141/1000: 100%|██████████| 1/1 [00:00<00:00, 157.80epoch/s, test_loss=6.5, train_loss=6.01]\n",
            "Epoch 142/1000: 100%|██████████| 1/1 [00:00<00:00, 109.22epoch/s, test_loss=6.53, train_loss=6.05]\n",
            "Epoch 143/1000: 100%|██████████| 1/1 [00:00<00:00, 185.60epoch/s, test_loss=6.47, train_loss=6.08]\n",
            "Epoch 144/1000: 100%|██████████| 1/1 [00:00<00:00, 101.93epoch/s, test_loss=6.49, train_loss=6.06]\n",
            "Epoch 145/1000: 100%|██████████| 1/1 [00:00<00:00, 169.88epoch/s, test_loss=6.57, train_loss=6.06]\n",
            "Epoch 146/1000: 100%|██████████| 1/1 [00:00<00:00, 383.15epoch/s, test_loss=6.54, train_loss=6.04]\n",
            "Epoch 147/1000: 100%|██████████| 1/1 [00:00<00:00, 363.99epoch/s, test_loss=6.53, train_loss=6.02]\n",
            "Epoch 148/1000: 100%|██████████| 1/1 [00:00<00:00, 230.01epoch/s, test_loss=6.48, train_loss=6.04]\n",
            "Epoch 149/1000: 100%|██████████| 1/1 [00:00<00:00, 169.54epoch/s, test_loss=6.49, train_loss=6.01]\n",
            "Epoch 150/1000: 100%|██████████| 1/1 [00:00<00:00, 157.38epoch/s, test_loss=6.5, train_loss=6.03]\n",
            "Epoch 151/1000: 100%|██████████| 1/1 [00:00<00:00, 122.76epoch/s, test_loss=6.56, train_loss=6.1]\n",
            "Epoch 152/1000: 100%|██████████| 1/1 [00:00<00:00, 98.93epoch/s, test_loss=6.57, train_loss=6.02]\n",
            "Epoch 153/1000: 100%|██████████| 1/1 [00:00<00:00, 207.20epoch/s, test_loss=6.49, train_loss=6.06]\n",
            "Epoch 154/1000: 100%|██████████| 1/1 [00:00<00:00, 93.24epoch/s, test_loss=6.48, train_loss=5.98]\n",
            "Epoch 155/1000: 100%|██████████| 1/1 [00:00<00:00, 132.84epoch/s, test_loss=6.53, train_loss=6.05]\n",
            "Epoch 156/1000: 100%|██████████| 1/1 [00:00<00:00, 70.37epoch/s, test_loss=6.5, train_loss=6.06]\n",
            "Epoch 157/1000: 100%|██████████| 1/1 [00:00<00:00, 228.60epoch/s, test_loss=6.42, train_loss=6.04]\n",
            "Epoch 158/1000: 100%|██████████| 1/1 [00:00<00:00, 193.59epoch/s, test_loss=6.46, train_loss=5.98]\n",
            "Epoch 159/1000: 100%|██████████| 1/1 [00:00<00:00, 106.59epoch/s, test_loss=6.54, train_loss=6.06]\n",
            "Epoch 160/1000: 100%|██████████| 1/1 [00:00<00:00, 214.97epoch/s, test_loss=6.49, train_loss=6.1]\n",
            "Epoch 161/1000: 100%|██████████| 1/1 [00:00<00:00, 210.95epoch/s, test_loss=6.45, train_loss=6.02]\n",
            "Epoch 162/1000: 100%|██████████| 1/1 [00:00<00:00, 229.10epoch/s, test_loss=6.47, train_loss=5.99]\n",
            "Epoch 163/1000: 100%|██████████| 1/1 [00:00<00:00, 239.74epoch/s, test_loss=6.36, train_loss=6]\n",
            "Epoch 164/1000: 100%|██████████| 1/1 [00:00<00:00, 229.52epoch/s, test_loss=6.48, train_loss=5.98]\n",
            "Epoch 165/1000: 100%|██████████| 1/1 [00:00<00:00, 215.29epoch/s, test_loss=6.47, train_loss=6.03]\n",
            "Epoch 166/1000: 100%|██████████| 1/1 [00:00<00:00, 201.59epoch/s, test_loss=6.52, train_loss=6]\n",
            "Epoch 167/1000: 100%|██████████| 1/1 [00:00<00:00, 193.05epoch/s, test_loss=6.4, train_loss=5.91]\n",
            "Epoch 168/1000: 100%|██████████| 1/1 [00:00<00:00, 124.12epoch/s, test_loss=6.47, train_loss=6.01]\n",
            "Epoch 169/1000: 100%|██████████| 1/1 [00:00<00:00, 140.67epoch/s, test_loss=6.46, train_loss=6.01]\n",
            "Epoch 170/1000: 100%|██████████| 1/1 [00:00<00:00, 229.57epoch/s, test_loss=6.48, train_loss=6.01]\n",
            "Epoch 171/1000: 100%|██████████| 1/1 [00:00<00:00, 140.15epoch/s, test_loss=6.41, train_loss=6]\n",
            "Epoch 172/1000: 100%|██████████| 1/1 [00:00<00:00, 242.22epoch/s, test_loss=6.51, train_loss=5.94]\n",
            "Epoch 173/1000: 100%|██████████| 1/1 [00:00<00:00, 234.86epoch/s, test_loss=6.43, train_loss=5.97]\n",
            "Epoch 174/1000: 100%|██████████| 1/1 [00:00<00:00, 232.98epoch/s, test_loss=6.44, train_loss=6.05]\n",
            "Epoch 175/1000: 100%|██████████| 1/1 [00:00<00:00, 199.38epoch/s, test_loss=6.42, train_loss=6.01]\n",
            "Epoch 176/1000: 100%|██████████| 1/1 [00:00<00:00, 190.95epoch/s, test_loss=6.42, train_loss=5.98]\n",
            "Epoch 177/1000: 100%|██████████| 1/1 [00:00<00:00, 146.41epoch/s, test_loss=6.46, train_loss=5.97]\n",
            "Epoch 178/1000: 100%|██████████| 1/1 [00:00<00:00, 240.90epoch/s, test_loss=6.57, train_loss=5.96]\n",
            "Epoch 179/1000: 100%|██████████| 1/1 [00:00<00:00, 189.56epoch/s, test_loss=6.44, train_loss=5.97]\n",
            "Epoch 180/1000: 100%|██████████| 1/1 [00:00<00:00, 115.92epoch/s, test_loss=6.45, train_loss=5.97]\n",
            "Epoch 181/1000: 100%|██████████| 1/1 [00:00<00:00, 181.74epoch/s, test_loss=6.46, train_loss=5.93]\n",
            "Epoch 182/1000: 100%|██████████| 1/1 [00:00<00:00, 154.57epoch/s, test_loss=6.46, train_loss=5.95]\n",
            "Epoch 183/1000: 100%|██████████| 1/1 [00:00<00:00, 120.88epoch/s, test_loss=6.56, train_loss=5.98]\n",
            "Epoch 184/1000: 100%|██████████| 1/1 [00:00<00:00, 185.30epoch/s, test_loss=6.47, train_loss=6.02]\n",
            "Epoch 185/1000: 100%|██████████| 1/1 [00:00<00:00, 228.50epoch/s, test_loss=6.53, train_loss=5.96]\n",
            "Epoch 186/1000: 100%|██████████| 1/1 [00:00<00:00, 198.90epoch/s, test_loss=6.45, train_loss=6.05]\n",
            "Epoch 187/1000: 100%|██████████| 1/1 [00:00<00:00, 346.29epoch/s, test_loss=6.38, train_loss=6.03]\n",
            "Epoch 188/1000: 100%|██████████| 1/1 [00:00<00:00, 86.17epoch/s, test_loss=6.44, train_loss=5.98]\n",
            "Epoch 189/1000: 100%|██████████| 1/1 [00:00<00:00, 246.94epoch/s, test_loss=6.37, train_loss=5.96]\n",
            "Epoch 190/1000: 100%|██████████| 1/1 [00:00<00:00, 211.48epoch/s, test_loss=6.48, train_loss=5.96]\n",
            "Epoch 191/1000: 100%|██████████| 1/1 [00:00<00:00, 174.52epoch/s, test_loss=6.47, train_loss=6.02]\n",
            "Epoch 192/1000: 100%|██████████| 1/1 [00:00<00:00, 119.25epoch/s, test_loss=6.35, train_loss=5.98]\n",
            "Epoch 193/1000: 100%|██████████| 1/1 [00:00<00:00, 193.77epoch/s, test_loss=6.51, train_loss=5.99]\n",
            "Epoch 194/1000: 100%|██████████| 1/1 [00:00<00:00, 218.50epoch/s, test_loss=6.58, train_loss=5.94]\n",
            "Epoch 195/1000: 100%|██████████| 1/1 [00:00<00:00, 164.75epoch/s, test_loss=6.37, train_loss=6.01]\n",
            "Epoch 196/1000: 100%|██████████| 1/1 [00:00<00:00, 71.84epoch/s, test_loss=6.47, train_loss=5.95]\n",
            "Epoch 197/1000: 100%|██████████| 1/1 [00:00<00:00, 138.26epoch/s, test_loss=6.45, train_loss=5.97]\n",
            "Epoch 198/1000: 100%|██████████| 1/1 [00:00<00:00, 203.34epoch/s, test_loss=6.51, train_loss=5.99]\n",
            "Epoch 199/1000: 100%|██████████| 1/1 [00:00<00:00, 91.34epoch/s, test_loss=6.47, train_loss=5.91]\n",
            "Epoch 200/1000: 100%|██████████| 1/1 [00:00<00:00, 191.86epoch/s, test_loss=6.56, train_loss=5.93]\n",
            "Epoch 201/1000: 100%|██████████| 1/1 [00:00<00:00, 96.16epoch/s, test_loss=6.39, train_loss=5.98]\n",
            "Epoch 202/1000: 100%|██████████| 1/1 [00:00<00:00, 147.08epoch/s, test_loss=6.49, train_loss=6]\n",
            "Epoch 203/1000: 100%|██████████| 1/1 [00:00<00:00, 88.63epoch/s, test_loss=6.55, train_loss=5.99]\n",
            "Epoch 204/1000: 100%|██████████| 1/1 [00:00<00:00, 258.81epoch/s, test_loss=6.43, train_loss=5.95]\n",
            "Epoch 205/1000: 100%|██████████| 1/1 [00:00<00:00, 95.41epoch/s, test_loss=6.36, train_loss=5.97]\n",
            "Epoch 206/1000: 100%|██████████| 1/1 [00:00<00:00, 140.90epoch/s, test_loss=6.52, train_loss=5.98]\n",
            "Epoch 207/1000: 100%|██████████| 1/1 [00:00<00:00, 282.60epoch/s, test_loss=6.47, train_loss=5.99]\n",
            "Epoch 208/1000: 100%|██████████| 1/1 [00:00<00:00, 250.98epoch/s, test_loss=6.4, train_loss=5.88]\n",
            "Epoch 209/1000: 100%|██████████| 1/1 [00:00<00:00, 96.21epoch/s, test_loss=6.39, train_loss=5.9]\n",
            "Epoch 210/1000: 100%|██████████| 1/1 [00:00<00:00, 192.97epoch/s, test_loss=6.56, train_loss=5.91]\n",
            "Epoch 211/1000: 100%|██████████| 1/1 [00:00<00:00, 73.20epoch/s, test_loss=6.35, train_loss=5.95]\n",
            "Epoch 212/1000: 100%|██████████| 1/1 [00:00<00:00, 365.64epoch/s, test_loss=6.4, train_loss=5.96]\n",
            "Epoch 213/1000: 100%|██████████| 1/1 [00:00<00:00, 229.86epoch/s, test_loss=6.56, train_loss=5.93]\n",
            "Epoch 214/1000: 100%|██████████| 1/1 [00:00<00:00, 206.54epoch/s, test_loss=6.49, train_loss=5.95]\n",
            "Epoch 215/1000: 100%|██████████| 1/1 [00:00<00:00, 355.84epoch/s, test_loss=6.57, train_loss=5.84]\n",
            "Epoch 216/1000: 100%|██████████| 1/1 [00:00<00:00, 73.84epoch/s, test_loss=6.58, train_loss=5.95]\n",
            "Epoch 217/1000: 100%|██████████| 1/1 [00:00<00:00, 166.00epoch/s, test_loss=6.48, train_loss=5.92]\n",
            "Epoch 218/1000: 100%|██████████| 1/1 [00:00<00:00, 189.66epoch/s, test_loss=6.47, train_loss=5.96]\n",
            "Epoch 219/1000: 100%|██████████| 1/1 [00:00<00:00, 174.75epoch/s, test_loss=6.48, train_loss=6]\n",
            "Epoch 220/1000: 100%|██████████| 1/1 [00:00<00:00, 192.87epoch/s, test_loss=6.46, train_loss=5.99]\n",
            "Epoch 221/1000: 100%|██████████| 1/1 [00:00<00:00, 214.83epoch/s, test_loss=6.47, train_loss=5.91]\n",
            "Epoch 222/1000: 100%|██████████| 1/1 [00:00<00:00, 111.36epoch/s, test_loss=6.35, train_loss=5.9]\n",
            "Epoch 223/1000: 100%|██████████| 1/1 [00:00<00:00, 82.30epoch/s, test_loss=6.38, train_loss=6.01]\n",
            "Epoch 224/1000: 100%|██████████| 1/1 [00:00<00:00, 216.03epoch/s, test_loss=6.49, train_loss=5.94]\n",
            "Epoch 225/1000: 100%|██████████| 1/1 [00:00<00:00, 327.86epoch/s, test_loss=6.42, train_loss=5.98]\n",
            "Epoch 226/1000: 100%|██████████| 1/1 [00:00<00:00, 199.04epoch/s, test_loss=6.44, train_loss=5.92]\n",
            "Epoch 227/1000: 100%|██████████| 1/1 [00:00<00:00, 214.30epoch/s, test_loss=6.58, train_loss=5.91]\n",
            "Epoch 228/1000: 100%|██████████| 1/1 [00:00<00:00, 260.58epoch/s, test_loss=6.5, train_loss=5.99]\n",
            "Epoch 229/1000: 100%|██████████| 1/1 [00:00<00:00, 82.65epoch/s, test_loss=6.46, train_loss=5.89]\n",
            "Epoch 230/1000: 100%|██████████| 1/1 [00:00<00:00, 175.87epoch/s, test_loss=6.37, train_loss=5.89]\n",
            "Epoch 231/1000: 100%|██████████| 1/1 [00:00<00:00, 200.17epoch/s, test_loss=6.64, train_loss=5.87]\n",
            "Epoch 232/1000: 100%|██████████| 1/1 [00:00<00:00, 368.63epoch/s, test_loss=6.31, train_loss=5.96]\n",
            "Epoch 233/1000: 100%|██████████| 1/1 [00:00<00:00, 179.37epoch/s, test_loss=6.48, train_loss=5.96]\n",
            "Epoch 234/1000: 100%|██████████| 1/1 [00:00<00:00, 254.91epoch/s, test_loss=6.39, train_loss=5.89]\n",
            "Epoch 235/1000: 100%|██████████| 1/1 [00:00<00:00, 215.24epoch/s, test_loss=6.41, train_loss=5.95]\n",
            "Epoch 236/1000: 100%|██████████| 1/1 [00:00<00:00, 172.85epoch/s, test_loss=6.49, train_loss=5.96]\n",
            "Epoch 237/1000: 100%|██████████| 1/1 [00:00<00:00, 200.86epoch/s, test_loss=6.37, train_loss=5.89]\n",
            "Epoch 238/1000: 100%|██████████| 1/1 [00:00<00:00, 231.91epoch/s, test_loss=6.45, train_loss=5.94]\n",
            "Epoch 239/1000: 100%|██████████| 1/1 [00:00<00:00, 87.48epoch/s, test_loss=6.35, train_loss=5.89]\n",
            "Epoch 240/1000: 100%|██████████| 1/1 [00:00<00:00, 124.25epoch/s, test_loss=6.52, train_loss=5.88]\n",
            "Epoch 241/1000: 100%|██████████| 1/1 [00:00<00:00, 109.00epoch/s, test_loss=6.44, train_loss=5.9]\n",
            "Epoch 242/1000: 100%|██████████| 1/1 [00:00<00:00, 231.45epoch/s, test_loss=6.5, train_loss=5.92]\n",
            "Epoch 243/1000: 100%|██████████| 1/1 [00:00<00:00, 199.50epoch/s, test_loss=6.4, train_loss=5.93]\n",
            "Epoch 244/1000: 100%|██████████| 1/1 [00:00<00:00, 224.23epoch/s, test_loss=6.51, train_loss=5.94]\n",
            "Epoch 245/1000: 100%|██████████| 1/1 [00:00<00:00, 193.76epoch/s, test_loss=6.38, train_loss=5.93]\n",
            "Epoch 246/1000: 100%|██████████| 1/1 [00:00<00:00, 136.45epoch/s, test_loss=6.46, train_loss=5.91]\n",
            "Epoch 247/1000: 100%|██████████| 1/1 [00:00<00:00, 208.43epoch/s, test_loss=6.52, train_loss=5.84]\n",
            "Epoch 248/1000: 100%|██████████| 1/1 [00:00<00:00, 178.44epoch/s, test_loss=6.42, train_loss=5.91]\n",
            "Epoch 249/1000: 100%|██████████| 1/1 [00:00<00:00, 236.75epoch/s, test_loss=6.52, train_loss=5.91]\n",
            "Epoch 250/1000: 100%|██████████| 1/1 [00:00<00:00, 231.86epoch/s, test_loss=6.43, train_loss=5.89]\n",
            "Epoch 251/1000: 100%|██████████| 1/1 [00:00<00:00, 382.80epoch/s, test_loss=6.55, train_loss=5.9]\n",
            "Epoch 252/1000: 100%|██████████| 1/1 [00:00<00:00, 244.87epoch/s, test_loss=6.33, train_loss=5.84]\n",
            "Epoch 253/1000: 100%|██████████| 1/1 [00:00<00:00, 234.06epoch/s, test_loss=6.4, train_loss=5.86]\n",
            "Epoch 254/1000: 100%|██████████| 1/1 [00:00<00:00, 118.71epoch/s, test_loss=6.4, train_loss=5.92]\n",
            "Epoch 255/1000: 100%|██████████| 1/1 [00:00<00:00, 93.64epoch/s, test_loss=6.44, train_loss=5.93]\n",
            "Epoch 256/1000: 100%|██████████| 1/1 [00:00<00:00, 154.74epoch/s, test_loss=6.48, train_loss=5.93]\n",
            "Epoch 257/1000: 100%|██████████| 1/1 [00:00<00:00, 195.28epoch/s, test_loss=6.46, train_loss=5.96]\n",
            "Epoch 258/1000: 100%|██████████| 1/1 [00:00<00:00, 91.14epoch/s, test_loss=6.5, train_loss=5.83]\n",
            "Epoch 259/1000: 100%|██████████| 1/1 [00:00<00:00, 150.98epoch/s, test_loss=6.56, train_loss=5.86]\n",
            "Epoch 260/1000: 100%|██████████| 1/1 [00:00<00:00, 170.93epoch/s, test_loss=6.47, train_loss=5.91]\n",
            "Epoch 261/1000: 100%|██████████| 1/1 [00:00<00:00, 125.24epoch/s, test_loss=6.38, train_loss=5.92]\n",
            "Epoch 262/1000: 100%|██████████| 1/1 [00:00<00:00, 378.34epoch/s, test_loss=6.38, train_loss=5.86]\n",
            "Epoch 263/1000: 100%|██████████| 1/1 [00:00<00:00, 215.85epoch/s, test_loss=6.39, train_loss=5.83]\n",
            "Epoch 264/1000: 100%|██████████| 1/1 [00:00<00:00, 195.02epoch/s, test_loss=6.33, train_loss=5.82]\n",
            "Epoch 265/1000: 100%|██████████| 1/1 [00:00<00:00, 119.60epoch/s, test_loss=6.51, train_loss=5.81]\n",
            "Epoch 266/1000: 100%|██████████| 1/1 [00:00<00:00, 178.41epoch/s, test_loss=6.61, train_loss=5.96]\n",
            "Epoch 267/1000: 100%|██████████| 1/1 [00:00<00:00, 301.51epoch/s, test_loss=6.54, train_loss=5.88]\n",
            "Epoch 268/1000: 100%|██████████| 1/1 [00:00<00:00, 367.92epoch/s, test_loss=6.61, train_loss=5.86]\n",
            "Epoch 269/1000: 100%|██████████| 1/1 [00:00<00:00, 241.69epoch/s, test_loss=6.57, train_loss=5.78]\n",
            "Epoch 270/1000: 100%|██████████| 1/1 [00:00<00:00, 244.17epoch/s, test_loss=6.45, train_loss=5.88]\n",
            "Epoch 271/1000: 100%|██████████| 1/1 [00:00<00:00, 197.05epoch/s, test_loss=6.47, train_loss=5.89]\n",
            "Epoch 272/1000: 100%|██████████| 1/1 [00:00<00:00, 236.51epoch/s, test_loss=6.5, train_loss=5.87]\n",
            "Epoch 273/1000: 100%|██████████| 1/1 [00:00<00:00, 215.90epoch/s, test_loss=6.56, train_loss=5.87]\n",
            "Epoch 274/1000: 100%|██████████| 1/1 [00:00<00:00, 312.15epoch/s, test_loss=6.42, train_loss=5.93]\n",
            "Epoch 275/1000: 100%|██████████| 1/1 [00:00<00:00, 85.33epoch/s, test_loss=6.3, train_loss=5.95]\n",
            "Epoch 276/1000: 100%|██████████| 1/1 [00:00<00:00, 128.89epoch/s, test_loss=6.51, train_loss=5.93]\n",
            "Epoch 277/1000: 100%|██████████| 1/1 [00:00<00:00, 181.66epoch/s, test_loss=6.49, train_loss=5.81]\n",
            "Epoch 278/1000: 100%|██████████| 1/1 [00:00<00:00, 131.35epoch/s, test_loss=6.43, train_loss=5.85]\n",
            "Epoch 279/1000: 100%|██████████| 1/1 [00:00<00:00, 75.21epoch/s, test_loss=6.41, train_loss=5.8]\n",
            "Epoch 280/1000: 100%|██████████| 1/1 [00:00<00:00, 101.15epoch/s, test_loss=6.48, train_loss=5.83]\n",
            "Epoch 281/1000: 100%|██████████| 1/1 [00:00<00:00, 124.38epoch/s, test_loss=6.31, train_loss=5.83]\n",
            "Epoch 282/1000: 100%|██████████| 1/1 [00:00<00:00, 94.99epoch/s, test_loss=6.37, train_loss=5.89]\n",
            "Epoch 283/1000: 100%|██████████| 1/1 [00:00<00:00, 162.27epoch/s, test_loss=6.47, train_loss=5.9]\n",
            "Epoch 284/1000: 100%|██████████| 1/1 [00:00<00:00, 187.64epoch/s, test_loss=6.43, train_loss=5.83]\n",
            "Epoch 285/1000: 100%|██████████| 1/1 [00:00<00:00, 199.74epoch/s, test_loss=6.4, train_loss=5.86]\n",
            "Epoch 286/1000: 100%|██████████| 1/1 [00:00<00:00, 195.38epoch/s, test_loss=6.44, train_loss=5.85]\n",
            "Epoch 287/1000: 100%|██████████| 1/1 [00:00<00:00, 191.86epoch/s, test_loss=6.44, train_loss=5.84]\n",
            "Epoch 288/1000: 100%|██████████| 1/1 [00:00<00:00, 183.18epoch/s, test_loss=6.36, train_loss=5.83]\n",
            "Epoch 289/1000: 100%|██████████| 1/1 [00:00<00:00, 198.95epoch/s, test_loss=6.44, train_loss=5.86]\n",
            "Epoch 290/1000: 100%|██████████| 1/1 [00:00<00:00, 136.19epoch/s, test_loss=6.53, train_loss=5.87]\n",
            "Epoch 291/1000: 100%|██████████| 1/1 [00:00<00:00, 206.52epoch/s, test_loss=6.38, train_loss=5.93]\n",
            "Epoch 292/1000: 100%|██████████| 1/1 [00:00<00:00, 88.66epoch/s, test_loss=6.47, train_loss=5.77]\n",
            "Epoch 293/1000: 100%|██████████| 1/1 [00:00<00:00, 97.85epoch/s, test_loss=6.55, train_loss=5.81]\n",
            "Epoch 294/1000: 100%|██████████| 1/1 [00:00<00:00, 230.18epoch/s, test_loss=6.4, train_loss=5.78]\n",
            "Epoch 295/1000: 100%|██████████| 1/1 [00:00<00:00, 223.21epoch/s, test_loss=6.47, train_loss=5.91]\n",
            "Epoch 296/1000: 100%|██████████| 1/1 [00:00<00:00, 207.39epoch/s, test_loss=6.32, train_loss=5.85]\n",
            "Epoch 297/1000: 100%|██████████| 1/1 [00:00<00:00, 253.92epoch/s, test_loss=6.46, train_loss=5.78]\n",
            "Epoch 298/1000: 100%|██████████| 1/1 [00:00<00:00, 89.22epoch/s, test_loss=6.35, train_loss=5.95]\n",
            "Epoch 299/1000: 100%|██████████| 1/1 [00:00<00:00, 130.08epoch/s, test_loss=6.44, train_loss=5.87]\n",
            "Epoch 300/1000: 100%|██████████| 1/1 [00:00<00:00, 287.18epoch/s, test_loss=6.38, train_loss=5.82]\n",
            "Epoch 301/1000: 100%|██████████| 1/1 [00:00<00:00, 175.99epoch/s, test_loss=6.5, train_loss=5.81]\n",
            "Epoch 302/1000: 100%|██████████| 1/1 [00:00<00:00, 163.64epoch/s, test_loss=6.54, train_loss=5.79]\n",
            "Epoch 303/1000: 100%|██████████| 1/1 [00:00<00:00, 370.91epoch/s, test_loss=6.65, train_loss=5.79]\n",
            "Epoch 304/1000: 100%|██████████| 1/1 [00:00<00:00, 231.84epoch/s, test_loss=6.52, train_loss=5.73]\n",
            "Epoch 305/1000: 100%|██████████| 1/1 [00:00<00:00, 333.73epoch/s, test_loss=6.45, train_loss=5.9]\n",
            "Epoch 306/1000: 100%|██████████| 1/1 [00:00<00:00, 201.13epoch/s, test_loss=6.37, train_loss=5.97]\n",
            "Epoch 307/1000: 100%|██████████| 1/1 [00:00<00:00, 205.94epoch/s, test_loss=6.49, train_loss=5.88]\n",
            "Epoch 308/1000: 100%|██████████| 1/1 [00:00<00:00, 144.47epoch/s, test_loss=6.31, train_loss=5.83]\n",
            "Epoch 309/1000: 100%|██████████| 1/1 [00:00<00:00, 242.70epoch/s, test_loss=6.41, train_loss=5.87]\n",
            "Epoch 310/1000: 100%|██████████| 1/1 [00:00<00:00, 179.63epoch/s, test_loss=6.52, train_loss=5.86]\n",
            "Epoch 311/1000: 100%|██████████| 1/1 [00:00<00:00, 224.04epoch/s, test_loss=6.4, train_loss=5.82]\n",
            "Epoch 312/1000: 100%|██████████| 1/1 [00:00<00:00, 214.92epoch/s, test_loss=6.45, train_loss=5.84]\n",
            "Epoch 313/1000: 100%|██████████| 1/1 [00:00<00:00, 220.86epoch/s, test_loss=6.42, train_loss=5.84]\n",
            "Epoch 314/1000: 100%|██████████| 1/1 [00:00<00:00, 220.21epoch/s, test_loss=6.65, train_loss=5.67]\n",
            "Epoch 315/1000: 100%|██████████| 1/1 [00:00<00:00, 232.62epoch/s, test_loss=6.53, train_loss=5.84]\n",
            "Epoch 316/1000: 100%|██████████| 1/1 [00:00<00:00, 85.01epoch/s, test_loss=6.39, train_loss=5.88]\n",
            "Epoch 317/1000: 100%|██████████| 1/1 [00:00<00:00, 219.15epoch/s, test_loss=6.43, train_loss=5.86]\n",
            "Epoch 318/1000: 100%|██████████| 1/1 [00:00<00:00, 228.83epoch/s, test_loss=6.38, train_loss=5.82]\n",
            "Epoch 319/1000: 100%|██████████| 1/1 [00:00<00:00, 218.23epoch/s, test_loss=6.48, train_loss=5.83]\n",
            "Epoch 320/1000: 100%|██████████| 1/1 [00:00<00:00, 220.72epoch/s, test_loss=6.46, train_loss=5.83]\n",
            "Epoch 321/1000: 100%|██████████| 1/1 [00:00<00:00, 236.61epoch/s, test_loss=6.5, train_loss=5.74]\n",
            "Epoch 322/1000: 100%|██████████| 1/1 [00:00<00:00, 118.46epoch/s, test_loss=6.47, train_loss=5.77]\n",
            "Epoch 323/1000: 100%|██████████| 1/1 [00:00<00:00, 169.13epoch/s, test_loss=6.58, train_loss=5.89]\n",
            "Epoch 324/1000: 100%|██████████| 1/1 [00:00<00:00, 212.43epoch/s, test_loss=6.43, train_loss=5.85]\n",
            "Epoch 325/1000: 100%|██████████| 1/1 [00:00<00:00, 145.94epoch/s, test_loss=6.39, train_loss=5.79]\n",
            "Epoch 326/1000: 100%|██████████| 1/1 [00:00<00:00, 88.11epoch/s, test_loss=6.4, train_loss=5.72]\n",
            "Epoch 327/1000: 100%|██████████| 1/1 [00:00<00:00, 120.29epoch/s, test_loss=6.55, train_loss=5.8]\n",
            "Epoch 328/1000: 100%|██████████| 1/1 [00:00<00:00, 192.26epoch/s, test_loss=6.44, train_loss=5.77]\n",
            "Epoch 329/1000: 100%|██████████| 1/1 [00:00<00:00, 178.44epoch/s, test_loss=6.39, train_loss=5.76]\n",
            "Epoch 330/1000: 100%|██████████| 1/1 [00:00<00:00, 280.42epoch/s, test_loss=6.56, train_loss=5.8]\n",
            "Epoch 331/1000: 100%|██████████| 1/1 [00:00<00:00, 237.56epoch/s, test_loss=6.57, train_loss=5.76]\n",
            "Epoch 332/1000: 100%|██████████| 1/1 [00:00<00:00, 206.02epoch/s, test_loss=6.5, train_loss=5.83]\n",
            "Epoch 333/1000: 100%|██████████| 1/1 [00:00<00:00, 137.69epoch/s, test_loss=6.4, train_loss=5.75]\n",
            "Epoch 334/1000: 100%|██████████| 1/1 [00:00<00:00, 145.65epoch/s, test_loss=6.6, train_loss=5.74]\n",
            "Epoch 335/1000: 100%|██████████| 1/1 [00:00<00:00, 90.66epoch/s, test_loss=6.43, train_loss=5.78]\n",
            "Epoch 336/1000: 100%|██████████| 1/1 [00:00<00:00, 362.17epoch/s, test_loss=6.32, train_loss=5.8]\n",
            "Epoch 337/1000: 100%|██████████| 1/1 [00:00<00:00, 179.64epoch/s, test_loss=6.55, train_loss=5.74]\n",
            "Epoch 338/1000: 100%|██████████| 1/1 [00:00<00:00, 172.51epoch/s, test_loss=6.37, train_loss=5.83]\n",
            "Epoch 339/1000: 100%|██████████| 1/1 [00:00<00:00, 172.12epoch/s, test_loss=6.67, train_loss=5.8]\n",
            "Epoch 340/1000: 100%|██████████| 1/1 [00:00<00:00, 188.25epoch/s, test_loss=6.41, train_loss=5.91]\n",
            "Epoch 341/1000: 100%|██████████| 1/1 [00:00<00:00, 200.89epoch/s, test_loss=6.34, train_loss=5.94]\n",
            "Epoch 342/1000: 100%|██████████| 1/1 [00:00<00:00, 184.92epoch/s, test_loss=6.31, train_loss=5.8]\n",
            "Epoch 343/1000: 100%|██████████| 1/1 [00:00<00:00, 109.16epoch/s, test_loss=6.42, train_loss=5.78]\n",
            "Epoch 344/1000: 100%|██████████| 1/1 [00:00<00:00, 148.97epoch/s, test_loss=6.46, train_loss=5.84]\n",
            "Epoch 345/1000: 100%|██████████| 1/1 [00:00<00:00, 223.10epoch/s, test_loss=6.61, train_loss=5.85]\n",
            "Epoch 346/1000: 100%|██████████| 1/1 [00:00<00:00, 231.22epoch/s, test_loss=6.46, train_loss=5.83]\n",
            "Epoch 347/1000: 100%|██████████| 1/1 [00:00<00:00, 384.23epoch/s, test_loss=6.44, train_loss=5.77]\n",
            "Epoch 348/1000: 100%|██████████| 1/1 [00:00<00:00, 182.77epoch/s, test_loss=6.57, train_loss=5.82]\n",
            "Epoch 349/1000: 100%|██████████| 1/1 [00:00<00:00, 68.37epoch/s, test_loss=6.38, train_loss=5.71]\n",
            "Epoch 350/1000: 100%|██████████| 1/1 [00:00<00:00, 69.53epoch/s, test_loss=6.5, train_loss=5.78]\n",
            "Epoch 351/1000: 100%|██████████| 1/1 [00:00<00:00, 96.24epoch/s, test_loss=6.52, train_loss=5.8]\n",
            "Epoch 352/1000: 100%|██████████| 1/1 [00:00<00:00, 75.93epoch/s, test_loss=6.62, train_loss=5.72]\n",
            "Epoch 353/1000: 100%|██████████| 1/1 [00:00<00:00, 186.36epoch/s, test_loss=6.41, train_loss=5.78]\n",
            "Epoch 354/1000: 100%|██████████| 1/1 [00:00<00:00, 163.85epoch/s, test_loss=6.34, train_loss=5.77]\n",
            "Epoch 355/1000: 100%|██████████| 1/1 [00:00<00:00, 86.21epoch/s, test_loss=6.62, train_loss=5.73]\n",
            "Epoch 356/1000: 100%|██████████| 1/1 [00:00<00:00, 222.56epoch/s, test_loss=6.55, train_loss=5.82]\n",
            "Epoch 357/1000: 100%|██████████| 1/1 [00:00<00:00, 186.05epoch/s, test_loss=6.47, train_loss=5.85]\n",
            "Epoch 358/1000: 100%|██████████| 1/1 [00:00<00:00, 247.00epoch/s, test_loss=6.37, train_loss=5.85]\n",
            "Epoch 359/1000: 100%|██████████| 1/1 [00:00<00:00, 83.21epoch/s, test_loss=6.37, train_loss=5.83]\n",
            "Epoch 360/1000: 100%|██████████| 1/1 [00:00<00:00, 210.57epoch/s, test_loss=6.44, train_loss=5.8]\n",
            "Epoch 361/1000: 100%|██████████| 1/1 [00:00<00:00, 163.80epoch/s, test_loss=6.35, train_loss=5.86]\n",
            "Epoch 362/1000: 100%|██████████| 1/1 [00:00<00:00, 382.52epoch/s, test_loss=6.4, train_loss=5.8]\n",
            "Epoch 363/1000: 100%|██████████| 1/1 [00:00<00:00, 58.89epoch/s, test_loss=6.42, train_loss=5.78]\n",
            "Epoch 364/1000: 100%|██████████| 1/1 [00:00<00:00, 181.59epoch/s, test_loss=6.41, train_loss=5.8]\n",
            "Epoch 365/1000: 100%|██████████| 1/1 [00:00<00:00, 64.74epoch/s, test_loss=6.41, train_loss=5.84]\n",
            "Epoch 366/1000: 100%|██████████| 1/1 [00:00<00:00, 178.31epoch/s, test_loss=6.48, train_loss=5.8]\n",
            "Epoch 367/1000: 100%|██████████| 1/1 [00:00<00:00, 255.22epoch/s, test_loss=6.51, train_loss=5.79]\n",
            "Epoch 368/1000: 100%|██████████| 1/1 [00:00<00:00, 381.23epoch/s, test_loss=6.44, train_loss=5.84]\n",
            "Epoch 369/1000: 100%|██████████| 1/1 [00:00<00:00, 234.94epoch/s, test_loss=6.6, train_loss=5.72]\n",
            "Epoch 370/1000: 100%|██████████| 1/1 [00:00<00:00, 134.35epoch/s, test_loss=6.43, train_loss=5.79]\n",
            "Epoch 371/1000: 100%|██████████| 1/1 [00:00<00:00, 211.73epoch/s, test_loss=6.55, train_loss=5.81]\n",
            "Epoch 372/1000: 100%|██████████| 1/1 [00:00<00:00, 375.40epoch/s, test_loss=6.22, train_loss=5.77]\n",
            "Epoch 373/1000: 100%|██████████| 1/1 [00:00<00:00, 81.45epoch/s, test_loss=6.3, train_loss=5.87]\n",
            "Epoch 374/1000: 100%|██████████| 1/1 [00:00<00:00, 118.54epoch/s, test_loss=6.26, train_loss=5.76]\n",
            "Epoch 375/1000: 100%|██████████| 1/1 [00:00<00:00, 251.16epoch/s, test_loss=6.26, train_loss=5.74]\n",
            "Epoch 376/1000: 100%|██████████| 1/1 [00:00<00:00, 261.02epoch/s, test_loss=6.43, train_loss=5.82]\n",
            "Epoch 377/1000: 100%|██████████| 1/1 [00:00<00:00, 356.84epoch/s, test_loss=6.34, train_loss=5.78]\n",
            "Epoch 378/1000: 100%|██████████| 1/1 [00:00<00:00, 241.05epoch/s, test_loss=6.49, train_loss=5.78]\n",
            "Epoch 379/1000: 100%|██████████| 1/1 [00:00<00:00, 351.52epoch/s, test_loss=6.5, train_loss=5.78]\n",
            "Epoch 380/1000: 100%|██████████| 1/1 [00:00<00:00, 194.10epoch/s, test_loss=6.34, train_loss=5.86]\n",
            "Epoch 381/1000: 100%|██████████| 1/1 [00:00<00:00, 204.94epoch/s, test_loss=6.58, train_loss=5.76]\n",
            "Epoch 382/1000: 100%|██████████| 1/1 [00:00<00:00, 110.37epoch/s, test_loss=6.46, train_loss=5.76]\n",
            "Epoch 383/1000: 100%|██████████| 1/1 [00:00<00:00, 266.58epoch/s, test_loss=6.44, train_loss=5.69]\n",
            "Epoch 384/1000: 100%|██████████| 1/1 [00:00<00:00, 185.88epoch/s, test_loss=6.39, train_loss=5.73]\n",
            "Epoch 385/1000: 100%|██████████| 1/1 [00:00<00:00, 235.64epoch/s, test_loss=6.55, train_loss=5.7]\n",
            "Epoch 386/1000: 100%|██████████| 1/1 [00:00<00:00, 282.98epoch/s, test_loss=6.39, train_loss=5.91]\n",
            "Epoch 387/1000: 100%|██████████| 1/1 [00:00<00:00, 101.80epoch/s, test_loss=6.4, train_loss=5.72]\n",
            "Epoch 388/1000: 100%|██████████| 1/1 [00:00<00:00, 198.66epoch/s, test_loss=6.51, train_loss=5.8]\n",
            "Epoch 389/1000: 100%|██████████| 1/1 [00:00<00:00, 218.88epoch/s, test_loss=6.63, train_loss=5.71]\n",
            "Epoch 390/1000: 100%|██████████| 1/1 [00:00<00:00, 223.24epoch/s, test_loss=6.38, train_loss=5.84]\n",
            "Epoch 391/1000: 100%|██████████| 1/1 [00:00<00:00, 219.06epoch/s, test_loss=6.46, train_loss=5.79]\n",
            "Epoch 392/1000: 100%|██████████| 1/1 [00:00<00:00, 223.43epoch/s, test_loss=6.47, train_loss=5.63]\n",
            "Epoch 393/1000: 100%|██████████| 1/1 [00:00<00:00, 218.91epoch/s, test_loss=6.49, train_loss=5.81]\n",
            "Epoch 394/1000: 100%|██████████| 1/1 [00:00<00:00, 137.02epoch/s, test_loss=6.51, train_loss=5.76]\n",
            "Epoch 395/1000: 100%|██████████| 1/1 [00:00<00:00, 234.65epoch/s, test_loss=6.56, train_loss=5.65]\n",
            "Epoch 396/1000: 100%|██████████| 1/1 [00:00<00:00, 150.90epoch/s, test_loss=6.4, train_loss=5.73]\n",
            "Epoch 397/1000: 100%|██████████| 1/1 [00:00<00:00, 125.60epoch/s, test_loss=6.65, train_loss=5.79]\n",
            "Epoch 398/1000: 100%|██████████| 1/1 [00:00<00:00, 229.59epoch/s, test_loss=6.48, train_loss=5.73]\n",
            "Epoch 399/1000: 100%|██████████| 1/1 [00:00<00:00, 173.66epoch/s, test_loss=6.26, train_loss=5.83]\n",
            "Epoch 400/1000: 100%|██████████| 1/1 [00:00<00:00, 227.58epoch/s, test_loss=6.44, train_loss=5.77]\n",
            "Epoch 401/1000: 100%|██████████| 1/1 [00:00<00:00, 99.42epoch/s, test_loss=6.4, train_loss=5.84]\n",
            "Epoch 402/1000: 100%|██████████| 1/1 [00:00<00:00, 237.09epoch/s, test_loss=6.54, train_loss=5.72]\n",
            "Epoch 403/1000: 100%|██████████| 1/1 [00:00<00:00, 120.09epoch/s, test_loss=6.38, train_loss=5.66]\n",
            "Epoch 404/1000: 100%|██████████| 1/1 [00:00<00:00, 207.93epoch/s, test_loss=6.41, train_loss=5.7]\n",
            "Epoch 405/1000: 100%|██████████| 1/1 [00:00<00:00, 131.36epoch/s, test_loss=6.56, train_loss=5.79]\n",
            "Epoch 406/1000: 100%|██████████| 1/1 [00:00<00:00, 130.99epoch/s, test_loss=6.25, train_loss=5.81]\n",
            "Epoch 407/1000: 100%|██████████| 1/1 [00:00<00:00, 204.15epoch/s, test_loss=6.37, train_loss=5.69]\n",
            "Epoch 408/1000: 100%|██████████| 1/1 [00:00<00:00, 175.23epoch/s, test_loss=6.45, train_loss=5.67]\n",
            "Epoch 409/1000: 100%|██████████| 1/1 [00:00<00:00, 136.32epoch/s, test_loss=6.42, train_loss=5.72]\n",
            "Epoch 410/1000: 100%|██████████| 1/1 [00:00<00:00, 142.39epoch/s, test_loss=6.5, train_loss=5.73]\n",
            "Epoch 411/1000: 100%|██████████| 1/1 [00:00<00:00, 105.69epoch/s, test_loss=6.42, train_loss=5.77]\n",
            "Epoch 412/1000: 100%|██████████| 1/1 [00:00<00:00, 260.29epoch/s, test_loss=6.45, train_loss=5.79]\n",
            "Epoch 413/1000: 100%|██████████| 1/1 [00:00<00:00, 100.37epoch/s, test_loss=6.41, train_loss=5.72]\n",
            "Epoch 414/1000: 100%|██████████| 1/1 [00:00<00:00, 255.21epoch/s, test_loss=6.41, train_loss=5.77]\n",
            "Epoch 415/1000: 100%|██████████| 1/1 [00:00<00:00, 219.90epoch/s, test_loss=6.47, train_loss=5.63]\n",
            "Epoch 416/1000: 100%|██████████| 1/1 [00:00<00:00, 145.93epoch/s, test_loss=6.39, train_loss=5.79]\n",
            "Epoch 417/1000: 100%|██████████| 1/1 [00:00<00:00, 207.46epoch/s, test_loss=6.38, train_loss=5.73]\n",
            "Epoch 418/1000: 100%|██████████| 1/1 [00:00<00:00, 295.71epoch/s, test_loss=6.29, train_loss=5.79]\n",
            "Epoch 419/1000: 100%|██████████| 1/1 [00:00<00:00, 106.74epoch/s, test_loss=6.33, train_loss=5.73]\n",
            "Epoch 420/1000: 100%|██████████| 1/1 [00:00<00:00, 108.70epoch/s, test_loss=6.63, train_loss=5.71]\n",
            "Epoch 421/1000: 100%|██████████| 1/1 [00:00<00:00, 187.98epoch/s, test_loss=6.44, train_loss=5.83]\n",
            "Epoch 422/1000: 100%|██████████| 1/1 [00:00<00:00, 211.76epoch/s, test_loss=6.44, train_loss=5.8]\n",
            "Epoch 423/1000: 100%|██████████| 1/1 [00:00<00:00, 357.21epoch/s, test_loss=6.37, train_loss=5.74]\n",
            "Epoch 424/1000: 100%|██████████| 1/1 [00:00<00:00, 250.48epoch/s, test_loss=6.44, train_loss=5.76]\n",
            "Epoch 425/1000: 100%|██████████| 1/1 [00:00<00:00, 227.83epoch/s, test_loss=6.44, train_loss=5.78]\n",
            "Epoch 426/1000: 100%|██████████| 1/1 [00:00<00:00, 353.32epoch/s, test_loss=6.37, train_loss=5.7]\n",
            "Epoch 427/1000: 100%|██████████| 1/1 [00:00<00:00, 177.82epoch/s, test_loss=6.48, train_loss=5.79]\n",
            "Epoch 428/1000: 100%|██████████| 1/1 [00:00<00:00, 152.11epoch/s, test_loss=6.46, train_loss=5.76]\n",
            "Epoch 429/1000: 100%|██████████| 1/1 [00:00<00:00, 105.03epoch/s, test_loss=6.52, train_loss=5.64]\n",
            "Epoch 430/1000: 100%|██████████| 1/1 [00:00<00:00, 193.65epoch/s, test_loss=6.46, train_loss=5.69]\n",
            "Epoch 431/1000: 100%|██████████| 1/1 [00:00<00:00, 261.88epoch/s, test_loss=6.46, train_loss=5.77]\n",
            "Epoch 432/1000: 100%|██████████| 1/1 [00:00<00:00, 198.42epoch/s, test_loss=6.42, train_loss=5.77]\n",
            "Epoch 433/1000: 100%|██████████| 1/1 [00:00<00:00, 194.15epoch/s, test_loss=6.51, train_loss=5.73]\n",
            "Epoch 434/1000: 100%|██████████| 1/1 [00:00<00:00, 138.44epoch/s, test_loss=6.38, train_loss=5.77]\n",
            "Epoch 435/1000: 100%|██████████| 1/1 [00:00<00:00, 342.06epoch/s, test_loss=6.49, train_loss=5.75]\n",
            "Epoch 436/1000: 100%|██████████| 1/1 [00:00<00:00, 159.16epoch/s, test_loss=6.28, train_loss=5.7]\n",
            "Epoch 437/1000: 100%|██████████| 1/1 [00:00<00:00, 225.22epoch/s, test_loss=6.5, train_loss=5.72]\n",
            "Epoch 438/1000: 100%|██████████| 1/1 [00:00<00:00, 75.01epoch/s, test_loss=6.4, train_loss=5.75]\n",
            "Epoch 439/1000: 100%|██████████| 1/1 [00:00<00:00, 363.65epoch/s, test_loss=6.25, train_loss=5.7]\n",
            "Epoch 440/1000: 100%|██████████| 1/1 [00:00<00:00, 236.50epoch/s, test_loss=6.49, train_loss=5.81]\n",
            "Epoch 441/1000: 100%|██████████| 1/1 [00:00<00:00, 170.40epoch/s, test_loss=6.32, train_loss=5.76]\n",
            "Epoch 442/1000: 100%|██████████| 1/1 [00:00<00:00, 275.09epoch/s, test_loss=6.38, train_loss=5.81]\n",
            "Epoch 443/1000: 100%|██████████| 1/1 [00:00<00:00, 147.35epoch/s, test_loss=6.46, train_loss=5.81]\n",
            "Epoch 444/1000: 100%|██████████| 1/1 [00:00<00:00, 219.48epoch/s, test_loss=6.42, train_loss=5.7]\n",
            "Epoch 445/1000: 100%|██████████| 1/1 [00:00<00:00, 319.03epoch/s, test_loss=6.41, train_loss=5.66]\n",
            "Epoch 446/1000: 100%|██████████| 1/1 [00:00<00:00, 158.74epoch/s, test_loss=6.33, train_loss=5.66]\n",
            "Epoch 447/1000: 100%|██████████| 1/1 [00:00<00:00, 224.23epoch/s, test_loss=6.48, train_loss=5.79]\n",
            "Epoch 448/1000: 100%|██████████| 1/1 [00:00<00:00, 197.44epoch/s, test_loss=6.39, train_loss=5.67]\n",
            "Epoch 449/1000: 100%|██████████| 1/1 [00:00<00:00, 352.49epoch/s, test_loss=6.53, train_loss=5.71]\n",
            "Epoch 450/1000: 100%|██████████| 1/1 [00:00<00:00, 89.79epoch/s, test_loss=6.38, train_loss=5.69]\n",
            "Epoch 451/1000: 100%|██████████| 1/1 [00:00<00:00, 142.21epoch/s, test_loss=6.24, train_loss=5.71]\n",
            "Epoch 452/1000: 100%|██████████| 1/1 [00:00<00:00, 66.91epoch/s, test_loss=6.5, train_loss=5.56]\n",
            "Epoch 453/1000: 100%|██████████| 1/1 [00:00<00:00, 244.65epoch/s, test_loss=6.39, train_loss=5.76]\n",
            "Epoch 454/1000: 100%|██████████| 1/1 [00:00<00:00, 121.51epoch/s, test_loss=6.66, train_loss=5.71]\n",
            "Epoch 455/1000: 100%|██████████| 1/1 [00:00<00:00, 192.37epoch/s, test_loss=6.47, train_loss=5.84]\n",
            "Epoch 456/1000: 100%|██████████| 1/1 [00:00<00:00, 232.17epoch/s, test_loss=6.32, train_loss=5.71]\n",
            "Epoch 457/1000: 100%|██████████| 1/1 [00:00<00:00, 260.08epoch/s, test_loss=6.43, train_loss=5.79]\n",
            "Epoch 458/1000: 100%|██████████| 1/1 [00:00<00:00, 221.78epoch/s, test_loss=6.31, train_loss=5.74]\n",
            "Epoch 459/1000: 100%|██████████| 1/1 [00:00<00:00, 257.04epoch/s, test_loss=6.35, train_loss=5.75]\n",
            "Epoch 460/1000: 100%|██████████| 1/1 [00:00<00:00, 214.65epoch/s, test_loss=6.36, train_loss=5.76]\n",
            "Epoch 461/1000: 100%|██████████| 1/1 [00:00<00:00, 233.33epoch/s, test_loss=6.42, train_loss=5.72]\n",
            "Epoch 462/1000: 100%|██████████| 1/1 [00:00<00:00, 259.93epoch/s, test_loss=6.32, train_loss=5.6]\n",
            "Epoch 463/1000: 100%|██████████| 1/1 [00:00<00:00, 249.79epoch/s, test_loss=6.46, train_loss=5.66]\n",
            "Epoch 464/1000: 100%|██████████| 1/1 [00:00<00:00, 194.67epoch/s, test_loss=6.53, train_loss=5.63]\n",
            "Epoch 465/1000: 100%|██████████| 1/1 [00:00<00:00, 331.51epoch/s, test_loss=6.4, train_loss=5.68]\n",
            "Epoch 466/1000: 100%|██████████| 1/1 [00:00<00:00, 121.46epoch/s, test_loss=6.41, train_loss=5.74]\n",
            "Epoch 467/1000: 100%|██████████| 1/1 [00:00<00:00, 237.68epoch/s, test_loss=6.32, train_loss=5.69]\n",
            "Epoch 468/1000: 100%|██████████| 1/1 [00:00<00:00, 255.50epoch/s, test_loss=6.28, train_loss=5.72]\n",
            "Epoch 469/1000: 100%|██████████| 1/1 [00:00<00:00, 244.57epoch/s, test_loss=6.46, train_loss=5.67]\n",
            "Epoch 470/1000: 100%|██████████| 1/1 [00:00<00:00, 218.39epoch/s, test_loss=6.38, train_loss=5.63]\n",
            "Epoch 471/1000: 100%|██████████| 1/1 [00:00<00:00, 237.73epoch/s, test_loss=6.51, train_loss=5.7]\n",
            "Epoch 472/1000: 100%|██████████| 1/1 [00:00<00:00, 241.52epoch/s, test_loss=6.44, train_loss=5.65]\n",
            "Epoch 473/1000: 100%|██████████| 1/1 [00:00<00:00, 195.82epoch/s, test_loss=6.25, train_loss=5.75]\n",
            "Epoch 474/1000: 100%|██████████| 1/1 [00:00<00:00, 169.23epoch/s, test_loss=6.37, train_loss=5.7]\n",
            "Epoch 475/1000: 100%|██████████| 1/1 [00:00<00:00, 95.39epoch/s, test_loss=6.64, train_loss=5.65]\n",
            "Epoch 476/1000: 100%|██████████| 1/1 [00:00<00:00, 248.88epoch/s, test_loss=6.36, train_loss=5.76]\n",
            "Epoch 477/1000: 100%|██████████| 1/1 [00:00<00:00, 80.76epoch/s, test_loss=6.28, train_loss=5.78]\n",
            "Epoch 478/1000: 100%|██████████| 1/1 [00:00<00:00, 350.49epoch/s, test_loss=6.58, train_loss=5.73]\n",
            "Epoch 479/1000: 100%|██████████| 1/1 [00:00<00:00, 94.91epoch/s, test_loss=6.48, train_loss=5.73]\n",
            "Epoch 480/1000: 100%|██████████| 1/1 [00:00<00:00, 190.13epoch/s, test_loss=6.43, train_loss=5.7]\n",
            "Epoch 481/1000: 100%|██████████| 1/1 [00:00<00:00, 150.75epoch/s, test_loss=6.36, train_loss=5.77]\n",
            "Epoch 482/1000: 100%|██████████| 1/1 [00:00<00:00, 244.74epoch/s, test_loss=6.37, train_loss=5.8]\n",
            "Epoch 483/1000: 100%|██████████| 1/1 [00:00<00:00, 197.54epoch/s, test_loss=6.28, train_loss=5.66]\n",
            "Epoch 484/1000: 100%|██████████| 1/1 [00:00<00:00, 224.45epoch/s, test_loss=6.45, train_loss=5.66]\n",
            "Epoch 485/1000: 100%|██████████| 1/1 [00:00<00:00, 205.10epoch/s, test_loss=6.34, train_loss=5.7]\n",
            "Epoch 486/1000: 100%|██████████| 1/1 [00:00<00:00, 127.60epoch/s, test_loss=6.46, train_loss=5.75]\n",
            "Epoch 487/1000: 100%|██████████| 1/1 [00:00<00:00, 221.96epoch/s, test_loss=6.57, train_loss=5.69]\n",
            "Epoch 488/1000: 100%|██████████| 1/1 [00:00<00:00, 318.28epoch/s, test_loss=6.43, train_loss=5.67]\n",
            "Epoch 489/1000: 100%|██████████| 1/1 [00:00<00:00, 77.31epoch/s, test_loss=6.5, train_loss=5.65]\n",
            "Epoch 490/1000: 100%|██████████| 1/1 [00:00<00:00, 129.88epoch/s, test_loss=6.65, train_loss=5.58]\n",
            "Epoch 491/1000: 100%|██████████| 1/1 [00:00<00:00, 220.96epoch/s, test_loss=6.31, train_loss=5.78]\n",
            "Epoch 492/1000: 100%|██████████| 1/1 [00:00<00:00, 227.75epoch/s, test_loss=6.25, train_loss=5.68]\n",
            "Epoch 493/1000: 100%|██████████| 1/1 [00:00<00:00, 223.64epoch/s, test_loss=6.45, train_loss=5.78]\n",
            "Epoch 494/1000: 100%|██████████| 1/1 [00:00<00:00, 187.89epoch/s, test_loss=6.39, train_loss=5.64]\n",
            "Epoch 495/1000: 100%|██████████| 1/1 [00:00<00:00, 49.51epoch/s, test_loss=6.26, train_loss=5.8]\n",
            "Epoch 496/1000: 100%|██████████| 1/1 [00:00<00:00, 220.78epoch/s, test_loss=6.54, train_loss=5.64]\n",
            "Epoch 497/1000: 100%|██████████| 1/1 [00:00<00:00, 228.37epoch/s, test_loss=6.48, train_loss=5.74]\n",
            "Epoch 498/1000: 100%|██████████| 1/1 [00:00<00:00, 197.69epoch/s, test_loss=6.48, train_loss=5.68]\n",
            "Epoch 499/1000: 100%|██████████| 1/1 [00:00<00:00, 210.02epoch/s, test_loss=6.33, train_loss=5.72]\n",
            "Epoch 500/1000: 100%|██████████| 1/1 [00:00<00:00, 205.95epoch/s, test_loss=6.44, train_loss=5.66]\n",
            "Epoch 501/1000: 100%|██████████| 1/1 [00:00<00:00, 206.49epoch/s, test_loss=6.33, train_loss=5.71]\n",
            "Epoch 502/1000: 100%|██████████| 1/1 [00:00<00:00, 98.09epoch/s, test_loss=6.26, train_loss=5.64]\n",
            "Epoch 503/1000: 100%|██████████| 1/1 [00:00<00:00, 269.70epoch/s, test_loss=6.5, train_loss=5.77]\n",
            "Epoch 504/1000: 100%|██████████| 1/1 [00:00<00:00, 202.65epoch/s, test_loss=6.46, train_loss=5.65]\n",
            "Epoch 505/1000: 100%|██████████| 1/1 [00:00<00:00, 159.97epoch/s, test_loss=6.26, train_loss=5.66]\n",
            "Epoch 506/1000: 100%|██████████| 1/1 [00:00<00:00, 285.87epoch/s, test_loss=6.43, train_loss=5.65]\n",
            "Epoch 507/1000: 100%|██████████| 1/1 [00:00<00:00, 139.88epoch/s, test_loss=6.46, train_loss=5.68]\n",
            "Epoch 508/1000: 100%|██████████| 1/1 [00:00<00:00, 123.62epoch/s, test_loss=6.36, train_loss=5.76]\n",
            "Epoch 509/1000: 100%|██████████| 1/1 [00:00<00:00, 202.58epoch/s, test_loss=6.52, train_loss=5.64]\n",
            "Epoch 510/1000: 100%|██████████| 1/1 [00:00<00:00, 347.18epoch/s, test_loss=6.53, train_loss=5.66]\n",
            "Epoch 511/1000: 100%|██████████| 1/1 [00:00<00:00, 200.70epoch/s, test_loss=6.24, train_loss=5.67]\n",
            "Epoch 512/1000: 100%|██████████| 1/1 [00:00<00:00, 130.63epoch/s, test_loss=6.4, train_loss=5.57]\n",
            "Epoch 513/1000: 100%|██████████| 1/1 [00:00<00:00, 93.10epoch/s, test_loss=6.44, train_loss=5.73]\n",
            "Epoch 514/1000: 100%|██████████| 1/1 [00:00<00:00, 115.06epoch/s, test_loss=6.33, train_loss=5.73]\n",
            "Epoch 515/1000: 100%|██████████| 1/1 [00:00<00:00, 204.12epoch/s, test_loss=6.68, train_loss=5.66]\n",
            "Epoch 516/1000: 100%|██████████| 1/1 [00:00<00:00, 183.69epoch/s, test_loss=6.53, train_loss=5.6]\n",
            "Epoch 517/1000: 100%|██████████| 1/1 [00:00<00:00, 235.94epoch/s, test_loss=6.18, train_loss=5.6]\n",
            "Epoch 518/1000: 100%|██████████| 1/1 [00:00<00:00, 160.74epoch/s, test_loss=6.28, train_loss=5.65]\n",
            "Epoch 519/1000: 100%|██████████| 1/1 [00:00<00:00, 319.40epoch/s, test_loss=6.37, train_loss=5.66]\n",
            "Epoch 520/1000: 100%|██████████| 1/1 [00:00<00:00, 296.42epoch/s, test_loss=6.29, train_loss=5.61]\n",
            "Epoch 521/1000: 100%|██████████| 1/1 [00:00<00:00, 181.03epoch/s, test_loss=6.39, train_loss=5.68]\n",
            "Epoch 522/1000: 100%|██████████| 1/1 [00:00<00:00, 207.62epoch/s, test_loss=6.36, train_loss=5.69]\n",
            "Epoch 523/1000: 100%|██████████| 1/1 [00:00<00:00, 226.74epoch/s, test_loss=6.32, train_loss=5.76]\n",
            "Epoch 524/1000: 100%|██████████| 1/1 [00:00<00:00, 234.19epoch/s, test_loss=6.42, train_loss=5.68]\n",
            "Epoch 525/1000: 100%|██████████| 1/1 [00:00<00:00, 133.95epoch/s, test_loss=6.42, train_loss=5.73]\n",
            "Epoch 526/1000: 100%|██████████| 1/1 [00:00<00:00, 180.22epoch/s, test_loss=6.58, train_loss=5.51]\n",
            "Epoch 527/1000: 100%|██████████| 1/1 [00:00<00:00, 163.94epoch/s, test_loss=6.52, train_loss=5.59]\n",
            "Epoch 528/1000: 100%|██████████| 1/1 [00:00<00:00, 133.90epoch/s, test_loss=6.57, train_loss=5.68]\n",
            "Epoch 529/1000: 100%|██████████| 1/1 [00:00<00:00, 211.96epoch/s, test_loss=6.46, train_loss=5.7]\n",
            "Epoch 530/1000: 100%|██████████| 1/1 [00:00<00:00, 213.85epoch/s, test_loss=6.5, train_loss=5.67]\n",
            "Epoch 531/1000: 100%|██████████| 1/1 [00:00<00:00, 180.20epoch/s, test_loss=6.38, train_loss=5.7]\n",
            "Epoch 532/1000: 100%|██████████| 1/1 [00:00<00:00, 146.08epoch/s, test_loss=6.29, train_loss=5.67]\n",
            "Epoch 533/1000: 100%|██████████| 1/1 [00:00<00:00, 174.46epoch/s, test_loss=6.52, train_loss=5.69]\n",
            "Epoch 534/1000: 100%|██████████| 1/1 [00:00<00:00, 200.49epoch/s, test_loss=6.35, train_loss=5.69]\n",
            "Epoch 535/1000: 100%|██████████| 1/1 [00:00<00:00, 197.99epoch/s, test_loss=6.63, train_loss=5.56]\n",
            "Epoch 536/1000: 100%|██████████| 1/1 [00:00<00:00, 266.81epoch/s, test_loss=6.54, train_loss=5.55]\n",
            "Epoch 537/1000: 100%|██████████| 1/1 [00:00<00:00, 254.82epoch/s, test_loss=6.36, train_loss=5.74]\n",
            "Epoch 538/1000: 100%|██████████| 1/1 [00:00<00:00, 247.03epoch/s, test_loss=6.34, train_loss=5.69]\n",
            "Epoch 539/1000: 100%|██████████| 1/1 [00:00<00:00, 144.16epoch/s, test_loss=6.51, train_loss=5.65]\n",
            "Epoch 540/1000: 100%|██████████| 1/1 [00:00<00:00, 227.57epoch/s, test_loss=6.6, train_loss=5.73]\n",
            "Epoch 541/1000: 100%|██████████| 1/1 [00:00<00:00, 163.54epoch/s, test_loss=6.49, train_loss=5.54]\n",
            "Epoch 542/1000: 100%|██████████| 1/1 [00:00<00:00, 127.33epoch/s, test_loss=6.37, train_loss=5.57]\n",
            "Epoch 543/1000: 100%|██████████| 1/1 [00:00<00:00, 236.19epoch/s, test_loss=6.49, train_loss=5.7]\n",
            "Epoch 544/1000: 100%|██████████| 1/1 [00:00<00:00, 148.17epoch/s, test_loss=6.55, train_loss=5.64]\n",
            "Epoch 545/1000: 100%|██████████| 1/1 [00:00<00:00, 152.84epoch/s, test_loss=6.34, train_loss=5.67]\n",
            "Epoch 546/1000: 100%|██████████| 1/1 [00:00<00:00, 241.96epoch/s, test_loss=6.5, train_loss=5.81]\n",
            "Epoch 547/1000: 100%|██████████| 1/1 [00:00<00:00, 247.86epoch/s, test_loss=6.38, train_loss=5.68]\n",
            "Epoch 548/1000: 100%|██████████| 1/1 [00:00<00:00, 209.17epoch/s, test_loss=6.37, train_loss=5.64]\n",
            "Epoch 549/1000: 100%|██████████| 1/1 [00:00<00:00, 194.71epoch/s, test_loss=6.46, train_loss=5.69]\n",
            "Epoch 550/1000: 100%|██████████| 1/1 [00:00<00:00, 246.20epoch/s, test_loss=6.49, train_loss=5.69]\n",
            "Epoch 551/1000: 100%|██████████| 1/1 [00:00<00:00, 169.99epoch/s, test_loss=6.68, train_loss=5.59]\n",
            "Epoch 552/1000: 100%|██████████| 1/1 [00:00<00:00, 199.61epoch/s, test_loss=6.44, train_loss=5.69]\n",
            "Epoch 553/1000: 100%|██████████| 1/1 [00:00<00:00, 157.67epoch/s, test_loss=6.57, train_loss=5.76]\n",
            "Epoch 554/1000: 100%|██████████| 1/1 [00:00<00:00, 248.08epoch/s, test_loss=6.12, train_loss=5.67]\n",
            "Epoch 555/1000: 100%|██████████| 1/1 [00:00<00:00, 204.69epoch/s, test_loss=6.41, train_loss=5.7]\n",
            "Epoch 556/1000: 100%|██████████| 1/1 [00:00<00:00, 300.99epoch/s, test_loss=6.49, train_loss=5.66]\n",
            "Epoch 557/1000: 100%|██████████| 1/1 [00:00<00:00, 230.43epoch/s, test_loss=6.39, train_loss=5.65]\n",
            "Epoch 558/1000: 100%|██████████| 1/1 [00:00<00:00, 224.52epoch/s, test_loss=6.26, train_loss=5.67]\n",
            "Epoch 559/1000: 100%|██████████| 1/1 [00:00<00:00, 254.03epoch/s, test_loss=6.43, train_loss=5.68]\n",
            "Epoch 560/1000: 100%|██████████| 1/1 [00:00<00:00, 248.04epoch/s, test_loss=6.23, train_loss=5.63]\n",
            "Epoch 561/1000: 100%|██████████| 1/1 [00:00<00:00, 149.68epoch/s, test_loss=6.5, train_loss=5.63]\n",
            "Epoch 562/1000: 100%|██████████| 1/1 [00:00<00:00, 401.75epoch/s, test_loss=6.46, train_loss=5.48]\n",
            "Epoch 563/1000: 100%|██████████| 1/1 [00:00<00:00, 245.99epoch/s, test_loss=6.42, train_loss=5.6]\n",
            "Epoch 564/1000: 100%|██████████| 1/1 [00:00<00:00, 83.33epoch/s, test_loss=6.45, train_loss=5.66]\n",
            "Epoch 565/1000: 100%|██████████| 1/1 [00:00<00:00, 262.78epoch/s, test_loss=6.36, train_loss=5.6]\n",
            "Epoch 566/1000: 100%|██████████| 1/1 [00:00<00:00, 160.69epoch/s, test_loss=6.56, train_loss=5.59]\n",
            "Epoch 567/1000: 100%|██████████| 1/1 [00:00<00:00, 352.73epoch/s, test_loss=6.55, train_loss=5.6]\n",
            "Epoch 568/1000: 100%|██████████| 1/1 [00:00<00:00, 157.94epoch/s, test_loss=6.53, train_loss=5.69]\n",
            "Epoch 569/1000: 100%|██████████| 1/1 [00:00<00:00, 261.38epoch/s, test_loss=6.32, train_loss=5.7]\n",
            "Epoch 570/1000: 100%|██████████| 1/1 [00:00<00:00, 178.82epoch/s, test_loss=6.41, train_loss=5.75]\n",
            "Epoch 571/1000: 100%|██████████| 1/1 [00:00<00:00, 142.84epoch/s, test_loss=6.41, train_loss=5.6]\n",
            "Epoch 572/1000: 100%|██████████| 1/1 [00:00<00:00, 232.13epoch/s, test_loss=6.39, train_loss=5.7]\n",
            "Epoch 573/1000: 100%|██████████| 1/1 [00:00<00:00, 208.68epoch/s, test_loss=6.55, train_loss=5.73]\n",
            "Epoch 574/1000: 100%|██████████| 1/1 [00:00<00:00, 178.13epoch/s, test_loss=6.35, train_loss=5.65]\n",
            "Epoch 575/1000: 100%|██████████| 1/1 [00:00<00:00, 152.73epoch/s, test_loss=6.46, train_loss=5.62]\n",
            "Epoch 576/1000: 100%|██████████| 1/1 [00:00<00:00, 56.67epoch/s, test_loss=6.38, train_loss=5.71]\n",
            "Epoch 577/1000: 100%|██████████| 1/1 [00:00<00:00, 92.90epoch/s, test_loss=6.41, train_loss=5.75]\n",
            "Epoch 578/1000: 100%|██████████| 1/1 [00:00<00:00, 86.89epoch/s, test_loss=6.25, train_loss=5.63]\n",
            "Epoch 579/1000: 100%|██████████| 1/1 [00:00<00:00, 194.33epoch/s, test_loss=6.29, train_loss=5.66]\n",
            "Epoch 580/1000: 100%|██████████| 1/1 [00:00<00:00, 182.34epoch/s, test_loss=6.5, train_loss=5.58]\n",
            "Epoch 581/1000: 100%|██████████| 1/1 [00:00<00:00, 145.33epoch/s, test_loss=6.29, train_loss=5.74]\n",
            "Epoch 582/1000: 100%|██████████| 1/1 [00:00<00:00, 212.05epoch/s, test_loss=6.36, train_loss=5.62]\n",
            "Epoch 583/1000: 100%|██████████| 1/1 [00:00<00:00, 173.12epoch/s, test_loss=6.55, train_loss=5.64]\n",
            "Epoch 584/1000: 100%|██████████| 1/1 [00:00<00:00, 199.16epoch/s, test_loss=6.51, train_loss=5.61]\n",
            "Epoch 585/1000: 100%|██████████| 1/1 [00:00<00:00, 107.55epoch/s, test_loss=6.32, train_loss=5.63]\n",
            "Epoch 586/1000: 100%|██████████| 1/1 [00:00<00:00, 200.50epoch/s, test_loss=6.36, train_loss=5.71]\n",
            "Epoch 587/1000: 100%|██████████| 1/1 [00:00<00:00, 192.22epoch/s, test_loss=6.36, train_loss=5.72]\n",
            "Epoch 588/1000: 100%|██████████| 1/1 [00:00<00:00, 330.10epoch/s, test_loss=6.38, train_loss=5.58]\n",
            "Epoch 589/1000: 100%|██████████| 1/1 [00:00<00:00, 106.62epoch/s, test_loss=6.35, train_loss=5.68]\n",
            "Epoch 590/1000: 100%|██████████| 1/1 [00:00<00:00, 236.53epoch/s, test_loss=6.24, train_loss=5.63]\n",
            "Epoch 591/1000: 100%|██████████| 1/1 [00:00<00:00, 149.43epoch/s, test_loss=6.35, train_loss=5.75]\n",
            "Epoch 592/1000: 100%|██████████| 1/1 [00:00<00:00, 205.13epoch/s, test_loss=6.32, train_loss=5.59]\n",
            "Epoch 593/1000: 100%|██████████| 1/1 [00:00<00:00, 219.49epoch/s, test_loss=6.4, train_loss=5.63]\n",
            "Epoch 594/1000: 100%|██████████| 1/1 [00:00<00:00, 120.14epoch/s, test_loss=6.46, train_loss=5.68]\n",
            "Epoch 595/1000: 100%|██████████| 1/1 [00:00<00:00, 82.04epoch/s, test_loss=6.39, train_loss=5.7]\n",
            "Epoch 596/1000: 100%|██████████| 1/1 [00:00<00:00, 213.22epoch/s, test_loss=6.26, train_loss=5.66]\n",
            "Epoch 597/1000: 100%|██████████| 1/1 [00:00<00:00, 83.36epoch/s, test_loss=6.38, train_loss=5.73]\n",
            "Epoch 598/1000: 100%|██████████| 1/1 [00:00<00:00, 121.25epoch/s, test_loss=6.38, train_loss=5.68]\n",
            "Epoch 599/1000: 100%|██████████| 1/1 [00:00<00:00, 342.62epoch/s, test_loss=6.4, train_loss=5.62]\n",
            "Epoch 600/1000: 100%|██████████| 1/1 [00:00<00:00, 180.68epoch/s, test_loss=6.47, train_loss=5.72]\n",
            "Epoch 601/1000: 100%|██████████| 1/1 [00:00<00:00, 212.40epoch/s, test_loss=6.34, train_loss=5.65]\n",
            "Epoch 602/1000: 100%|██████████| 1/1 [00:00<00:00, 254.37epoch/s, test_loss=6.35, train_loss=5.59]\n",
            "Epoch 603/1000: 100%|██████████| 1/1 [00:00<00:00, 209.89epoch/s, test_loss=6.29, train_loss=5.62]\n",
            "Epoch 604/1000: 100%|██████████| 1/1 [00:00<00:00, 202.50epoch/s, test_loss=6.29, train_loss=5.7]\n",
            "Epoch 605/1000: 100%|██████████| 1/1 [00:00<00:00, 216.10epoch/s, test_loss=6.25, train_loss=5.57]\n",
            "Epoch 606/1000: 100%|██████████| 1/1 [00:00<00:00, 244.22epoch/s, test_loss=6.29, train_loss=5.55]\n",
            "Epoch 607/1000: 100%|██████████| 1/1 [00:00<00:00, 240.39epoch/s, test_loss=6.55, train_loss=5.7]\n",
            "Epoch 608/1000: 100%|██████████| 1/1 [00:00<00:00, 138.11epoch/s, test_loss=6.54, train_loss=5.7]\n",
            "Epoch 609/1000: 100%|██████████| 1/1 [00:00<00:00, 218.77epoch/s, test_loss=6.26, train_loss=5.63]\n",
            "Epoch 610/1000: 100%|██████████| 1/1 [00:00<00:00, 341.44epoch/s, test_loss=6.38, train_loss=5.62]\n",
            "Epoch 611/1000: 100%|██████████| 1/1 [00:00<00:00, 131.24epoch/s, test_loss=6.45, train_loss=5.67]\n",
            "Epoch 612/1000: 100%|██████████| 1/1 [00:00<00:00, 238.37epoch/s, test_loss=6.48, train_loss=5.65]\n",
            "Epoch 613/1000: 100%|██████████| 1/1 [00:00<00:00, 375.63epoch/s, test_loss=6.31, train_loss=5.76]\n",
            "Epoch 614/1000: 100%|██████████| 1/1 [00:00<00:00, 144.65epoch/s, test_loss=6.39, train_loss=5.51]\n",
            "Epoch 615/1000: 100%|██████████| 1/1 [00:00<00:00, 216.86epoch/s, test_loss=6.12, train_loss=5.57]\n",
            "Epoch 616/1000: 100%|██████████| 1/1 [00:00<00:00, 105.75epoch/s, test_loss=6.48, train_loss=5.63]\n",
            "Epoch 617/1000: 100%|██████████| 1/1 [00:00<00:00, 141.83epoch/s, test_loss=6.56, train_loss=5.64]\n",
            "Epoch 618/1000: 100%|██████████| 1/1 [00:00<00:00, 147.43epoch/s, test_loss=6.2, train_loss=5.72]\n",
            "Epoch 619/1000: 100%|██████████| 1/1 [00:00<00:00, 311.47epoch/s, test_loss=6.23, train_loss=5.62]\n",
            "Epoch 620/1000: 100%|██████████| 1/1 [00:00<00:00, 214.17epoch/s, test_loss=6.35, train_loss=5.51]\n",
            "Epoch 621/1000: 100%|██████████| 1/1 [00:00<00:00, 198.29epoch/s, test_loss=6.38, train_loss=5.58]\n",
            "Epoch 622/1000: 100%|██████████| 1/1 [00:00<00:00, 233.37epoch/s, test_loss=6.35, train_loss=5.72]\n",
            "Epoch 623/1000: 100%|██████████| 1/1 [00:00<00:00, 232.71epoch/s, test_loss=6.22, train_loss=5.67]\n",
            "Epoch 624/1000: 100%|██████████| 1/1 [00:00<00:00, 207.85epoch/s, test_loss=6.67, train_loss=5.57]\n",
            "Epoch 625/1000: 100%|██████████| 1/1 [00:00<00:00, 314.34epoch/s, test_loss=6.57, train_loss=5.72]\n",
            "Epoch 626/1000: 100%|██████████| 1/1 [00:00<00:00, 293.35epoch/s, test_loss=6.46, train_loss=5.59]\n",
            "Epoch 627/1000: 100%|██████████| 1/1 [00:00<00:00, 168.10epoch/s, test_loss=6.62, train_loss=5.55]\n",
            "Epoch 628/1000: 100%|██████████| 1/1 [00:00<00:00, 247.96epoch/s, test_loss=6.53, train_loss=5.54]\n",
            "Epoch 629/1000: 100%|██████████| 1/1 [00:00<00:00, 357.27epoch/s, test_loss=6.15, train_loss=5.59]\n",
            "Epoch 630/1000: 100%|██████████| 1/1 [00:00<00:00, 190.36epoch/s, test_loss=6.58, train_loss=5.56]\n",
            "Epoch 631/1000: 100%|██████████| 1/1 [00:00<00:00, 99.17epoch/s, test_loss=6.5, train_loss=5.68]\n",
            "Epoch 632/1000: 100%|██████████| 1/1 [00:00<00:00, 193.54epoch/s, test_loss=6.53, train_loss=5.61]\n",
            "Epoch 633/1000: 100%|██████████| 1/1 [00:00<00:00, 214.01epoch/s, test_loss=6.34, train_loss=5.67]\n",
            "Epoch 634/1000: 100%|██████████| 1/1 [00:00<00:00, 106.52epoch/s, test_loss=6.32, train_loss=5.71]\n",
            "Epoch 635/1000: 100%|██████████| 1/1 [00:00<00:00, 222.37epoch/s, test_loss=6.46, train_loss=5.64]\n",
            "Epoch 636/1000: 100%|██████████| 1/1 [00:00<00:00, 166.40epoch/s, test_loss=6.27, train_loss=5.73]\n",
            "Epoch 637/1000: 100%|██████████| 1/1 [00:00<00:00, 239.61epoch/s, test_loss=6.44, train_loss=5.63]\n",
            "Epoch 638/1000: 100%|██████████| 1/1 [00:00<00:00, 318.96epoch/s, test_loss=6.53, train_loss=5.59]\n",
            "Epoch 639/1000: 100%|██████████| 1/1 [00:00<00:00, 201.67epoch/s, test_loss=6.44, train_loss=5.59]\n",
            "Epoch 640/1000: 100%|██████████| 1/1 [00:00<00:00, 232.06epoch/s, test_loss=6.18, train_loss=5.53]\n",
            "Epoch 641/1000: 100%|██████████| 1/1 [00:00<00:00, 228.75epoch/s, test_loss=6.4, train_loss=5.63]\n",
            "Epoch 642/1000: 100%|██████████| 1/1 [00:00<00:00, 195.47epoch/s, test_loss=6.39, train_loss=5.64]\n",
            "Epoch 643/1000: 100%|██████████| 1/1 [00:00<00:00, 102.97epoch/s, test_loss=6.58, train_loss=5.51]\n",
            "Epoch 644/1000: 100%|██████████| 1/1 [00:00<00:00, 255.07epoch/s, test_loss=6.55, train_loss=5.56]\n",
            "Epoch 645/1000: 100%|██████████| 1/1 [00:00<00:00, 187.05epoch/s, test_loss=6.49, train_loss=5.66]\n",
            "Epoch 646/1000: 100%|██████████| 1/1 [00:00<00:00, 53.97epoch/s, test_loss=6.54, train_loss=5.53]\n",
            "Epoch 647/1000: 100%|██████████| 1/1 [00:00<00:00, 214.61epoch/s, test_loss=6.29, train_loss=5.69]\n",
            "Epoch 648/1000: 100%|██████████| 1/1 [00:00<00:00, 368.37epoch/s, test_loss=6.29, train_loss=5.74]\n",
            "Epoch 649/1000: 100%|██████████| 1/1 [00:00<00:00, 236.79epoch/s, test_loss=6.3, train_loss=5.67]\n",
            "Epoch 650/1000: 100%|██████████| 1/1 [00:00<00:00, 272.30epoch/s, test_loss=6.33, train_loss=5.63]\n",
            "Epoch 651/1000: 100%|██████████| 1/1 [00:00<00:00, 198.85epoch/s, test_loss=6.3, train_loss=5.55]\n",
            "Epoch 652/1000: 100%|██████████| 1/1 [00:00<00:00, 386.22epoch/s, test_loss=6.41, train_loss=5.69]\n",
            "Epoch 653/1000: 100%|██████████| 1/1 [00:00<00:00, 109.25epoch/s, test_loss=6.58, train_loss=5.75]\n",
            "Epoch 654/1000: 100%|██████████| 1/1 [00:00<00:00, 117.84epoch/s, test_loss=6.47, train_loss=5.58]\n",
            "Epoch 655/1000: 100%|██████████| 1/1 [00:00<00:00, 92.58epoch/s, test_loss=6.59, train_loss=5.51]\n",
            "Epoch 656/1000: 100%|██████████| 1/1 [00:00<00:00, 129.55epoch/s, test_loss=6.62, train_loss=5.59]\n",
            "Epoch 657/1000: 100%|██████████| 1/1 [00:00<00:00, 256.11epoch/s, test_loss=6.49, train_loss=5.53]\n",
            "Epoch 658/1000: 100%|██████████| 1/1 [00:00<00:00, 129.64epoch/s, test_loss=6.42, train_loss=5.51]\n",
            "Epoch 659/1000: 100%|██████████| 1/1 [00:00<00:00, 215.63epoch/s, test_loss=6.25, train_loss=5.61]\n",
            "Epoch 660/1000: 100%|██████████| 1/1 [00:00<00:00, 130.16epoch/s, test_loss=6.25, train_loss=5.53]\n",
            "Epoch 661/1000: 100%|██████████| 1/1 [00:00<00:00, 225.16epoch/s, test_loss=6.36, train_loss=5.62]\n",
            "Epoch 662/1000: 100%|██████████| 1/1 [00:00<00:00, 212.92epoch/s, test_loss=6.33, train_loss=5.64]\n",
            "Epoch 663/1000: 100%|██████████| 1/1 [00:00<00:00, 311.73epoch/s, test_loss=6.39, train_loss=5.53]\n",
            "Epoch 664/1000: 100%|██████████| 1/1 [00:00<00:00, 186.47epoch/s, test_loss=6.61, train_loss=5.56]\n",
            "Epoch 665/1000: 100%|██████████| 1/1 [00:00<00:00, 203.05epoch/s, test_loss=6.44, train_loss=5.62]\n",
            "Epoch 666/1000: 100%|██████████| 1/1 [00:00<00:00, 195.73epoch/s, test_loss=6.34, train_loss=5.53]\n",
            "Epoch 667/1000: 100%|██████████| 1/1 [00:00<00:00, 181.62epoch/s, test_loss=6.46, train_loss=5.71]\n",
            "Epoch 668/1000: 100%|██████████| 1/1 [00:00<00:00, 74.79epoch/s, test_loss=6.52, train_loss=5.64]\n",
            "Epoch 669/1000: 100%|██████████| 1/1 [00:00<00:00, 237.85epoch/s, test_loss=6.41, train_loss=5.6]\n",
            "Epoch 670/1000: 100%|██████████| 1/1 [00:00<00:00, 163.22epoch/s, test_loss=6.4, train_loss=5.84]\n",
            "Epoch 671/1000: 100%|██████████| 1/1 [00:00<00:00, 301.23epoch/s, test_loss=6.38, train_loss=5.63]\n",
            "Epoch 672/1000: 100%|██████████| 1/1 [00:00<00:00, 144.03epoch/s, test_loss=6.35, train_loss=5.67]\n",
            "Epoch 673/1000: 100%|██████████| 1/1 [00:00<00:00, 146.71epoch/s, test_loss=6.26, train_loss=5.58]\n",
            "Epoch 674/1000: 100%|██████████| 1/1 [00:00<00:00, 96.33epoch/s, test_loss=6.2, train_loss=5.73]\n",
            "Epoch 675/1000: 100%|██████████| 1/1 [00:00<00:00, 130.88epoch/s, test_loss=6.34, train_loss=5.63]\n",
            "Epoch 676/1000: 100%|██████████| 1/1 [00:00<00:00, 187.62epoch/s, test_loss=6.36, train_loss=5.54]\n",
            "Epoch 677/1000: 100%|██████████| 1/1 [00:00<00:00, 201.69epoch/s, test_loss=6.23, train_loss=5.63]\n",
            "Epoch 678/1000: 100%|██████████| 1/1 [00:00<00:00, 247.76epoch/s, test_loss=6.37, train_loss=5.48]\n",
            "Epoch 679/1000: 100%|██████████| 1/1 [00:00<00:00, 201.34epoch/s, test_loss=6.37, train_loss=5.6]\n",
            "Epoch 680/1000: 100%|██████████| 1/1 [00:00<00:00, 267.63epoch/s, test_loss=6.44, train_loss=5.65]\n",
            "Epoch 681/1000: 100%|██████████| 1/1 [00:00<00:00, 87.12epoch/s, test_loss=6.44, train_loss=5.68]\n",
            "Epoch 682/1000: 100%|██████████| 1/1 [00:00<00:00, 122.89epoch/s, test_loss=6.45, train_loss=5.51]\n",
            "Epoch 683/1000: 100%|██████████| 1/1 [00:00<00:00, 257.13epoch/s, test_loss=6.32, train_loss=5.56]\n",
            "Epoch 684/1000: 100%|██████████| 1/1 [00:00<00:00, 241.55epoch/s, test_loss=6.46, train_loss=5.57]\n",
            "Epoch 685/1000: 100%|██████████| 1/1 [00:00<00:00, 245.87epoch/s, test_loss=6.35, train_loss=5.68]\n",
            "Epoch 686/1000: 100%|██████████| 1/1 [00:00<00:00, 255.16epoch/s, test_loss=6.45, train_loss=5.6]\n",
            "Epoch 687/1000: 100%|██████████| 1/1 [00:00<00:00, 233.63epoch/s, test_loss=6.19, train_loss=5.61]\n",
            "Epoch 688/1000: 100%|██████████| 1/1 [00:00<00:00, 243.27epoch/s, test_loss=6.41, train_loss=5.51]\n",
            "Epoch 689/1000: 100%|██████████| 1/1 [00:00<00:00, 141.34epoch/s, test_loss=6.42, train_loss=5.61]\n",
            "Epoch 690/1000: 100%|██████████| 1/1 [00:00<00:00, 213.52epoch/s, test_loss=6.27, train_loss=5.65]\n",
            "Epoch 691/1000: 100%|██████████| 1/1 [00:00<00:00, 99.88epoch/s, test_loss=6.37, train_loss=5.71]\n",
            "Epoch 692/1000: 100%|██████████| 1/1 [00:00<00:00, 136.88epoch/s, test_loss=6.44, train_loss=5.56]\n",
            "Epoch 693/1000: 100%|██████████| 1/1 [00:00<00:00, 137.08epoch/s, test_loss=6.33, train_loss=5.51]\n",
            "Epoch 694/1000: 100%|██████████| 1/1 [00:00<00:00, 194.24epoch/s, test_loss=6.23, train_loss=5.54]\n",
            "Epoch 695/1000: 100%|██████████| 1/1 [00:00<00:00, 236.51epoch/s, test_loss=6.36, train_loss=5.61]\n",
            "Epoch 696/1000: 100%|██████████| 1/1 [00:00<00:00, 128.34epoch/s, test_loss=6.47, train_loss=5.55]\n",
            "Epoch 697/1000: 100%|██████████| 1/1 [00:00<00:00, 240.42epoch/s, test_loss=6.33, train_loss=5.65]\n",
            "Epoch 698/1000: 100%|██████████| 1/1 [00:00<00:00, 218.49epoch/s, test_loss=6.16, train_loss=5.6]\n",
            "Epoch 699/1000: 100%|██████████| 1/1 [00:00<00:00, 80.91epoch/s, test_loss=6.59, train_loss=5.6]\n",
            "Epoch 700/1000: 100%|██████████| 1/1 [00:00<00:00, 55.25epoch/s, test_loss=6.25, train_loss=5.57]\n",
            "Epoch 701/1000: 100%|██████████| 1/1 [00:00<00:00, 99.92epoch/s, test_loss=6.39, train_loss=5.46]\n",
            "Epoch 702/1000: 100%|██████████| 1/1 [00:00<00:00, 250.69epoch/s, test_loss=6.58, train_loss=5.61]\n",
            "Epoch 703/1000: 100%|██████████| 1/1 [00:00<00:00, 219.74epoch/s, test_loss=6.61, train_loss=5.51]\n",
            "Epoch 704/1000: 100%|██████████| 1/1 [00:00<00:00, 66.07epoch/s, test_loss=6.22, train_loss=5.69]\n",
            "Epoch 705/1000: 100%|██████████| 1/1 [00:00<00:00, 96.97epoch/s, test_loss=6.42, train_loss=5.53]\n",
            "Epoch 706/1000: 100%|██████████| 1/1 [00:00<00:00, 245.28epoch/s, test_loss=6.32, train_loss=5.48]\n",
            "Epoch 707/1000: 100%|██████████| 1/1 [00:00<00:00, 146.97epoch/s, test_loss=6.32, train_loss=5.57]\n",
            "Epoch 708/1000: 100%|██████████| 1/1 [00:00<00:00, 274.89epoch/s, test_loss=6.28, train_loss=5.51]\n",
            "Epoch 709/1000: 100%|██████████| 1/1 [00:00<00:00, 133.60epoch/s, test_loss=6.35, train_loss=5.56]\n",
            "Epoch 710/1000: 100%|██████████| 1/1 [00:00<00:00, 208.56epoch/s, test_loss=6.41, train_loss=5.58]\n",
            "Epoch 711/1000: 100%|██████████| 1/1 [00:00<00:00, 226.80epoch/s, test_loss=6.34, train_loss=5.69]\n",
            "Epoch 712/1000: 100%|██████████| 1/1 [00:00<00:00, 160.27epoch/s, test_loss=6.49, train_loss=5.44]\n",
            "Epoch 713/1000: 100%|██████████| 1/1 [00:00<00:00, 199.93epoch/s, test_loss=6.37, train_loss=5.55]\n",
            "Epoch 714/1000: 100%|██████████| 1/1 [00:00<00:00, 204.75epoch/s, test_loss=6.47, train_loss=5.56]\n",
            "Epoch 715/1000: 100%|██████████| 1/1 [00:00<00:00, 126.35epoch/s, test_loss=6.26, train_loss=5.54]\n",
            "Epoch 716/1000: 100%|██████████| 1/1 [00:00<00:00, 130.53epoch/s, test_loss=6.33, train_loss=5.64]\n",
            "Epoch 717/1000: 100%|██████████| 1/1 [00:00<00:00, 461.88epoch/s, test_loss=6.46, train_loss=5.57]\n",
            "Epoch 718/1000: 100%|██████████| 1/1 [00:00<00:00, 88.95epoch/s, test_loss=6.38, train_loss=5.54]\n",
            "Epoch 719/1000: 100%|██████████| 1/1 [00:00<00:00, 178.43epoch/s, test_loss=6.52, train_loss=5.54]\n",
            "Epoch 720/1000: 100%|██████████| 1/1 [00:00<00:00, 185.69epoch/s, test_loss=6.57, train_loss=5.65]\n",
            "Epoch 721/1000: 100%|██████████| 1/1 [00:00<00:00, 84.35epoch/s, test_loss=6.55, train_loss=5.49]\n",
            "Epoch 722/1000: 100%|██████████| 1/1 [00:00<00:00, 360.71epoch/s, test_loss=6.34, train_loss=5.5]\n",
            "Epoch 723/1000: 100%|██████████| 1/1 [00:00<00:00, 118.46epoch/s, test_loss=6.42, train_loss=5.61]\n",
            "Epoch 724/1000: 100%|██████████| 1/1 [00:00<00:00, 375.26epoch/s, test_loss=6.43, train_loss=5.61]\n",
            "Epoch 725/1000: 100%|██████████| 1/1 [00:00<00:00, 207.88epoch/s, test_loss=6.32, train_loss=5.48]\n",
            "Epoch 726/1000: 100%|██████████| 1/1 [00:00<00:00, 247.66epoch/s, test_loss=6.38, train_loss=5.6]\n",
            "Epoch 727/1000: 100%|██████████| 1/1 [00:00<00:00, 75.76epoch/s, test_loss=6.49, train_loss=5.62]\n",
            "Epoch 728/1000: 100%|██████████| 1/1 [00:00<00:00, 96.08epoch/s, test_loss=6.38, train_loss=5.52]\n",
            "Epoch 729/1000: 100%|██████████| 1/1 [00:00<00:00, 204.74epoch/s, test_loss=6.27, train_loss=5.44]\n",
            "Epoch 730/1000: 100%|██████████| 1/1 [00:00<00:00, 177.85epoch/s, test_loss=6.52, train_loss=5.48]\n",
            "Epoch 731/1000: 100%|██████████| 1/1 [00:00<00:00, 222.00epoch/s, test_loss=6.43, train_loss=5.65]\n",
            "Epoch 732/1000: 100%|██████████| 1/1 [00:00<00:00, 250.48epoch/s, test_loss=6.38, train_loss=5.59]\n",
            "Epoch 733/1000: 100%|██████████| 1/1 [00:00<00:00, 109.65epoch/s, test_loss=6.27, train_loss=5.69]\n",
            "Epoch 734/1000: 100%|██████████| 1/1 [00:00<00:00, 130.99epoch/s, test_loss=6.51, train_loss=5.56]\n",
            "Epoch 735/1000: 100%|██████████| 1/1 [00:00<00:00, 213.35epoch/s, test_loss=6.37, train_loss=5.56]\n",
            "Epoch 736/1000: 100%|██████████| 1/1 [00:00<00:00, 234.33epoch/s, test_loss=6.25, train_loss=5.65]\n",
            "Epoch 737/1000: 100%|██████████| 1/1 [00:00<00:00, 125.72epoch/s, test_loss=6.51, train_loss=5.52]\n",
            "Epoch 738/1000: 100%|██████████| 1/1 [00:00<00:00, 136.36epoch/s, test_loss=6.24, train_loss=5.48]\n",
            "Epoch 739/1000: 100%|██████████| 1/1 [00:00<00:00, 130.29epoch/s, test_loss=6.42, train_loss=5.56]\n",
            "Epoch 740/1000: 100%|██████████| 1/1 [00:00<00:00, 194.68epoch/s, test_loss=6.36, train_loss=5.51]\n",
            "Epoch 741/1000: 100%|██████████| 1/1 [00:00<00:00, 210.79epoch/s, test_loss=6.39, train_loss=5.64]\n",
            "Epoch 742/1000: 100%|██████████| 1/1 [00:00<00:00, 186.55epoch/s, test_loss=6.27, train_loss=5.66]\n",
            "Epoch 743/1000: 100%|██████████| 1/1 [00:00<00:00, 235.87epoch/s, test_loss=6.32, train_loss=5.5]\n",
            "Epoch 744/1000: 100%|██████████| 1/1 [00:00<00:00, 127.06epoch/s, test_loss=6.28, train_loss=5.62]\n",
            "Epoch 745/1000: 100%|██████████| 1/1 [00:00<00:00, 195.85epoch/s, test_loss=6.34, train_loss=5.68]\n",
            "Epoch 746/1000: 100%|██████████| 1/1 [00:00<00:00, 207.11epoch/s, test_loss=6.29, train_loss=5.62]\n",
            "Epoch 747/1000: 100%|██████████| 1/1 [00:00<00:00, 175.82epoch/s, test_loss=6.46, train_loss=5.59]\n",
            "Epoch 748/1000: 100%|██████████| 1/1 [00:00<00:00, 214.89epoch/s, test_loss=6.63, train_loss=5.54]\n",
            "Epoch 749/1000: 100%|██████████| 1/1 [00:00<00:00, 188.26epoch/s, test_loss=6.11, train_loss=5.51]\n",
            "Epoch 750/1000: 100%|██████████| 1/1 [00:00<00:00, 162.04epoch/s, test_loss=6.38, train_loss=5.58]\n",
            "Epoch 751/1000: 100%|██████████| 1/1 [00:00<00:00, 141.69epoch/s, test_loss=6.29, train_loss=5.62]\n",
            "Epoch 752/1000: 100%|██████████| 1/1 [00:00<00:00, 167.18epoch/s, test_loss=6.61, train_loss=5.68]\n",
            "Epoch 753/1000: 100%|██████████| 1/1 [00:00<00:00, 150.79epoch/s, test_loss=6.49, train_loss=5.5]\n",
            "Epoch 754/1000: 100%|██████████| 1/1 [00:00<00:00, 156.37epoch/s, test_loss=6.45, train_loss=5.58]\n",
            "Epoch 755/1000: 100%|██████████| 1/1 [00:00<00:00, 122.98epoch/s, test_loss=6.39, train_loss=5.52]\n",
            "Epoch 756/1000: 100%|██████████| 1/1 [00:00<00:00, 285.11epoch/s, test_loss=6.43, train_loss=5.59]\n",
            "Epoch 757/1000: 100%|██████████| 1/1 [00:00<00:00, 247.25epoch/s, test_loss=6.41, train_loss=5.6]\n",
            "Epoch 758/1000: 100%|██████████| 1/1 [00:00<00:00, 221.57epoch/s, test_loss=6.43, train_loss=5.63]\n",
            "Epoch 759/1000: 100%|██████████| 1/1 [00:00<00:00, 219.20epoch/s, test_loss=6.5, train_loss=5.68]\n",
            "Epoch 760/1000: 100%|██████████| 1/1 [00:00<00:00, 263.49epoch/s, test_loss=6.54, train_loss=5.54]\n",
            "Epoch 761/1000: 100%|██████████| 1/1 [00:00<00:00, 234.63epoch/s, test_loss=6.51, train_loss=5.6]\n",
            "Epoch 762/1000: 100%|██████████| 1/1 [00:00<00:00, 229.31epoch/s, test_loss=6.54, train_loss=5.54]\n",
            "Epoch 763/1000: 100%|██████████| 1/1 [00:00<00:00, 248.98epoch/s, test_loss=6.57, train_loss=5.5]\n",
            "Epoch 764/1000: 100%|██████████| 1/1 [00:00<00:00, 254.09epoch/s, test_loss=6.44, train_loss=5.45]\n",
            "Epoch 765/1000: 100%|██████████| 1/1 [00:00<00:00, 253.43epoch/s, test_loss=6.51, train_loss=5.56]\n",
            "Epoch 766/1000: 100%|██████████| 1/1 [00:00<00:00, 235.61epoch/s, test_loss=6.31, train_loss=5.6]\n",
            "Epoch 767/1000: 100%|██████████| 1/1 [00:00<00:00, 261.64epoch/s, test_loss=6.4, train_loss=5.66]\n",
            "Epoch 768/1000: 100%|██████████| 1/1 [00:00<00:00, 262.00epoch/s, test_loss=6.5, train_loss=5.62]\n",
            "Epoch 769/1000: 100%|██████████| 1/1 [00:00<00:00, 243.85epoch/s, test_loss=6.44, train_loss=5.57]\n",
            "Epoch 770/1000: 100%|██████████| 1/1 [00:00<00:00, 247.51epoch/s, test_loss=6.29, train_loss=5.56]\n",
            "Epoch 771/1000: 100%|██████████| 1/1 [00:00<00:00, 242.53epoch/s, test_loss=6.34, train_loss=5.54]\n",
            "Epoch 772/1000: 100%|██████████| 1/1 [00:00<00:00, 257.08epoch/s, test_loss=6.34, train_loss=5.54]\n",
            "Epoch 773/1000: 100%|██████████| 1/1 [00:00<00:00, 216.42epoch/s, test_loss=6.58, train_loss=5.59]\n",
            "Epoch 774/1000: 100%|██████████| 1/1 [00:00<00:00, 245.91epoch/s, test_loss=6.25, train_loss=5.58]\n",
            "Epoch 775/1000: 100%|██████████| 1/1 [00:00<00:00, 174.92epoch/s, test_loss=6.33, train_loss=5.51]\n",
            "Epoch 776/1000: 100%|██████████| 1/1 [00:00<00:00, 233.21epoch/s, test_loss=6.4, train_loss=5.55]\n",
            "Epoch 777/1000: 100%|██████████| 1/1 [00:00<00:00, 252.02epoch/s, test_loss=6.31, train_loss=5.58]\n",
            "Epoch 778/1000: 100%|██████████| 1/1 [00:00<00:00, 102.32epoch/s, test_loss=6.59, train_loss=5.65]\n",
            "Epoch 779/1000: 100%|██████████| 1/1 [00:00<00:00, 251.94epoch/s, test_loss=6.33, train_loss=5.63]\n",
            "Epoch 780/1000: 100%|██████████| 1/1 [00:00<00:00, 238.46epoch/s, test_loss=6.26, train_loss=5.65]\n",
            "Epoch 781/1000: 100%|██████████| 1/1 [00:00<00:00, 202.02epoch/s, test_loss=6.24, train_loss=5.54]\n",
            "Epoch 782/1000: 100%|██████████| 1/1 [00:00<00:00, 362.42epoch/s, test_loss=6.41, train_loss=5.55]\n",
            "Epoch 783/1000: 100%|██████████| 1/1 [00:00<00:00, 286.54epoch/s, test_loss=6.52, train_loss=5.49]\n",
            "Epoch 784/1000: 100%|██████████| 1/1 [00:00<00:00, 149.33epoch/s, test_loss=6.39, train_loss=5.67]\n",
            "Epoch 785/1000: 100%|██████████| 1/1 [00:00<00:00, 124.42epoch/s, test_loss=6.36, train_loss=5.62]\n",
            "Epoch 786/1000: 100%|██████████| 1/1 [00:00<00:00, 53.03epoch/s, test_loss=6.22, train_loss=5.6]\n",
            "Epoch 787/1000: 100%|██████████| 1/1 [00:00<00:00, 343.60epoch/s, test_loss=6.44, train_loss=5.43]\n",
            "Epoch 788/1000: 100%|██████████| 1/1 [00:00<00:00, 197.54epoch/s, test_loss=6.39, train_loss=5.58]\n",
            "Epoch 789/1000: 100%|██████████| 1/1 [00:00<00:00, 135.81epoch/s, test_loss=6.39, train_loss=5.54]\n",
            "Epoch 790/1000: 100%|██████████| 1/1 [00:00<00:00, 232.76epoch/s, test_loss=6.38, train_loss=5.51]\n",
            "Epoch 791/1000: 100%|██████████| 1/1 [00:00<00:00, 202.81epoch/s, test_loss=6.42, train_loss=5.64]\n",
            "Epoch 792/1000: 100%|██████████| 1/1 [00:00<00:00, 99.33epoch/s, test_loss=6.57, train_loss=5.5]\n",
            "Epoch 793/1000: 100%|██████████| 1/1 [00:00<00:00, 331.12epoch/s, test_loss=6.37, train_loss=5.48]\n",
            "Epoch 794/1000: 100%|██████████| 1/1 [00:00<00:00, 306.85epoch/s, test_loss=6.56, train_loss=5.42]\n",
            "Epoch 795/1000: 100%|██████████| 1/1 [00:00<00:00, 202.03epoch/s, test_loss=6.63, train_loss=5.47]\n",
            "Epoch 796/1000: 100%|██████████| 1/1 [00:00<00:00, 210.11epoch/s, test_loss=6.58, train_loss=5.48]\n",
            "Epoch 797/1000: 100%|██████████| 1/1 [00:00<00:00, 262.87epoch/s, test_loss=6.46, train_loss=5.63]\n",
            "Epoch 798/1000: 100%|██████████| 1/1 [00:00<00:00, 187.99epoch/s, test_loss=6.32, train_loss=5.53]\n",
            "Epoch 799/1000: 100%|██████████| 1/1 [00:00<00:00, 243.74epoch/s, test_loss=6.31, train_loss=5.54]\n",
            "Epoch 800/1000: 100%|██████████| 1/1 [00:00<00:00, 214.31epoch/s, test_loss=6.5, train_loss=5.55]\n",
            "Epoch 801/1000: 100%|██████████| 1/1 [00:00<00:00, 208.26epoch/s, test_loss=6.48, train_loss=5.52]\n",
            "Epoch 802/1000: 100%|██████████| 1/1 [00:00<00:00, 213.57epoch/s, test_loss=6.43, train_loss=5.52]\n",
            "Epoch 803/1000: 100%|██████████| 1/1 [00:00<00:00, 174.86epoch/s, test_loss=6.39, train_loss=5.39]\n",
            "Epoch 804/1000: 100%|██████████| 1/1 [00:00<00:00, 235.26epoch/s, test_loss=6.27, train_loss=5.55]\n",
            "Epoch 805/1000: 100%|██████████| 1/1 [00:00<00:00, 181.30epoch/s, test_loss=6.25, train_loss=5.43]\n",
            "Epoch 806/1000: 100%|██████████| 1/1 [00:00<00:00, 186.78epoch/s, test_loss=6.51, train_loss=5.52]\n",
            "Epoch 807/1000: 100%|██████████| 1/1 [00:00<00:00, 286.16epoch/s, test_loss=6.2, train_loss=5.55]\n",
            "Epoch 808/1000: 100%|██████████| 1/1 [00:00<00:00, 170.27epoch/s, test_loss=6.14, train_loss=5.48]\n",
            "Epoch 809/1000: 100%|██████████| 1/1 [00:00<00:00, 227.14epoch/s, test_loss=6.34, train_loss=5.6]\n",
            "Epoch 810/1000: 100%|██████████| 1/1 [00:00<00:00, 62.62epoch/s, test_loss=6.25, train_loss=5.51]\n",
            "Epoch 811/1000: 100%|██████████| 1/1 [00:00<00:00, 233.42epoch/s, test_loss=6.51, train_loss=5.49]\n",
            "Epoch 812/1000: 100%|██████████| 1/1 [00:00<00:00, 122.49epoch/s, test_loss=6.27, train_loss=5.54]\n",
            "Epoch 813/1000: 100%|██████████| 1/1 [00:00<00:00, 189.95epoch/s, test_loss=6.33, train_loss=5.52]\n",
            "Epoch 814/1000: 100%|██████████| 1/1 [00:00<00:00, 356.66epoch/s, test_loss=6.44, train_loss=5.55]\n",
            "Epoch 815/1000: 100%|██████████| 1/1 [00:00<00:00, 74.30epoch/s, test_loss=6.28, train_loss=5.7]\n",
            "Epoch 816/1000: 100%|██████████| 1/1 [00:00<00:00, 311.29epoch/s, test_loss=6.45, train_loss=5.51]\n",
            "Epoch 817/1000: 100%|██████████| 1/1 [00:00<00:00, 257.00epoch/s, test_loss=6.24, train_loss=5.67]\n",
            "Epoch 818/1000: 100%|██████████| 1/1 [00:00<00:00, 341.44epoch/s, test_loss=6.42, train_loss=5.59]\n",
            "Epoch 819/1000: 100%|██████████| 1/1 [00:00<00:00, 187.10epoch/s, test_loss=6.41, train_loss=5.64]\n",
            "Epoch 820/1000: 100%|██████████| 1/1 [00:00<00:00, 251.55epoch/s, test_loss=6.29, train_loss=5.55]\n",
            "Epoch 821/1000: 100%|██████████| 1/1 [00:00<00:00, 221.09epoch/s, test_loss=6.53, train_loss=5.44]\n",
            "Epoch 822/1000: 100%|██████████| 1/1 [00:00<00:00, 264.19epoch/s, test_loss=6.45, train_loss=5.51]\n",
            "Epoch 823/1000: 100%|██████████| 1/1 [00:00<00:00, 244.57epoch/s, test_loss=5.96, train_loss=5.54]\n",
            "Epoch 824/1000: 100%|██████████| 1/1 [00:00<00:00, 238.03epoch/s, test_loss=6.42, train_loss=5.64]\n",
            "Epoch 825/1000: 100%|██████████| 1/1 [00:00<00:00, 203.99epoch/s, test_loss=6.32, train_loss=5.56]\n",
            "Epoch 826/1000: 100%|██████████| 1/1 [00:00<00:00, 240.14epoch/s, test_loss=6.28, train_loss=5.55]\n",
            "Epoch 827/1000: 100%|██████████| 1/1 [00:00<00:00, 128.38epoch/s, test_loss=6.35, train_loss=5.5]\n",
            "Epoch 828/1000: 100%|██████████| 1/1 [00:00<00:00, 184.32epoch/s, test_loss=6.46, train_loss=5.51]\n",
            "Epoch 829/1000: 100%|██████████| 1/1 [00:00<00:00, 178.12epoch/s, test_loss=6.42, train_loss=5.57]\n",
            "Epoch 830/1000: 100%|██████████| 1/1 [00:00<00:00, 347.96epoch/s, test_loss=6.28, train_loss=5.55]\n",
            "Epoch 831/1000: 100%|██████████| 1/1 [00:00<00:00, 227.06epoch/s, test_loss=6.44, train_loss=5.57]\n",
            "Epoch 832/1000: 100%|██████████| 1/1 [00:00<00:00, 221.35epoch/s, test_loss=6.43, train_loss=5.68]\n",
            "Epoch 833/1000: 100%|██████████| 1/1 [00:00<00:00, 301.71epoch/s, test_loss=6.15, train_loss=5.54]\n",
            "Epoch 834/1000: 100%|██████████| 1/1 [00:00<00:00, 192.36epoch/s, test_loss=6.45, train_loss=5.49]\n",
            "Epoch 835/1000: 100%|██████████| 1/1 [00:00<00:00, 176.91epoch/s, test_loss=6.39, train_loss=5.68]\n",
            "Epoch 836/1000: 100%|██████████| 1/1 [00:00<00:00, 232.68epoch/s, test_loss=6.43, train_loss=5.57]\n",
            "Epoch 837/1000: 100%|██████████| 1/1 [00:00<00:00, 260.77epoch/s, test_loss=6.36, train_loss=5.61]\n",
            "Epoch 838/1000: 100%|██████████| 1/1 [00:00<00:00, 239.35epoch/s, test_loss=6.43, train_loss=5.4]\n",
            "Epoch 839/1000: 100%|██████████| 1/1 [00:00<00:00, 247.89epoch/s, test_loss=6.25, train_loss=5.67]\n",
            "Epoch 840/1000: 100%|██████████| 1/1 [00:00<00:00, 106.62epoch/s, test_loss=6.39, train_loss=5.4]\n",
            "Epoch 841/1000: 100%|██████████| 1/1 [00:00<00:00, 379.51epoch/s, test_loss=6.33, train_loss=5.44]\n",
            "Epoch 842/1000: 100%|██████████| 1/1 [00:00<00:00, 199.42epoch/s, test_loss=6.29, train_loss=5.51]\n",
            "Epoch 843/1000: 100%|██████████| 1/1 [00:00<00:00, 139.20epoch/s, test_loss=6.44, train_loss=5.49]\n",
            "Epoch 844/1000: 100%|██████████| 1/1 [00:00<00:00, 136.85epoch/s, test_loss=6.36, train_loss=5.48]\n",
            "Epoch 845/1000: 100%|██████████| 1/1 [00:00<00:00, 188.47epoch/s, test_loss=6.55, train_loss=5.54]\n",
            "Epoch 846/1000: 100%|██████████| 1/1 [00:00<00:00, 233.54epoch/s, test_loss=6.44, train_loss=5.48]\n",
            "Epoch 847/1000: 100%|██████████| 1/1 [00:00<00:00, 235.77epoch/s, test_loss=6.33, train_loss=5.57]\n",
            "Epoch 848/1000: 100%|██████████| 1/1 [00:00<00:00, 263.99epoch/s, test_loss=6.44, train_loss=5.57]\n",
            "Epoch 849/1000: 100%|██████████| 1/1 [00:00<00:00, 206.64epoch/s, test_loss=6.36, train_loss=5.51]\n",
            "Epoch 850/1000: 100%|██████████| 1/1 [00:00<00:00, 199.30epoch/s, test_loss=6.34, train_loss=5.42]\n",
            "Epoch 851/1000: 100%|██████████| 1/1 [00:00<00:00, 94.26epoch/s, test_loss=6.51, train_loss=5.56]\n",
            "Epoch 852/1000: 100%|██████████| 1/1 [00:00<00:00, 291.90epoch/s, test_loss=6.28, train_loss=5.52]\n",
            "Epoch 853/1000: 100%|██████████| 1/1 [00:00<00:00, 208.49epoch/s, test_loss=6.28, train_loss=5.5]\n",
            "Epoch 854/1000: 100%|██████████| 1/1 [00:00<00:00, 92.32epoch/s, test_loss=6.33, train_loss=5.48]\n",
            "Epoch 855/1000: 100%|██████████| 1/1 [00:00<00:00, 147.13epoch/s, test_loss=6.6, train_loss=5.34]\n",
            "Epoch 856/1000: 100%|██████████| 1/1 [00:00<00:00, 175.77epoch/s, test_loss=6.2, train_loss=5.56]\n",
            "Epoch 857/1000: 100%|██████████| 1/1 [00:00<00:00, 79.40epoch/s, test_loss=6.59, train_loss=5.52]\n",
            "Epoch 858/1000: 100%|██████████| 1/1 [00:00<00:00, 192.81epoch/s, test_loss=6.4, train_loss=5.59]\n",
            "Epoch 859/1000: 100%|██████████| 1/1 [00:00<00:00, 245.31epoch/s, test_loss=6.42, train_loss=5.6]\n",
            "Epoch 860/1000: 100%|██████████| 1/1 [00:00<00:00, 201.52epoch/s, test_loss=6.58, train_loss=5.42]\n",
            "Epoch 861/1000: 100%|██████████| 1/1 [00:00<00:00, 133.59epoch/s, test_loss=6.26, train_loss=5.43]\n",
            "Epoch 862/1000: 100%|██████████| 1/1 [00:00<00:00, 164.51epoch/s, test_loss=6.26, train_loss=5.62]\n",
            "Epoch 863/1000: 100%|██████████| 1/1 [00:00<00:00, 183.85epoch/s, test_loss=6.41, train_loss=5.63]\n",
            "Epoch 864/1000: 100%|██████████| 1/1 [00:00<00:00, 148.16epoch/s, test_loss=6.62, train_loss=5.4]\n",
            "Epoch 865/1000: 100%|██████████| 1/1 [00:00<00:00, 124.10epoch/s, test_loss=6.3, train_loss=5.47]\n",
            "Epoch 866/1000: 100%|██████████| 1/1 [00:00<00:00, 185.17epoch/s, test_loss=6.44, train_loss=5.46]\n",
            "Epoch 867/1000: 100%|██████████| 1/1 [00:00<00:00, 207.61epoch/s, test_loss=6.46, train_loss=5.56]\n",
            "Epoch 868/1000: 100%|██████████| 1/1 [00:00<00:00, 260.90epoch/s, test_loss=6.52, train_loss=5.43]\n",
            "Epoch 869/1000: 100%|██████████| 1/1 [00:00<00:00, 65.55epoch/s, test_loss=6.42, train_loss=5.54]\n",
            "Epoch 870/1000: 100%|██████████| 1/1 [00:00<00:00, 252.81epoch/s, test_loss=6.42, train_loss=5.4]\n",
            "Epoch 871/1000: 100%|██████████| 1/1 [00:00<00:00, 221.02epoch/s, test_loss=6.35, train_loss=5.52]\n",
            "Epoch 872/1000: 100%|██████████| 1/1 [00:00<00:00, 255.45epoch/s, test_loss=6.54, train_loss=5.4]\n",
            "Epoch 873/1000: 100%|██████████| 1/1 [00:00<00:00, 194.35epoch/s, test_loss=6.43, train_loss=5.67]\n",
            "Epoch 874/1000: 100%|██████████| 1/1 [00:00<00:00, 305.86epoch/s, test_loss=6.37, train_loss=5.45]\n",
            "Epoch 875/1000: 100%|██████████| 1/1 [00:00<00:00, 144.22epoch/s, test_loss=6.25, train_loss=5.52]\n",
            "Epoch 876/1000: 100%|██████████| 1/1 [00:00<00:00, 96.65epoch/s, test_loss=6.41, train_loss=5.5]\n",
            "Epoch 877/1000: 100%|██████████| 1/1 [00:00<00:00, 182.11epoch/s, test_loss=6.5, train_loss=5.5]\n",
            "Epoch 878/1000: 100%|██████████| 1/1 [00:00<00:00, 331.07epoch/s, test_loss=6.46, train_loss=5.43]\n",
            "Epoch 879/1000: 100%|██████████| 1/1 [00:00<00:00, 82.92epoch/s, test_loss=6.49, train_loss=5.62]\n",
            "Epoch 880/1000: 100%|██████████| 1/1 [00:00<00:00, 245.76epoch/s, test_loss=6.3, train_loss=5.56]\n",
            "Epoch 881/1000: 100%|██████████| 1/1 [00:00<00:00, 224.73epoch/s, test_loss=6.55, train_loss=5.5]\n",
            "Epoch 882/1000: 100%|██████████| 1/1 [00:00<00:00, 193.00epoch/s, test_loss=6.35, train_loss=5.51]\n",
            "Epoch 883/1000: 100%|██████████| 1/1 [00:00<00:00, 207.05epoch/s, test_loss=6.46, train_loss=5.46]\n",
            "Epoch 884/1000: 100%|██████████| 1/1 [00:00<00:00, 355.84epoch/s, test_loss=6.45, train_loss=5.49]\n",
            "Epoch 885/1000: 100%|██████████| 1/1 [00:00<00:00, 124.26epoch/s, test_loss=6.12, train_loss=5.52]\n",
            "Epoch 886/1000: 100%|██████████| 1/1 [00:00<00:00, 314.04epoch/s, test_loss=6.4, train_loss=5.49]\n",
            "Epoch 887/1000: 100%|██████████| 1/1 [00:00<00:00, 245.96epoch/s, test_loss=6.43, train_loss=5.51]\n",
            "Epoch 888/1000: 100%|██████████| 1/1 [00:00<00:00, 111.33epoch/s, test_loss=6.45, train_loss=5.5]\n",
            "Epoch 889/1000: 100%|██████████| 1/1 [00:00<00:00, 226.29epoch/s, test_loss=6.49, train_loss=5.46]\n",
            "Epoch 890/1000: 100%|██████████| 1/1 [00:00<00:00, 233.11epoch/s, test_loss=6.43, train_loss=5.52]\n",
            "Epoch 891/1000: 100%|██████████| 1/1 [00:00<00:00, 214.48epoch/s, test_loss=6.29, train_loss=5.38]\n",
            "Epoch 892/1000: 100%|██████████| 1/1 [00:00<00:00, 366.00epoch/s, test_loss=6.44, train_loss=5.52]\n",
            "Epoch 893/1000: 100%|██████████| 1/1 [00:00<00:00, 246.94epoch/s, test_loss=6.47, train_loss=5.6]\n",
            "Epoch 894/1000: 100%|██████████| 1/1 [00:00<00:00, 243.66epoch/s, test_loss=6.45, train_loss=5.59]\n",
            "Epoch 895/1000: 100%|██████████| 1/1 [00:00<00:00, 252.56epoch/s, test_loss=6.41, train_loss=5.46]\n",
            "Epoch 896/1000: 100%|██████████| 1/1 [00:00<00:00, 192.28epoch/s, test_loss=6.59, train_loss=5.44]\n",
            "Epoch 897/1000: 100%|██████████| 1/1 [00:00<00:00, 246.84epoch/s, test_loss=6.19, train_loss=5.63]\n",
            "Epoch 898/1000: 100%|██████████| 1/1 [00:00<00:00, 247.22epoch/s, test_loss=6.29, train_loss=5.54]\n",
            "Epoch 899/1000: 100%|██████████| 1/1 [00:00<00:00, 123.28epoch/s, test_loss=6.3, train_loss=5.51]\n",
            "Epoch 900/1000: 100%|██████████| 1/1 [00:00<00:00, 251.31epoch/s, test_loss=6.25, train_loss=5.35]\n",
            "Epoch 901/1000: 100%|██████████| 1/1 [00:00<00:00, 254.20epoch/s, test_loss=6.58, train_loss=5.62]\n",
            "Epoch 902/1000: 100%|██████████| 1/1 [00:00<00:00, 221.27epoch/s, test_loss=6.35, train_loss=5.67]\n",
            "Epoch 903/1000: 100%|██████████| 1/1 [00:00<00:00, 247.66epoch/s, test_loss=6.39, train_loss=5.47]\n",
            "Epoch 904/1000: 100%|██████████| 1/1 [00:00<00:00, 257.87epoch/s, test_loss=6.51, train_loss=5.55]\n",
            "Epoch 905/1000: 100%|██████████| 1/1 [00:00<00:00, 260.89epoch/s, test_loss=6.35, train_loss=5.58]\n",
            "Epoch 906/1000: 100%|██████████| 1/1 [00:00<00:00, 210.02epoch/s, test_loss=6.41, train_loss=5.4]\n",
            "Epoch 907/1000: 100%|██████████| 1/1 [00:00<00:00, 248.85epoch/s, test_loss=6.48, train_loss=5.42]\n",
            "Epoch 908/1000: 100%|██████████| 1/1 [00:00<00:00, 234.36epoch/s, test_loss=6.63, train_loss=5.45]\n",
            "Epoch 909/1000: 100%|██████████| 1/1 [00:00<00:00, 228.75epoch/s, test_loss=6.29, train_loss=5.55]\n",
            "Epoch 910/1000: 100%|██████████| 1/1 [00:00<00:00, 207.30epoch/s, test_loss=6.34, train_loss=5.53]\n",
            "Epoch 911/1000: 100%|██████████| 1/1 [00:00<00:00, 247.96epoch/s, test_loss=6.24, train_loss=5.58]\n",
            "Epoch 912/1000: 100%|██████████| 1/1 [00:00<00:00, 241.61epoch/s, test_loss=6.29, train_loss=5.62]\n",
            "Epoch 913/1000: 100%|██████████| 1/1 [00:00<00:00, 232.27epoch/s, test_loss=6.5, train_loss=5.41]\n",
            "Epoch 914/1000: 100%|██████████| 1/1 [00:00<00:00, 241.07epoch/s, test_loss=6.15, train_loss=5.61]\n",
            "Epoch 915/1000: 100%|██████████| 1/1 [00:00<00:00, 255.05epoch/s, test_loss=6.45, train_loss=5.48]\n",
            "Epoch 916/1000: 100%|██████████| 1/1 [00:00<00:00, 244.10epoch/s, test_loss=6.44, train_loss=5.5]\n",
            "Epoch 917/1000: 100%|██████████| 1/1 [00:00<00:00, 261.25epoch/s, test_loss=6.33, train_loss=5.58]\n",
            "Epoch 918/1000: 100%|██████████| 1/1 [00:00<00:00, 242.46epoch/s, test_loss=6.32, train_loss=5.55]\n",
            "Epoch 919/1000: 100%|██████████| 1/1 [00:00<00:00, 211.03epoch/s, test_loss=6.17, train_loss=5.38]\n",
            "Epoch 920/1000: 100%|██████████| 1/1 [00:00<00:00, 257.49epoch/s, test_loss=6.33, train_loss=5.54]\n",
            "Epoch 921/1000: 100%|██████████| 1/1 [00:00<00:00, 182.71epoch/s, test_loss=6.33, train_loss=5.44]\n",
            "Epoch 922/1000: 100%|██████████| 1/1 [00:00<00:00, 253.10epoch/s, test_loss=6.53, train_loss=5.41]\n",
            "Epoch 923/1000: 100%|██████████| 1/1 [00:00<00:00, 83.76epoch/s, test_loss=6.54, train_loss=5.54]\n",
            "Epoch 924/1000: 100%|██████████| 1/1 [00:00<00:00, 227.60epoch/s, test_loss=6.32, train_loss=5.49]\n",
            "Epoch 925/1000: 100%|██████████| 1/1 [00:00<00:00, 217.95epoch/s, test_loss=6.44, train_loss=5.55]\n",
            "Epoch 926/1000: 100%|██████████| 1/1 [00:00<00:00, 185.70epoch/s, test_loss=6.34, train_loss=5.66]\n",
            "Epoch 927/1000: 100%|██████████| 1/1 [00:00<00:00, 145.23epoch/s, test_loss=6.35, train_loss=5.44]\n",
            "Epoch 928/1000: 100%|██████████| 1/1 [00:00<00:00, 223.80epoch/s, test_loss=6.28, train_loss=5.43]\n",
            "Epoch 929/1000: 100%|██████████| 1/1 [00:00<00:00, 134.40epoch/s, test_loss=6.56, train_loss=5.4]\n",
            "Epoch 930/1000: 100%|██████████| 1/1 [00:00<00:00, 224.26epoch/s, test_loss=6.55, train_loss=5.45]\n",
            "Epoch 931/1000: 100%|██████████| 1/1 [00:00<00:00, 171.33epoch/s, test_loss=6.22, train_loss=5.59]\n",
            "Epoch 932/1000: 100%|██████████| 1/1 [00:00<00:00, 216.01epoch/s, test_loss=6.4, train_loss=5.49]\n",
            "Epoch 933/1000: 100%|██████████| 1/1 [00:00<00:00, 212.00epoch/s, test_loss=6.53, train_loss=5.51]\n",
            "Epoch 934/1000: 100%|██████████| 1/1 [00:00<00:00, 153.10epoch/s, test_loss=6.41, train_loss=5.61]\n",
            "Epoch 935/1000: 100%|██████████| 1/1 [00:00<00:00, 213.68epoch/s, test_loss=6.28, train_loss=5.52]\n",
            "Epoch 936/1000: 100%|██████████| 1/1 [00:00<00:00, 223.97epoch/s, test_loss=6.31, train_loss=5.53]\n",
            "Epoch 937/1000: 100%|██████████| 1/1 [00:00<00:00, 117.47epoch/s, test_loss=6.51, train_loss=5.52]\n",
            "Epoch 938/1000: 100%|██████████| 1/1 [00:00<00:00, 110.23epoch/s, test_loss=6.51, train_loss=5.62]\n",
            "Epoch 939/1000: 100%|██████████| 1/1 [00:00<00:00, 65.32epoch/s, test_loss=6.58, train_loss=5.55]\n",
            "Epoch 940/1000: 100%|██████████| 1/1 [00:00<00:00, 247.85epoch/s, test_loss=5.93, train_loss=5.49]\n",
            "Epoch 941/1000: 100%|██████████| 1/1 [00:00<00:00, 190.56epoch/s, test_loss=6.35, train_loss=5.47]\n",
            "Epoch 942/1000: 100%|██████████| 1/1 [00:00<00:00, 388.90epoch/s, test_loss=6.39, train_loss=5.56]\n",
            "Epoch 943/1000: 100%|██████████| 1/1 [00:00<00:00, 107.93epoch/s, test_loss=6.26, train_loss=5.63]\n",
            "Epoch 944/1000: 100%|██████████| 1/1 [00:00<00:00, 299.17epoch/s, test_loss=6.66, train_loss=5.5]\n",
            "Epoch 945/1000: 100%|██████████| 1/1 [00:00<00:00, 194.01epoch/s, test_loss=6.22, train_loss=5.55]\n",
            "Epoch 946/1000: 100%|██████████| 1/1 [00:00<00:00, 248.23epoch/s, test_loss=6.43, train_loss=5.53]\n",
            "Epoch 947/1000: 100%|██████████| 1/1 [00:00<00:00, 215.63epoch/s, test_loss=6.21, train_loss=5.43]\n",
            "Epoch 948/1000: 100%|██████████| 1/1 [00:00<00:00, 330.44epoch/s, test_loss=6.53, train_loss=5.5]\n",
            "Epoch 949/1000: 100%|██████████| 1/1 [00:00<00:00, 207.62epoch/s, test_loss=6.44, train_loss=5.49]\n",
            "Epoch 950/1000: 100%|██████████| 1/1 [00:00<00:00, 391.99epoch/s, test_loss=6.5, train_loss=5.53]\n",
            "Epoch 951/1000: 100%|██████████| 1/1 [00:00<00:00, 328.01epoch/s, test_loss=6.27, train_loss=5.56]\n",
            "Epoch 952/1000: 100%|██████████| 1/1 [00:00<00:00, 122.12epoch/s, test_loss=6.49, train_loss=5.49]\n",
            "Epoch 953/1000: 100%|██████████| 1/1 [00:00<00:00, 123.85epoch/s, test_loss=6.29, train_loss=5.54]\n",
            "Epoch 954/1000: 100%|██████████| 1/1 [00:00<00:00, 225.78epoch/s, test_loss=6.27, train_loss=5.57]\n",
            "Epoch 955/1000: 100%|██████████| 1/1 [00:00<00:00, 243.80epoch/s, test_loss=6.47, train_loss=5.57]\n",
            "Epoch 956/1000: 100%|██████████| 1/1 [00:00<00:00, 139.61epoch/s, test_loss=6.36, train_loss=5.49]\n",
            "Epoch 957/1000: 100%|██████████| 1/1 [00:00<00:00, 286.20epoch/s, test_loss=6.16, train_loss=5.47]\n",
            "Epoch 958/1000: 100%|██████████| 1/1 [00:00<00:00, 382.62epoch/s, test_loss=6.5, train_loss=5.54]\n",
            "Epoch 959/1000: 100%|██████████| 1/1 [00:00<00:00, 63.44epoch/s, test_loss=6.34, train_loss=5.51]\n",
            "Epoch 960/1000: 100%|██████████| 1/1 [00:00<00:00, 210.41epoch/s, test_loss=6.3, train_loss=5.54]\n",
            "Epoch 961/1000: 100%|██████████| 1/1 [00:00<00:00, 208.51epoch/s, test_loss=6.31, train_loss=5.54]\n",
            "Epoch 962/1000: 100%|██████████| 1/1 [00:00<00:00, 125.06epoch/s, test_loss=6.43, train_loss=5.44]\n",
            "Epoch 963/1000: 100%|██████████| 1/1 [00:00<00:00, 419.81epoch/s, test_loss=6.35, train_loss=5.42]\n",
            "Epoch 964/1000: 100%|██████████| 1/1 [00:00<00:00, 391.22epoch/s, test_loss=6.55, train_loss=5.38]\n",
            "Epoch 965/1000: 100%|██████████| 1/1 [00:00<00:00, 254.23epoch/s, test_loss=6.32, train_loss=5.53]\n",
            "Epoch 966/1000: 100%|██████████| 1/1 [00:00<00:00, 207.81epoch/s, test_loss=6.36, train_loss=5.49]\n",
            "Epoch 967/1000: 100%|██████████| 1/1 [00:00<00:00, 224.57epoch/s, test_loss=6.57, train_loss=5.54]\n",
            "Epoch 968/1000: 100%|██████████| 1/1 [00:00<00:00, 244.59epoch/s, test_loss=6.64, train_loss=5.48]\n",
            "Epoch 969/1000: 100%|██████████| 1/1 [00:00<00:00, 238.73epoch/s, test_loss=6.34, train_loss=5.46]\n",
            "Epoch 970/1000: 100%|██████████| 1/1 [00:00<00:00, 287.68epoch/s, test_loss=6.33, train_loss=5.57]\n",
            "Epoch 971/1000: 100%|██████████| 1/1 [00:00<00:00, 344.93epoch/s, test_loss=6.44, train_loss=5.44]\n",
            "Epoch 972/1000: 100%|██████████| 1/1 [00:00<00:00, 432.67epoch/s, test_loss=6.49, train_loss=5.44]\n",
            "Epoch 973/1000: 100%|██████████| 1/1 [00:00<00:00, 218.92epoch/s, test_loss=6.44, train_loss=5.48]\n",
            "Epoch 974/1000: 100%|██████████| 1/1 [00:00<00:00, 71.76epoch/s, test_loss=6.29, train_loss=5.46]\n",
            "Epoch 975/1000: 100%|██████████| 1/1 [00:00<00:00, 115.93epoch/s, test_loss=6.27, train_loss=5.46]\n",
            "Epoch 976/1000: 100%|██████████| 1/1 [00:00<00:00, 145.14epoch/s, test_loss=6.41, train_loss=5.5]\n",
            "Epoch 977/1000: 100%|██████████| 1/1 [00:00<00:00, 241.14epoch/s, test_loss=6.55, train_loss=5.59]\n",
            "Epoch 978/1000: 100%|██████████| 1/1 [00:00<00:00, 284.63epoch/s, test_loss=6.41, train_loss=5.44]\n",
            "Epoch 979/1000: 100%|██████████| 1/1 [00:00<00:00, 221.11epoch/s, test_loss=6.29, train_loss=5.42]\n",
            "Epoch 980/1000: 100%|██████████| 1/1 [00:00<00:00, 336.14epoch/s, test_loss=6.35, train_loss=5.41]\n",
            "Epoch 981/1000: 100%|██████████| 1/1 [00:00<00:00, 237.54epoch/s, test_loss=6.19, train_loss=5.55]\n",
            "Epoch 982/1000: 100%|██████████| 1/1 [00:00<00:00, 254.17epoch/s, test_loss=6.41, train_loss=5.56]\n",
            "Epoch 983/1000: 100%|██████████| 1/1 [00:00<00:00, 215.06epoch/s, test_loss=6.65, train_loss=5.42]\n",
            "Epoch 984/1000: 100%|██████████| 1/1 [00:00<00:00, 222.11epoch/s, test_loss=6.41, train_loss=5.49]\n",
            "Epoch 985/1000: 100%|██████████| 1/1 [00:00<00:00, 226.03epoch/s, test_loss=6.09, train_loss=5.53]\n",
            "Epoch 986/1000: 100%|██████████| 1/1 [00:00<00:00, 182.15epoch/s, test_loss=6.34, train_loss=5.58]\n",
            "Epoch 987/1000: 100%|██████████| 1/1 [00:00<00:00, 352.61epoch/s, test_loss=6.53, train_loss=5.4]\n",
            "Epoch 988/1000: 100%|██████████| 1/1 [00:00<00:00, 200.18epoch/s, test_loss=6.26, train_loss=5.46]\n",
            "Epoch 989/1000: 100%|██████████| 1/1 [00:00<00:00, 195.96epoch/s, test_loss=6.66, train_loss=5.46]\n",
            "Epoch 990/1000: 100%|██████████| 1/1 [00:00<00:00, 162.65epoch/s, test_loss=6.26, train_loss=5.59]\n",
            "Epoch 991/1000: 100%|██████████| 1/1 [00:00<00:00, 218.26epoch/s, test_loss=6.37, train_loss=5.46]\n",
            "Epoch 992/1000: 100%|██████████| 1/1 [00:00<00:00, 238.57epoch/s, test_loss=6.17, train_loss=5.52]\n",
            "Epoch 993/1000: 100%|██████████| 1/1 [00:00<00:00, 330.03epoch/s, test_loss=6.44, train_loss=5.49]\n",
            "Epoch 994/1000: 100%|██████████| 1/1 [00:00<00:00, 142.83epoch/s, test_loss=6.39, train_loss=5.62]\n",
            "Epoch 995/1000: 100%|██████████| 1/1 [00:00<00:00, 188.25epoch/s, test_loss=6.35, train_loss=5.49]\n",
            "Epoch 996/1000: 100%|██████████| 1/1 [00:00<00:00, 147.33epoch/s, test_loss=6.1, train_loss=5.53]\n",
            "Epoch 997/1000: 100%|██████████| 1/1 [00:00<00:00, 184.61epoch/s, test_loss=6.24, train_loss=5.34]\n",
            "Epoch 998/1000: 100%|██████████| 1/1 [00:00<00:00, 153.23epoch/s, test_loss=6.42, train_loss=5.52]\n",
            "Epoch 999/1000: 100%|██████████| 1/1 [00:00<00:00, 210.38epoch/s, test_loss=6.23, train_loss=5.57]\n",
            "Epoch 1000/1000: 100%|██████████| 1/1 [00:00<00:00, 239.69epoch/s, test_loss=6.5, train_loss=5.41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = SoftMaxModel(\n",
        "    year_ids_count, author_ids_count,\n",
        "    book_ids_count, in_embed_dims)\n",
        "best_model.load_state_dict(best_weights)\n",
        "print(f\"EPOCH {best_epoch} had lowest TEST LOSS: {round(best_loss, 3)} \\\n",
        "with TRAIN LOSS: {round(train_loss_at_best, 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKOUjkTDpwMd",
        "outputId": "9891134d-b603-4f6b-e0b3-2fef4f445466"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 940 had lowest TEST LOSS: 5.931 with TRAIN LOSS: 5.846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save book embeddings"
      ],
      "metadata": {
        "id": "jWzgZ7Jn84Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = best_model.book_embeddings.weight.detach().numpy()\n",
        "with open(embeddings_save_path, 'wb') as f:\n",
        "    pickle.dump(embeddings, f)"
      ],
      "metadata": {
        "id": "cKFenIfKnvTJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot loss progression"
      ],
      "metadata": {
        "id": "0jnmX3pM8wdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = [item[0] for item in epoch_losses]\n",
        "test_loss = [item[1] for item in epoch_losses]\n",
        "\n",
        "window_size = 20\n",
        "train_loss_avg = np.convolve(\n",
        "    train_loss,\n",
        "    np.ones(window_size) / window_size,\n",
        "    mode='valid')\n",
        "test_loss_avg = np.convolve(\n",
        "    test_loss,\n",
        "    np.ones(window_size) / window_size,\n",
        "    mode='valid')\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=list(range(len(epoch_losses))),\n",
        "                         y=train_loss, mode='lines', name='Train Loss'))\n",
        "fig.add_trace(go.Scatter(x=list(range(len(epoch_losses))),\n",
        "                         y=test_loss, mode='lines', name='Test Loss'))\n",
        "fig.add_trace(go.Scatter(x=list(range(window_size-1, len(epoch_losses))),\n",
        "                         y=train_loss_avg,\n",
        "                         mode='lines', name='Averaged Train Loss',\n",
        "                         line=dict(dash='dash', width=5, color='blue')))\n",
        "fig.add_trace(go.Scatter(x=list(range(window_size-1, len(epoch_losses))),\n",
        "                         y=test_loss_avg,\n",
        "                         mode='lines', name='Averaged Test Loss',\n",
        "                         line=dict(dash='dash', width=5, color='red')))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=f'Average batch mean loss during training\\\n",
        " from dataset with {len(user_to_books_df)} users and {book_ids_count} books',\n",
        "    xaxis_title='Epoch',\n",
        "    yaxis_title='Loss'\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "WJWskqlZJx0t",
        "outputId": "6cc58495-a6ba-492e-e7de-1f955e215253"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"50a90494-3a96-4d60-aa5b-d0f11b6d8bb0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"50a90494-3a96-4d60-aa5b-d0f11b6d8bb0\")) {                    Plotly.newPlot(                        \"50a90494-3a96-4d60-aa5b-d0f11b6d8bb0\",                        [{\"mode\":\"lines\",\"name\":\"Train Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[8.847296873728434,7.282176244826544,6.604927108401344,6.321051075344994,6.251258532206218,6.1951579593476795,6.188384351276216,6.153915178208124,6.171504406701951,6.1629035132271905,6.1694390660240535,6.162616230192638,6.175967125665574,6.158680643354144,6.171165784200032,6.1604415121532625,6.18037950424921,6.159600961776007,6.146608693259103,6.158191930680048,6.169734296344576,6.155747436341786,6.172648316337948,6.154779343377976,6.16458831514631,6.168440750667027,6.168170225052607,6.156461329687209,6.152078424181257,6.155067103249686,6.152898266201928,6.1196371260143465,6.143323966435024,6.168432894207182,6.167273748488653,6.17896218526931,6.17060527347383,6.1684785570417136,6.154267583574567,6.159214087894985,6.120289847964332,6.143369992574056,6.161282312302363,6.15858268737793,6.126552536374047,6.167534419468471,6.156948679969425,6.162018321809315,6.1328850246611095,6.157515321459089,6.164597147987003,6.13847541809082,6.153427305675688,6.107050282614572,6.159939606984456,6.124641100565593,6.141522566477458,6.154517786843436,6.139422144208636,6.133398192269461,6.1491752579098655,6.151730650947208,6.1394773210797995,6.145253249577114,6.14647779010591,6.159041518256778,6.135837736583891,6.108824048723493,6.1262639590672086,6.136206059228806,6.098844233013335,6.13624704451788,6.106339182172503,6.155106680733817,6.1285938535417825,6.130659852709089,6.141703151521229,6.105451470329648,6.0891984303792315,6.088483356294178,6.117517630259196,6.093543279738653,6.09202766418457,6.134853272210984,6.105418250674293,6.109953040168399,6.133145672934396,6.109418437594459,6.0997391655331565,6.109164646693638,6.110069592793782,6.11027869724092,6.101747240339007,6.089344546908424,6.133707659585135,6.085001627604167,6.116747038705008,6.140841983613514,6.071991761525472,6.106497037978399,6.09146667662121,6.102322010766892,6.092931452251616,6.110009057181222,6.077457382565453,6.144256682623,6.111332075936454,6.07368228549049,6.092781589144752,6.123871757870629,6.09483460017613,6.073787598382859,6.089327721368699,6.094715845017206,6.060020310538156,6.04090082077753,6.059088525317964,6.079155445098877,6.075093814304897,6.111264478592646,6.061184565226237,6.0867227372669035,6.122534343174526,6.0160677546546575,6.056745597294399,6.0705559594290595,6.066960675375802,6.052802403767903,6.0783817654564265,6.062813213893345,6.035477706364223,6.055961540767124,6.057788054148356,6.036107199532645,6.046906266893659,6.048194294884091,6.038668291909354,6.065929253896077,6.038560821896508,6.067029044741676,6.012239206404913,6.049059232076009,6.080029760088239,6.0568597203209285,6.056843803042457,6.037084897359212,6.023886203765869,6.04287592569987,6.011339596339634,6.025105067661831,6.099052497318813,6.021793796902611,6.056346643538702,5.976371083940778,6.049007552010672,6.056933743613107,6.035285404750279,5.982841718764532,6.0607875642322355,6.09585071745373,6.020701748984201,5.994214829944429,6.003589357648577,5.983514853886196,6.033594721839542,6.000099295661563,5.911058834620884,6.014221463884626,6.0117803528195335,6.014957882109142,5.997624033973331,5.942703292483375,5.973104613167899,6.051571550823393,6.008513496035621,5.9753579412187845,5.972715491340274,5.955303192138672,5.967342331295922,5.9749555587768555,5.931611696879069,5.949622631072998,5.9803628240312845,6.022664615086147,5.963934035528274,6.048379648299444,6.0323288554237005,5.980538209279378,5.961156731560116,5.962754771822975,6.02015563419887,5.981050445919945,5.98880874542963,5.941533928825741,6.011254628499349,5.94999497277396,5.967410042172387,5.985298292977469,5.908166658310663,5.926909968966529,5.979120663234165,5.999918188367571,5.993964853740874,5.950097969600132,5.969567980085101,5.975936549050467,5.994877588181269,5.882553736368815,5.900431405930292,5.912682260785784,5.953834238506499,5.964831193288167,5.926844528743199,5.953272660573323,5.837288833799816,5.950531414576939,5.923614115942092,5.956790174756732,5.998359952654157,5.989913736070905,5.905020350501651,5.897032942090716,6.009518305460612,5.938992704663958,5.983960083552769,5.92096083504813,5.909776891980853,5.989948113759358,5.886811165582566,5.889568169911702,5.865967705136254,5.959817840939476,5.956119196755545,5.894858269464402,5.951640446980794,5.955490566435314,5.886869112650554,5.935551870436895,5.886965070452009,5.882859139215379,5.903957571302142,5.920705204918271,5.927117915380569,5.937581993284679,5.933181535629999,5.907716319674537,5.844778265271868,5.914948917570568,5.912075996398926,5.886899766467867,5.895146574292864,5.8404130935668945,5.856224355243501,5.917556149618966,5.932130200522287,5.929683253878639,5.957898911975679,5.826025213514056,5.864293529873803,5.906873044513521,5.917730944497245,5.858034678867885,5.82517535345895,5.821651208968389,5.8147714251563665,5.962028208233061,5.875087442852202,5.85593337104434,5.782587664467948,5.881847971961612,5.887308393205915,5.8670333453587125,5.872663497924805,5.926165739695231,5.953791686466762,5.927397569020589,5.808170727321079,5.849006539299374,5.796443462371826,5.831178369976225,5.834822904495966,5.8913225673493885,5.904210249582927,5.8309452420189265,5.857821850549607,5.849207287742978,5.844120684124174,5.828152860913958,5.856171358199346,5.869493007659912,5.9267372857956655,5.772347700028193,5.807104678381057,5.781308719090053,5.907126517522903,5.845449561164493,5.783006418318975,5.954914365495954,5.8657443182809015,5.819763705843971,5.814711729685466,5.791165624346052,5.793524174463181,5.7291861942836215,5.902046135493687,5.969548997424898,5.882372038705008,5.83187130519322,5.866599400838216,5.861572311038063,5.823087238130116,5.84336519241333,5.836184002104259,5.671636899312337,5.841323443821499,5.880161580585298,5.856938521067302,5.819836888994489,5.834097158341181,5.827021417163667,5.744900135766892,5.76511980238415,5.886472452254522,5.853317124503,5.788760298774356,5.717641285487583,5.801934673672631,5.767592929658436,5.760632651192801,5.796795776912144,5.764315128326416,5.830615747542608,5.747890608651297,5.739104452587309,5.779282796950567,5.796455292474656,5.7377033687773205,5.828398886181059,5.796806562514532,5.9051287060692195,5.938943272545224,5.80469848996117,5.782628082093739,5.83794659659976,5.848220666249593,5.82728556224278,5.766882033575149,5.824088528042748,5.711687133425758,5.780782381693522,5.8004753930228095,5.723693734123593,5.778849874223981,5.766512575603667,5.728005273001535,5.8235185486929755,5.850760142008464,5.852099577585856,5.829995336986723,5.798403058733259,5.861792178381057,5.795201801118397,5.784529890332903,5.804258914220901,5.8392563093276255,5.802849656059628,5.793216909681048,5.84012322198777,5.719851720900762,5.7898723511468795,5.813890865870884,5.771564211164202,5.874461310250418,5.760685398465111,5.739008381253197,5.824103332701183,5.7849652880714055,5.782633077530634,5.783585116976783,5.8602279254368375,5.759689921424503,5.760271072387695,5.685638791038876,5.726271924518404,5.696743511018299,5.907465321677072,5.722735972631545,5.797694024585542,5.710888022468204,5.839138348897298,5.79310283206758,5.632672673179989,5.80557310013544,5.764065106709798,5.646017437889462,5.729270503634498,5.787961710067022,5.727665492466518,5.829219477517264,5.771849132719494,5.843201478322347,5.715926760718936,5.65753816422962,5.702449457986014,5.793089071909587,5.805956000373477,5.693071569715228,5.665601594107492,5.719240370250883,5.728749434153239,5.7718095098223,5.79177611214774,5.7237760452997115,5.772858006613595,5.627320357731411,5.793545972733271,5.725637186141241,5.790763900393531,5.725894927978516,5.710035800933838,5.825778802235921,5.8015897160484675,5.743448666163853,5.759561447870164,5.78081396647862,5.698121547698975,5.786717028844924,5.755028656550816,5.6429165204366045,5.688797405787876,5.767622697921026,5.766311781747,5.725372450692313,5.765993595123291,5.745965685163226,5.697707721165249,5.721975076766241,5.7459675925118585,5.704150449661982,5.805382955641973,5.760257857186454,5.809292066664923,5.808772246042888,5.6987859180995395,5.65792567389352,5.658762500399635,5.789702528998966,5.673608280363537,5.705140136537098,5.687529404958089,5.709605898175921,5.555297760736375,5.763552347819011,5.711508137839181,5.835043271382649,5.712954475766137,5.7860383079165505,5.739216486612956,5.748936516898019,5.759158543178013,5.721575986771357,5.600384712219238,5.6586372057596845,5.6304348309834795,5.683842386518206,5.73581177847726,5.6898544402349565,5.721026988256545,5.666245869227818,5.630433423178537,5.6976791109357565,5.65135889961606,5.746364797864642,5.697588625408354,5.652793498266311,5.758701574234736,5.783953280675979,5.730663322267079,5.730023815518334,5.6967364719935825,5.7724345070975165,5.796747956957136,5.661424591427758,5.662951310475667,5.699619633810861,5.752721763792492,5.688555785587856,5.6731888680231,5.650983288174584,5.578441756112235,5.7796539806184315,5.681916759127662,5.777311870029995,5.6405230930873325,5.801992212023054,5.639088993980771,5.740126269204276,5.6795427685692195,5.718682311830067,5.6623518126351495,5.70692457471575,5.63570145198277,5.774509475344703,5.647255261739095,5.658935773940313,5.649084431784494,5.678457010359991,5.757203556242443,5.641359420049758,5.660364854903448,5.6729096458071755,5.565930230276925,5.734743731362479,5.7331002326238725,5.659716083889916,5.597776526496524,5.600318068549747,5.645844459533691,5.664663837069557,5.606678349631173,5.679864111400786,5.687013013022287,5.755688848949614,5.680491333916073,5.732535816374279,5.505406970069522,5.593920412517729,5.681792168390183,5.698557558513823,5.674244403839111,5.700489679972331,5.6697341828119185,5.6926126934233165,5.693933804829915,5.556873321533203,5.554995945521763,5.742778846195766,5.692071256183443,5.652910936446417,5.725425356910343,5.539779390607562,5.569232032412574,5.7037346703665595,5.640543710617792,5.668755849202474,5.814958731333415,5.680979410807292,5.638413315727597,5.691013336181641,5.688956124441964,5.59105650583903,5.688576107933407,5.758960633050828,5.667861007508778,5.696817988441104,5.658843358357747,5.6451273418608165,5.667235692342122,5.684381189800444,5.634332021077474,5.633065223693848,5.476996262868245,5.598329816545759,5.6645001002720425,5.600263073330834,5.592476254417782,5.602816082182384,5.692326704661052,5.695825349716913,5.745537190210252,5.60385224932716,5.703823180425735,5.731053874606178,5.646065484909784,5.624288286481585,5.709565208071754,5.745594070071266,5.626754238491967,5.66164425441197,5.5819705327351885,5.742666108267648,5.615137690589542,5.641049589429583,5.609504359109061,5.629019918895903,5.711581343696231,5.721701712835403,5.5767442385355634,5.682272161756243,5.633735452379499,5.746839182717459,5.592204911368234,5.625480356670561,5.67660209110805,5.699521291823614,5.657905782972064,5.731416180020287,5.684859957013812,5.619423707326253,5.720639092581613,5.651543730781192,5.591086614699591,5.622585092272077,5.704236348470052,5.574721132005964,5.550316220238095,5.702192306518555,5.701306388491676,5.629198959895542,5.624486764272054,5.665324869610014,5.64996337890625,5.757696038200741,5.506261167072115,5.5728867167518255,5.631106058756511,5.639155183519636,5.71608136949085,5.619952315375919,5.508814312162853,5.581844829377674,5.717340696425665,5.66966438293457,5.571800481705439,5.722833406357538,5.591082164219448,5.550582545144217,5.544481618063791,5.587933018094017,5.555946100325811,5.68012337457566,5.610562210991269,5.66620549701509,5.706102280389695,5.641712529318673,5.729897430964878,5.630749520801363,5.588088625953311,5.588976564861479,5.530672186896915,5.628643240247454,5.63833643141247,5.510865733737037,5.5586566016787575,5.66036639894758,5.529630161467052,5.693142482212612,5.740842614855085,5.665147395361037,5.62500034059797,5.545900617327009,5.68625070935204,5.75404827935355,5.583124387831915,5.5092426254635765,5.586841878436861,5.527692794799805,5.508622010548909,5.613949957348051,5.529380775633312,5.615868454887753,5.639420577457973,5.525164036523728,5.563026428222656,5.615157740456717,5.52836613428025,5.711017722175235,5.6406994093032115,5.598804473876953,5.842874050140381,5.632737137022472,5.667262100038075,5.581198760441372,5.731296970730736,5.625976290021624,5.5373326029096335,5.63338988167899,5.475918270292736,5.604583876473563,5.654370807466053,5.6771847407023115,5.508868762425014,5.560198488689604,5.570276192256382,5.67653074718657,5.599444911593483,5.61108044215611,5.508896736871629,5.605205967312767,5.652585279373896,5.713918458847773,5.558913707733154,5.506443704877581,5.539022763570149,5.614423615591867,5.551275979904902,5.651010831197103,5.597506318773542,5.60222746077038,5.573941457839239,5.4559531438918345,5.611202943892706,5.50665233248756,5.692089830126081,5.527492977323986,5.48090918858846,5.573922475179036,5.512504918234689,5.564026877993629,5.578879855927967,5.691554046812511,5.440579618726458,5.551017693110874,5.55849720182873,5.5427570797148205,5.640569641476586,5.5738631884257,5.536324887048631,5.5381472905476885,5.650777544294085,5.4936421031043645,5.498856885092599,5.610684485662551,5.6115915434701105,5.483091218130929,5.603340035393124,5.615137395404634,5.51806438536871,5.437245823088146,5.484306426275344,5.645633175259545,5.590721516382127,5.688447838737851,5.563907418932233,5.5609455562773205,5.649477572668166,5.524874800727481,5.482878616877964,5.560634771982829,5.513699531555176,5.641385668799991,5.662719113486154,5.4996748651777,5.615375269026983,5.679185799189976,5.61606468473162,5.591977868761335,5.53911338533674,5.505982694171724,5.581559090387254,5.623274962107341,5.683452311016264,5.499455179486956,5.581083865392776,5.516456535884312,5.59008739108131,5.596419357118153,5.6281424931117465,5.677518708365304,5.5407258215404696,5.59644737697783,5.538826965150379,5.497132505689349,5.4512904939197355,5.564949648720877,5.600596314384823,5.664964108240037,5.6178644725254605,5.573361464909145,5.557322434016636,5.5390118190220425,5.542370183127267,5.590974103836786,5.580804915655227,5.5106513159615655,5.547259194510324,5.582577455611456,5.647247496105376,5.627906958262126,5.654328891209194,5.542016460782006,5.548546200706845,5.494109948476155,5.666859649476551,5.620513257526216,5.601509003412156,5.430144219171433,5.581762813386463,5.544487499055409,5.510567074730282,5.641896020798456,5.50455515725272,5.478081589653378,5.4221686862763905,5.4745435941787,5.478143510364351,5.62943324588594,5.528482096535819,5.540790830339704,5.5478262674240835,5.5200639225187755,5.51784256526402,5.391928241366432,5.55460414432344,5.432547955285935,5.518965153467088,5.552494344257173,5.4771590914045065,5.597444874899728,5.50626262029012,5.494806902749198,5.537434032985142,5.516448815663655,5.549470992315383,5.696362132117862,5.508694875807989,5.665138676053002,5.587750139690581,5.637354782649449,5.55014215196882,5.443552494049072,5.513722192673456,5.536849657694499,5.644459111349923,5.558127039954776,5.554559866587321,5.497672580537342,5.513051850455148,5.566063108898344,5.546742552802677,5.567237013862247,5.68236798331851,5.5373500642322355,5.486619881221226,5.678568862733387,5.570599056425548,5.60938192549206,5.397600855146136,5.672665459769113,5.401583853222075,5.444467680794852,5.511489550272624,5.493474528903053,5.477179027739025,5.535915942419143,5.48052842276437,5.569868428366525,5.570393607729957,5.507347538357689,5.415907723563058,5.557973339444115,5.521398748670306,5.498452754247756,5.47980683190482,5.341806525275821,5.555562178293864,5.522370497385661,5.586231254395985,5.60018439519973,5.424276874178932,5.4346546445574075,5.620203949156261,5.630764961242676,5.395607357933407,5.47480905623663,5.459698790595645,5.556528409322103,5.430425030844552,5.542853378114247,5.395797411600749,5.515626907348633,5.403746355147589,5.672926039922805,5.454505193801153,5.5241988272894,5.501255171639579,5.501747131347656,5.428103515080044,5.62189295178368,5.559747900281634,5.498334067208426,5.514934153783889,5.4616889499482655,5.489272344680059,5.516804854075114,5.489617029825847,5.5095321337382,5.502172015962147,5.462589082263765,5.521304425739107,5.380899974278042,5.521816049303327,5.596653870173863,5.593816825321743,5.462021101088751,5.436588400886173,5.631470839182536,5.541995003109887,5.513986587524414,5.352354571932838,5.618446145738874,5.670827184404645,5.468831175849552,5.548493794032505,5.575998533339727,5.398291610536122,5.424251783461798,5.451508453914097,5.553014641716366,5.531680266062419,5.584034647260394,5.623408090500605,5.411272616613479,5.606598513466971,5.480844792865572,5.497971057891846,5.5791534923371815,5.5498147919064476,5.383974325089228,5.53633975982666,5.436004229954311,5.409811178843181,5.543993450346447,5.492225215548561,5.545572712307885,5.660984470730736,5.443760712941487,5.434386276063465,5.395529065813337,5.44670836130778,5.5878758657546275,5.491448833828881,5.5106643949236185,5.607091949099586,5.521139962332589,5.526886349632626,5.518382640112014,5.617523193359375,5.554523831322079,5.485168911161876,5.470738569895427,5.559633981613886,5.633117584955125,5.504177524929955,5.549337682269869,5.532605784279959,5.429799897330148,5.50490125020345,5.48959984098162,5.525641055334182,5.5586427961077005,5.4861614136468795,5.537795589083717,5.569729691460019,5.573260148366292,5.48901351292928,5.469031901586623,5.540017514001756,5.509119079226539,5.538381553831554,5.542795817057292,5.439044157663981,5.417458466121128,5.383088815779913,5.527449358077276,5.494549410683768,5.543672879536946,5.482649530683245,5.463088580540249,5.568668569837298,5.435365381694975,5.435502120426723,5.478004478272938,5.461757251194546,5.460010324205671,5.498833815256755,5.585478192283993,5.435629594893682,5.41797669728597,5.412525721958706,5.55118145261492,5.555168833051409,5.4203130177089145,5.48742709841047,5.525868529365177,5.579066299256825,5.397143341246105,5.4640788577851795,5.463624318440755,5.588763850075858,5.462681043715704,5.518938382466634,5.493570282345726,5.6181672641209195,5.485116186596098,5.534778436024983,5.340997287205288,5.5180793943859285,5.572535878136044,5.406514871688116],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Test Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[7.877546310424805,7.086803436279297,6.726574897766113,6.61953592300415,6.517645359039307,6.523248672485352,6.451627731323242,6.517729759216309,6.525259494781494,6.498183250427246,6.498736381530762,6.5380964279174805,6.5332136154174805,6.518682479858398,6.529767036437988,6.554632186889648,6.488377094268799,6.479578971862793,6.474056243896484,6.487706184387207,6.526465892791748,6.49557638168335,6.501725196838379,6.517338275909424,6.515706539154053,6.48513126373291,6.491335391998291,6.5645976066589355,6.4602274894714355,6.493956089019775,6.488320350646973,6.530136585235596,6.527122974395752,6.472939968109131,6.542335510253906,6.477945804595947,6.486637115478516,6.457706928253174,6.489610195159912,6.508029460906982,6.4873127937316895,6.455910682678223,6.512673854827881,6.479787826538086,6.50627326965332,6.499288082122803,6.502019882202148,6.495180606842041,6.490415096282959,6.528282642364502,6.522250652313232,6.493839263916016,6.456270217895508,6.489004611968994,6.543302059173584,6.472562313079834,6.467663764953613,6.500972747802734,6.470969200134277,6.454870700836182,6.494907855987549,6.458804607391357,6.536250591278076,6.538558006286621,6.5224432945251465,6.539215564727783,6.475887298583984,6.544591903686523,6.46665620803833,6.494575500488281,6.468146324157715,6.558755874633789,6.5092878341674805,6.51247501373291,6.4131879806518555,6.544317722320557,6.427611351013184,6.517468452453613,6.552133560180664,6.506621837615967,6.519205093383789,6.536539077758789,6.454536437988281,6.494894504547119,6.4997663497924805,6.478980541229248,6.522087097167969,6.537881851196289,6.516379356384277,6.4723968505859375,6.486443042755127,6.430427551269531,6.531811714172363,6.575643539428711,6.4614577293396,6.48257303237915,6.591943264007568,6.553622722625732,6.501428604125977,6.441232204437256,6.499190330505371,6.569118022918701,6.490139007568359,6.460270404815674,6.5487284660339355,6.529729843139648,6.339367866516113,6.503826141357422,6.482990264892578,6.454875469207764,6.4643144607543945,6.489088535308838,6.494409561157227,6.489311695098877,6.565674781799316,6.448421001434326,6.4535064697265625,6.510181427001953,6.502833843231201,6.596697807312012,6.58059024810791,6.433530330657959,6.547341346740723,6.440459728240967,6.500084400177002,6.438377380371094,6.505395412445068,6.5260796546936035,6.568888187408447,6.44246768951416,6.4860968589782715,6.502090930938721,6.454937934875488,6.5872063636779785,6.5280280113220215,6.524804592132568,6.457929611206055,6.605125427246094,6.5016984939575195,6.480657577514648,6.4951019287109375,6.530647277832031,6.465270519256592,6.488956928253174,6.566884994506836,6.5437421798706055,6.533965587615967,6.475216388702393,6.487138748168945,6.498662948608398,6.560969829559326,6.57460355758667,6.4879045486450195,6.479207515716553,6.533013343811035,6.497696399688721,6.422522068023682,6.458925247192383,6.538369178771973,6.49246072769165,6.446737289428711,6.474308967590332,6.363633632659912,6.475182056427002,6.471776485443115,6.5193772315979,6.401258945465088,6.465156078338623,6.462412357330322,6.47512674331665,6.407442092895508,6.508671283721924,6.427480697631836,6.442732334136963,6.421670913696289,6.41652774810791,6.4551682472229,6.566283702850342,6.4399518966674805,6.447895526885986,6.464383125305176,6.459792137145996,6.562108039855957,6.474505424499512,6.52796745300293,6.4525299072265625,6.3784260749816895,6.442750453948975,6.370964050292969,6.4830803871154785,6.466126918792725,6.350226879119873,6.511218070983887,6.580180644989014,6.368579387664795,6.465857982635498,6.446333885192871,6.511285305023193,6.465504169464111,6.55953311920166,6.391871929168701,6.4873433113098145,6.551632881164551,6.433065414428711,6.363943099975586,6.5201029777526855,6.471217155456543,6.396989822387695,6.38893985748291,6.559595108032227,6.345094203948975,6.397511005401611,6.555865287780762,6.494885444641113,6.565958023071289,6.579418659210205,6.480459213256836,6.470489501953125,6.484890937805176,6.457627296447754,6.474061489105225,6.347921371459961,6.381150245666504,6.4926371574401855,6.4227614402771,6.4432759284973145,6.5768303871154785,6.502446174621582,6.462436676025391,6.367749214172363,6.640991687774658,6.3116912841796875,6.476230144500732,6.390579700469971,6.408703804016113,6.487277030944824,6.371895790100098,6.454646587371826,6.353515148162842,6.519026279449463,6.43752384185791,6.502087593078613,6.399886608123779,6.512640476226807,6.383368968963623,6.458126068115234,6.523705005645752,6.423129081726074,6.523692607879639,6.428640365600586,6.547989368438721,6.333532810211182,6.395107746124268,6.401544570922852,6.442488670349121,6.477086544036865,6.464166164398193,6.502584457397461,6.563309669494629,6.467830181121826,6.377328395843506,6.380661487579346,6.386660575866699,6.331108570098877,6.507603168487549,6.60886812210083,6.538190841674805,6.614439010620117,6.569421768188477,6.446744441986084,6.471564292907715,6.50429105758667,6.559897422790527,6.420354843139648,6.297645092010498,6.508826732635498,6.490303039550781,6.4265828132629395,6.409003257751465,6.476857662200928,6.314843654632568,6.373249053955078,6.473837375640869,6.4259209632873535,6.398624897003174,6.436171531677246,6.438302993774414,6.35722541809082,6.444464206695557,6.525787353515625,6.375226020812988,6.46652364730835,6.545398712158203,6.403718948364258,6.473414897918701,6.317132949829102,6.463886737823486,6.353438377380371,6.436871528625488,6.380337715148926,6.497034072875977,6.537778377532959,6.651030540466309,6.515376091003418,6.45175838470459,6.3702473640441895,6.494688987731934,6.310362815856934,6.411105632781982,6.515104293823242,6.403106212615967,6.449788570404053,6.423679828643799,6.645184516906738,6.528733253479004,6.386020183563232,6.433716773986816,6.375765800476074,6.483028888702393,6.460007667541504,6.500536918640137,6.465428352355957,6.5755228996276855,6.427691459655762,6.3870463371276855,6.40370512008667,6.554466724395752,6.435620307922363,6.39064359664917,6.559126853942871,6.5677571296691895,6.500833511352539,6.403548240661621,6.5984673500061035,6.429309844970703,6.31659460067749,6.547198295593262,6.365170955657959,6.667670726776123,6.412095546722412,6.335343360900879,6.313711643218994,6.420853614807129,6.463829040527344,6.606131076812744,6.462218761444092,6.4395928382873535,6.567607879638672,6.381650447845459,6.50027322769165,6.519168853759766,6.616092681884766,6.413932800292969,6.342017650604248,6.622208118438721,6.5517754554748535,6.470803260803223,6.372456073760986,6.368565082550049,6.436671257019043,6.348382472991943,6.399677753448486,6.42437219619751,6.413292407989502,6.414181232452393,6.475113868713379,6.514242172241211,6.441305160522461,6.600083827972412,6.43087100982666,6.549131870269775,6.2237162590026855,6.295925617218018,6.261139869689941,6.263861656188965,6.4327311515808105,6.340692520141602,6.493338108062744,6.495926380157471,6.336172103881836,6.581795692443848,6.462964057922363,6.438339710235596,6.388593673706055,6.554028511047363,6.393759250640869,6.396388053894043,6.507767677307129,6.633482933044434,6.382203578948975,6.464064121246338,6.472914218902588,6.490335941314697,6.508933067321777,6.5550665855407715,6.398138523101807,6.64578104019165,6.4810662269592285,6.261510848999023,6.444005489349365,6.395471096038818,6.535378456115723,6.384430885314941,6.412452220916748,6.559449672698975,6.253692626953125,6.3678483963012695,6.447900295257568,6.415803909301758,6.502835750579834,6.417157173156738,6.453550815582275,6.410223007202148,6.406201362609863,6.467741966247559,6.391202926635742,6.379176616668701,6.289447784423828,6.334576606750488,6.629619598388672,6.444019794464111,6.441652297973633,6.367374897003174,6.444983959197998,6.441267490386963,6.368237495422363,6.482898235321045,6.462395191192627,6.524123668670654,6.463925838470459,6.456656455993652,6.424812316894531,6.510724067687988,6.375569820404053,6.49222469329834,6.2750563621521,6.502053260803223,6.399482250213623,6.252366542816162,6.493033409118652,6.323631763458252,6.378977298736572,6.456851005554199,6.424595355987549,6.406810760498047,6.333422660827637,6.483322620391846,6.389398574829102,6.5251898765563965,6.377949237823486,6.244390487670898,6.499600887298584,6.393463134765625,6.664539337158203,6.470869541168213,6.3230299949646,6.425500392913818,6.310088634490967,6.350823879241943,6.356578350067139,6.424959659576416,6.318655014038086,6.458560943603516,6.532149791717529,6.3981099128723145,6.4143571853637695,6.315870761871338,6.282386302947998,6.455788612365723,6.382866382598877,6.507306098937988,6.442166328430176,6.251264572143555,6.371958255767822,6.639399528503418,6.356785297393799,6.282662391662598,6.579545021057129,6.475893020629883,6.428363800048828,6.364536762237549,6.369847774505615,6.283700942993164,6.445517539978027,6.343318462371826,6.458297252655029,6.565068244934082,6.427101135253906,6.499834060668945,6.645709037780762,6.314094066619873,6.252800464630127,6.454154014587402,6.390070915222168,6.2630181312561035,6.544677734375,6.480821132659912,6.479135036468506,6.332834720611572,6.435094356536865,6.325760841369629,6.261834144592285,6.502176284790039,6.464452743530273,6.257606029510498,6.429286479949951,6.459543228149414,6.360795974731445,6.517402172088623,6.5288920402526855,6.2402496337890625,6.404634475708008,6.4372334480285645,6.3328094482421875,6.684039115905762,6.532717227935791,6.180014610290527,6.281168460845947,6.373443603515625,6.294366836547852,6.386760234832764,6.360878944396973,6.323687553405762,6.421848297119141,6.418507099151611,6.583464622497559,6.517458438873291,6.572841167449951,6.461579322814941,6.4980974197387695,6.380823612213135,6.29113245010376,6.515147686004639,6.350025653839111,6.63355016708374,6.536949634552002,6.355676174163818,6.344668388366699,6.512239456176758,6.5950164794921875,6.494403839111328,6.36631441116333,6.490848541259766,6.54903507232666,6.344010829925537,6.497379779815674,6.384592056274414,6.365293025970459,6.462775707244873,6.490156650543213,6.679935932159424,6.435964107513428,6.566027641296387,6.119506359100342,6.408791542053223,6.491988658905029,6.386929988861084,6.256665229797363,6.429331302642822,6.233492374420166,6.498195171356201,6.456450462341309,6.421304702758789,6.446910381317139,6.359334945678711,6.564570903778076,6.547048091888428,6.5344438552856445,6.318338394165039,6.408531665802002,6.407219409942627,6.392853736877441,6.550879955291748,6.349559783935547,6.464049339294434,6.3784565925598145,6.409801483154297,6.2483439445495605,6.294814586639404,6.498286247253418,6.285340309143066,6.35883903503418,6.546933174133301,6.508449554443359,6.320882797241211,6.361247539520264,6.359953880310059,6.380931377410889,6.3501715660095215,6.239516735076904,6.353092193603516,6.319924831390381,6.404489040374756,6.464755058288574,6.394984722137451,6.2557148933410645,6.3802924156188965,6.376040935516357,6.395994663238525,6.469996929168701,6.341316223144531,6.346962928771973,6.2923173904418945,6.291499614715576,6.250695705413818,6.293442249298096,6.547857761383057,6.536720275878906,6.259158611297607,6.379185199737549,6.450206756591797,6.479724884033203,6.306850433349609,6.390551567077637,6.115002155303955,6.482327938079834,6.563581943511963,6.198345184326172,6.229806900024414,6.349541187286377,6.375061511993408,6.350952625274658,6.222050189971924,6.672995090484619,6.566014289855957,6.461194038391113,6.619004726409912,6.5311455726623535,6.147036552429199,6.579707145690918,6.498420715332031,6.532214164733887,6.338187217712402,6.3200364112854,6.457360744476318,6.268327713012695,6.441720485687256,6.532561302185059,6.444849491119385,6.175090312957764,6.398924827575684,6.387550354003906,6.577080249786377,6.549008846282959,6.490055561065674,6.544238567352295,6.285829544067383,6.292789936065674,6.29807186126709,6.334832668304443,6.298462867736816,6.40663480758667,6.579288482666016,6.467989921569824,6.586940288543701,6.622137069702148,6.49421501159668,6.416627407073975,6.2503981590271,6.254615783691406,6.357973098754883,6.325977325439453,6.3900465965271,6.614026069641113,6.439731597900391,6.343073844909668,6.459761619567871,6.518195152282715,6.409999847412109,6.401354789733887,6.381948471069336,6.3510422706604,6.260776042938232,6.204882621765137,6.342991828918457,6.364823818206787,6.2281293869018555,6.372796535491943,6.366448879241943,6.436295986175537,6.443207263946533,6.447902679443359,6.320894718170166,6.458080768585205,6.349071979522705,6.4516119956970215,6.19385290145874,6.409030437469482,6.421998500823975,6.2688398361206055,6.374913215637207,6.443787574768066,6.330696105957031,6.226163387298584,6.3574676513671875,6.470200061798096,6.32846212387085,6.163115501403809,6.587906360626221,6.245379447937012,6.3924407958984375,6.578311920166016,6.613237380981445,6.21884298324585,6.419435024261475,6.316814422607422,6.324565887451172,6.2783203125,6.352472305297852,6.4123077392578125,6.3432745933532715,6.487306118011475,6.365225791931152,6.466707706451416,6.2616119384765625,6.332120418548584,6.45909309387207,6.381827354431152,6.520312309265137,6.571146011352539,6.552646636962891,6.3387908935546875,6.4161696434021,6.434250354766846,6.316732406616211,6.3792805671691895,6.490471839904785,6.381872653961182,6.265551567077637,6.519367694854736,6.4336838722229,6.379593372344971,6.274403095245361,6.512965679168701,6.372199058532715,6.251880168914795,6.514791965484619,6.240399360656738,6.416266441345215,6.36057710647583,6.387180328369141,6.269405364990234,6.322503089904785,6.276564598083496,6.3438720703125,6.287330150604248,6.457584857940674,6.626796245574951,6.113279342651367,6.3773322105407715,6.2876996994018555,6.60709285736084,6.486515522003174,6.447458744049072,6.393211364746094,6.43013334274292,6.414258003234863,6.432342052459717,6.496628761291504,6.536206245422363,6.512373447418213,6.5395827293396,6.5720624923706055,6.440845966339111,6.510446548461914,6.311431884765625,6.397517681121826,6.500542640686035,6.440754413604736,6.28583288192749,6.340015888214111,6.339650630950928,6.582169055938721,6.25021505355835,6.32943058013916,6.3997626304626465,6.310583114624023,6.588712215423584,6.327542304992676,6.256311893463135,6.2412638664245605,6.414879322052002,6.5154242515563965,6.39391565322876,6.355688095092773,6.219557762145996,6.438218593597412,6.388310432434082,6.3921074867248535,6.379312992095947,6.417521953582764,6.567469596862793,6.368670463562012,6.55914831161499,6.631801128387451,6.58108377456665,6.458541393280029,6.323444843292236,6.313754558563232,6.503772735595703,6.476417064666748,6.4278693199157715,6.387813091278076,6.27392053604126,6.245813846588135,6.505917072296143,6.19894552230835,6.140120983123779,6.3408203125,6.251782417297363,6.512500762939453,6.272971153259277,6.3280487060546875,6.444239139556885,6.278641223907471,6.445355415344238,6.2416768074035645,6.41996955871582,6.4075775146484375,6.2870306968688965,6.534437656402588,6.452805519104004,5.956571102142334,6.419429302215576,6.317049980163574,6.281746864318848,6.352710723876953,6.455022811889648,6.423614501953125,6.284707069396973,6.439347267150879,6.42875337600708,6.15291166305542,6.447232723236084,6.388927459716797,6.431192874908447,6.358055591583252,6.433141708374023,6.253561973571777,6.387037754058838,6.333986759185791,6.294086933135986,6.440174102783203,6.356331825256348,6.550879955291748,6.440356254577637,6.328428268432617,6.441359519958496,6.362070083618164,6.336343765258789,6.510112762451172,6.277879238128662,6.283718109130859,6.328559875488281,6.6005330085754395,6.199061870574951,6.590968132019043,6.402952194213867,6.420399188995361,6.582533359527588,6.2592058181762695,6.2591657638549805,6.406073093414307,6.621337413787842,6.300560474395752,6.4358439445495605,6.456432819366455,6.516522407531738,6.420131683349609,6.4183735847473145,6.351124286651611,6.543245792388916,6.433021068572998,6.36740779876709,6.251561164855957,6.406113624572754,6.502901077270508,6.457867622375488,6.489727973937988,6.301023483276367,6.547613143920898,6.3533711433410645,6.4590044021606445,6.446354389190674,6.11922550201416,6.403610706329346,6.4342803955078125,6.448816299438477,6.48546838760376,6.430363655090332,6.294712543487549,6.440569877624512,6.468920707702637,6.454616546630859,6.405958652496338,6.588400363922119,6.187669277191162,6.290975570678711,6.299315929412842,6.245972633361816,6.579855442047119,6.348554611206055,6.387786865234375,6.507976055145264,6.34553861618042,6.409055233001709,6.478837013244629,6.633072853088379,6.290597915649414,6.339122295379639,6.2410359382629395,6.287567615509033,6.498044490814209,6.149404525756836,6.454789638519287,6.443412780761719,6.326230049133301,6.316507816314697,6.166507244110107,6.334000110626221,6.327948570251465,6.529670238494873,6.543150424957275,6.318795204162598,6.436671733856201,6.344903945922852,6.346808910369873,6.284688949584961,6.558567523956299,6.546704292297363,6.222049236297607,6.399446964263916,6.52584981918335,6.414780616760254,6.278102397918701,6.312108516693115,6.512304782867432,6.511214733123779,6.581002712249756,5.9307050704956055,6.346346855163574,6.392729759216309,6.264690399169922,6.658401012420654,6.222918510437012,6.433518886566162,6.205451488494873,6.531233787536621,6.437999248504639,6.501744270324707,6.2668137550354,6.490011692047119,6.2850661277771,6.2704291343688965,6.465750217437744,6.3638997077941895,6.157156944274902,6.504005432128906,6.335210800170898,6.303903579711914,6.306636333465576,6.432464599609375,6.347264766693115,6.549587726593018,6.324740886688232,6.3609209060668945,6.566628456115723,6.640696048736572,6.340335369110107,6.333881855010986,6.437060356140137,6.487156391143799,6.440273284912109,6.286666393280029,6.267931938171387,6.411634922027588,6.554633140563965,6.413245677947998,6.2855706214904785,6.347619533538818,6.189364433288574,6.407779216766357,6.646073341369629,6.41136360168457,6.092533588409424,6.336694240570068,6.52590799331665,6.2569379806518555,6.663250923156738,6.258208274841309,6.3651123046875,6.168824195861816,6.438183784484863,6.392808437347412,6.352521896362305,6.097814559936523,6.236051082611084,6.420163631439209,6.228130340576172,6.496222496032715],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\",\"dash\":\"dash\",\"width\":5},\"mode\":\"lines\",\"name\":\"Averaged Train Loss\",\"x\":[19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[6.391083334741137,6.257205205871945,6.200883765447707,6.179269825844537,6.170956239246188,6.166622728393191,6.16528686795916,6.1642761616479795,6.1644034692219325,6.1634321700958985,6.163040349597023,6.162213309605916,6.160064354397003,6.158432196435476,6.158919808978127,6.158725207192558,6.159651240848361,6.159162529309592,6.159606409072877,6.15998935358865,6.160040461449396,6.157568239030385,6.156949366841998,6.156381066640219,6.156571233840216,6.154669444901603,6.154624128341675,6.154063051087515,6.1543409006936205,6.153381230717614,6.153503641628085,6.1540885857173375,6.155030500321162,6.155535667283195,6.152466536703566,6.152099829628355,6.149383775393168,6.14792964004335,6.147231601533436,6.146489329565139,6.145198534783863,6.1466428052811395,6.147060838199797,6.14597058863867,6.145304116748628,6.146300379435222,6.145875734374637,6.14482018720536,6.142160473551069,6.141829420271375,6.1407639571598605,6.137476311411176,6.1373648927325295,6.13501048655737,6.137413306463332,6.135846018791199,6.136146956398375,6.136155985650563,6.133702669824874,6.131191484133403,6.128945742334638,6.127362860952105,6.124453492391678,6.122081009546916,6.12156101067861,6.119508033707029,6.11705360980261,6.116919006620135,6.116948726063684,6.115622486386981,6.114270415760222,6.1148316837492445,6.113533266385397,6.113303669293722,6.110015562602453,6.11027125290462,6.107988341649373,6.106740536008562,6.108510061672755,6.107649728230069,6.10855041231428,6.107247864632379,6.1076868011837915,6.107731990587144,6.106489779835655,6.1050917364302135,6.106806918552943,6.105716238703047,6.103929431097849,6.1035815522784285,6.104316907837279,6.103555158206396,6.1017306032634915,6.101109627314976,6.101378192220416,6.097693824768066,6.095488784426735,6.092605858757382,6.089521531831652,6.089676634470623,6.089915006501334,6.088400900931586,6.087620937256586,6.089101081802731,6.084404016676404,6.083368427412851,6.0796833912531545,6.0774648212251225,6.0764208271389935,6.075700835954577,6.0726479087557115,6.069680064065117,6.068788761184329,6.067211777823313,6.064281345549085,6.06362564336686,6.063990317072188,6.062969305401756,6.062307995841617,6.060481346221198,6.058269574528648,6.055822306587582,6.053939131328039,6.051813902173723,6.053853500457039,6.053858410744441,6.052184857640947,6.050031134060452,6.049534810157049,6.046182701701211,6.044297294389635,6.0474760339373645,6.045767646744139,6.045695576213655,6.042708770434062,6.042813834689912,6.0432508071263635,6.043081662768409,6.038927286011832,6.040038623128618,6.041479706764221,6.041902833893186,6.039160613786606,6.035338593664623,6.0316713503428865,6.030508896282741,6.028659616197857,6.023018247740609,6.021585524649847,6.021607562473843,6.021100203196209,6.016028780028935,6.012074254807972,6.007912153289431,6.011672176633563,6.00964747383481,6.005568683715094,6.002440188044594,6.001063261713302,5.9963910000664855,5.990346242132642,5.985891739527385,5.983662129583814,5.9825008029029485,5.984458290962946,5.980975256647383,5.9833892742792765,5.9894527753194176,5.9877686125891545,5.985237431526184,5.982627276011876,5.983753856023153,5.985671213694982,5.986456420308068,5.980954539208185,5.981091595831373,5.979823447409132,5.9795581749507365,5.981057929992676,5.978099146343412,5.975696866852896,5.978072315170651,5.980587093035381,5.981267194520861,5.977638862246559,5.9779205594744,5.974298404511952,5.97242584114983,5.967526617504301,5.9644903512228105,5.9619867256709504,5.9586706558863325,5.9578596932547425,5.954761482420422,5.955348419007801,5.9466501292728235,5.9466769513629725,5.944487155051458,5.943061749140422,5.947571413857596,5.950721602212814,5.947016586576188,5.941872324262346,5.942649996848332,5.942094733601524,5.942814338774908,5.940065553074792,5.935810518264772,5.941180237134299,5.940499225116913,5.939343520573208,5.934950193904696,5.934699526287259,5.936163259687877,5.933242540132431,5.938960120791481,5.939208078384399,5.937370828219822,5.93630891300383,5.930739168893723,5.925386439050947,5.925333300090971,5.926516913232349,5.922396893728347,5.922326358159384,5.919787430763246,5.919125204994566,5.9158752736591165,5.912125313849676,5.913388555390494,5.913255135218304,5.914714078676134,5.908743841307503,5.903749099231902,5.904883993239631,5.903908480916705,5.902618115288871,5.906169605255127,5.900693272408986,5.899559695380074,5.900760390644982,5.9014490593047375,5.898315533002219,5.893218404906137,5.887421865690323,5.8815013601666415,5.884216954594567,5.885732413473583,5.882781636147273,5.876307219550723,5.87605462982541,5.875662720771063,5.876993733360654,5.877815690494719,5.878246169998533,5.879329244295756,5.879214960052854,5.871728550820125,5.87287761710939,5.869485113734291,5.865700380007426,5.861554978007363,5.863219372431438,5.867171117237636,5.867635818890164,5.869788340159825,5.8641472941353205,5.862598956198919,5.8612099306924,5.86488911537897,5.864271367163885,5.866242811793372,5.861508529526846,5.858230588549659,5.8509877375194,5.8486544790722075,5.8445570786794026,5.843298863229299,5.848594254539127,5.8520592973345815,5.851488564127969,5.850483005387443,5.845475158237276,5.839940854481289,5.834852902094523,5.837064116341727,5.843081201825824,5.844993769554867,5.845179691768829,5.845701093900772,5.845305059069679,5.840122556686403,5.843673431305659,5.845127397491819,5.839643806502932,5.836353652817863,5.838089253788902,5.841785858926319,5.835031985101246,5.833449627104261,5.833812512670244,5.830321932974315,5.82901964187622,5.833667055765788,5.839873602276756,5.83420931044079,5.821613924843924,5.817592056592305,5.814378137815567,5.809079800333295,5.805840973627,5.802902368136816,5.802264895893279,5.79785022622063,5.801223603884379,5.7981215715408325,5.7939362571353,5.787974499520802,5.78840259938013,5.786538069588797,5.790443434034075,5.800145590872992,5.802124525251843,5.796932306743804,5.796163780348642,5.799136798722403,5.804619012560163,5.802866380555289,5.805691160474505,5.803243884586153,5.802443214825222,5.804251228060042,5.798905127389091,5.800453090667725,5.801823496818542,5.799259620621091,5.800612783432006,5.806265622093563,5.8074506566638044,5.809110095387414,5.803773813020615,5.799916258312407,5.7994414238702685,5.799536514282227,5.797852130163284,5.7974039123171845,5.796182117008028,5.797498860813322,5.7983005955105735,5.798708824884324,5.799163323356993,5.799834096999397,5.802227620851427,5.807008192652749,5.806716833795821,5.8072669892084035,5.807296228408814,5.804006485711961,5.800533160709199,5.798212649708702,5.801303893043881,5.796198780196054,5.794452243759518,5.789507688794817,5.785608339309691,5.7784826993942255,5.783713482675098,5.7801894358226225,5.778067975952511,5.777619791030884,5.780083090918405,5.77904368922824,5.772099112329028,5.76865470182328,5.768823687235515,5.764174140067327,5.759432498613994,5.7595823197137745,5.756833940460568,5.759115658487593,5.754696718851726,5.758872296696618,5.7566550811131805,5.755250049772718,5.754058926446097,5.758876204490662,5.753800738425482,5.752317518279666,5.745712896755763,5.746130514144896,5.740611068407694,5.739546402295431,5.7475015742438185,5.7434117215020315,5.743851366497221,5.742916512489319,5.746130285944257,5.743014059747968,5.7461689801443185,5.741002752667381,5.737912086078099,5.7370409522737775,5.741324100040254,5.745619625136967,5.748475224631174,5.747861469359625,5.7424697467259,5.747152019682384,5.75162337280455,5.747807180313837,5.745809578895569,5.745600238300506,5.744327021780469,5.744406842050099,5.744063621475583,5.749995887847174,5.745203975268773,5.745020869800023,5.742781054405939,5.741693830490113,5.74646118822552,5.743185140973047,5.743570258503869,5.74683643749782,5.743797661009289,5.7376532463800345,5.735685294015067,5.735834569022769,5.731763550213405,5.73487473101843,5.734811330976941,5.731910490989685,5.721359789939154,5.723268784795489,5.720544511931283,5.724998391242255,5.725760728972299,5.728963890529814,5.728626335234869,5.730865638596671,5.728554417973473,5.7266203244527185,5.716174956730434,5.7086682047162745,5.705250650360471,5.706546485991705,5.710398949895587,5.705406545457386,5.7077774808520365,5.705832767486572,5.702977968397595,5.702381629035587,5.707184685979571,5.706325308481853,5.705629332860312,5.6965168442044956,5.698804199127924,5.698699947765895,5.698272289548601,5.6973266544796175,5.694205550920396,5.696748476936703,5.706566639173598,5.706706008457004,5.708331832431611,5.709120694796244,5.709966194062006,5.709901261329651,5.707509355317979,5.706746226265317,5.704146642912001,5.708245386396135,5.709773279371715,5.711320632979984,5.708467356363934,5.715927292051769,5.709946663039072,5.707755312465486,5.705199284780593,5.704632209596181,5.702912976628259,5.699637480009169,5.691585154760452,5.697239398956299,5.69645459651947,5.694420403525943,5.689238536925544,5.68873359816415,5.692934332575117,5.692453139168875,5.696549294108436,5.691212077367874,5.685412750925337,5.683284343991962,5.687913200968788,5.680799394562131,5.678733771187918,5.671743361155192,5.670058445703415,5.667357521965389,5.664573848815191,5.663220825649443,5.665786403701419,5.664845372381665,5.666507175990513,5.6701871781122115,5.6630033050264625,5.658776475134349,5.655005905741737,5.65786581266494,5.658559790111724,5.6599387918199815,5.665128989446731,5.663022437549773,5.661064116160075,5.65592197804224,5.653782948993501,5.660905987875801,5.66321732770829,5.662629682677133,5.668567033041092,5.661562797001431,5.655673747970945,5.6530760390417925,5.6510786578768775,5.647889659518287,5.6633672475814825,5.66772019749596,5.665551254862832,5.665174043746223,5.665909629776365,5.6604379710697,5.661380067325774,5.664697464307149,5.663393824441092,5.670391057786487,5.675583428428286,5.670700853211539,5.669459075019472,5.671032587687174,5.666477920895531,5.671142212549844,5.666530424072628,5.661260181381588,5.6624580008643015,5.65903336207072,5.647909238224938,5.644001071793692,5.646696741240365,5.646937341917128,5.649766395205543,5.65040618237995,5.651168536004566,5.649773198082334,5.648683421952385,5.645056936854409,5.647593029340109,5.652616365750631,5.650592293058123,5.6494554462887,5.646837371871586,5.652317416100275,5.65922448748634,5.661360476130531,5.658610689072382,5.660048531350636,5.666003785814557,5.671948067347209,5.666168944040935,5.665491284642901,5.659901197751364,5.667050544420877,5.661469630968003,5.656190955071222,5.657717785381136,5.661479435648237,5.658896464393253,5.658187569890704,5.661092855816797,5.658981828462511,5.665915256454832,5.661359137580509,5.660156583786011,5.6592333589281365,5.663969958396185,5.661255019051689,5.653191762878781,5.6522162925629384,5.658444400060745,5.65579073996771,5.655328305562336,5.651252589906965,5.654140513283866,5.660751297360375,5.652234251158578,5.645902522404989,5.644562536194212,5.639949486369179,5.641510556993031,5.641536987395514,5.630945748374575,5.627460803304399,5.6337735073907025,5.636127471923828,5.629505678585597,5.636911292303176,5.638949589502245,5.631369101433527,5.623527862912132,5.621464565822056,5.6180375326247445,5.618777457873026,5.616807399477278,5.612232872417995,5.622224928083875,5.625666218712217,5.630605787322635,5.630185504186721,5.623785867009844,5.622237079484122,5.623329973220825,5.625669893764314,5.621719680513655,5.613779748053778,5.613122554052444,5.609999203681946,5.606926603544327,5.614054600397745,5.623872650237311,5.627733369100661,5.631186081114269,5.624474943251837,5.628259368169876,5.632651507286799,5.626502612658909,5.619879117466154,5.612726339839754,5.607573503539676,5.603600172769456,5.604848842393784,5.604784271830604,5.60414553256262,5.604199739864894,5.604914655004229,5.605133146331424,5.602872713406881,5.602809512047541,5.603703274045672,5.598696113768078,5.595378967693874,5.606272653170993,5.6106144791557675,5.6096650486900685,5.601022572744459,5.608431201889401,5.614267885117304,5.611792421340941,5.617077275684902,5.6154420886720935,5.614973784628369,5.621223286220006,5.624289100510734,5.617761509759086,5.619513232367379,5.619875720569064,5.622944370905557,5.626498309771219,5.621501445770263,5.614911312148684,5.615231386820475,5.605716948282151,5.609776014373416,5.60435859475817,5.60062084197998,5.591007131621951,5.590429497900463,5.591126666750227,5.592007714226132,5.5980871166501736,5.597969295865013,5.593947828383673,5.582886248543148,5.588002957616534,5.585325649806432,5.591416331699917,5.583964443206788,5.578037657056536,5.576179758707682,5.576360167775835,5.5743012133098775,5.570615942137582,5.56949772153582,5.563581017085484,5.5658097164971485,5.566783438410079,5.5632001116162275,5.567664794694811,5.56380741255624,5.560748340969995,5.55754433245886,5.561386136781603,5.563270584742229,5.5576532818022235,5.562854889460972,5.558829975128174,5.556609887168521,5.562731429508754,5.564792175520035,5.565070148876736,5.558731096131462,5.554002424648831,5.551706381071183,5.559213475953964,5.566084983235314,5.566355494090488,5.567264917918614,5.567710314478194,5.5652608950932825,5.562588581584749,5.563712955656507,5.556859055019562,5.564246233304342,5.572439344724018,5.566888863699777,5.567078049977621,5.576882779030573,5.577519011497498,5.576361035165332,5.577413485163733,5.580850328717912,5.585712961923507,5.584595051265898,5.589231590997605,5.57978195803506,5.580640780358086,5.578416329338437,5.575446820259096,5.579024048078629,5.586287241890317,5.5921314387094405,5.593482753208706,5.591235838617597,5.585041231200808,5.58491411322639,5.576709874471029,5.570998066947573,5.5702246484302345,5.573873960404169,5.577811514763605,5.581180453300476,5.579968620481945,5.575755463327681,5.568701356933231,5.573277303150722,5.573263355663845,5.572973094667708,5.570831684839157,5.570139589763824,5.571094839913505,5.568614252408346,5.574294405891782,5.571572860081989,5.572058821859813,5.5719076939991545,5.582686151776996,5.585464332217263,5.58550996666863,5.573768972215198,5.571963889258248,5.570520190965561,5.568182423001243,5.5733266330900655,5.571435881796338,5.565791256087167,5.557859444618226,5.5560540585290825,5.552598274321783,5.554941063835507,5.54900279385703,5.544646987460909,5.539321856271653,5.538224229358493,5.536689047586351,5.531579962230864,5.525967186973208,5.516568921861194,5.512441729363942,5.518559235618229,5.51332904951913,5.515976918311345,5.515761695589337,5.508407239686874,5.510051183473495,5.511969544774009,5.5183346600759595,5.529425586972919,5.5309531552451014,5.532738426753453,5.535701828911192,5.540530026526678,5.540645820753916,5.536820249330431,5.536614230700902,5.543860301517305,5.548353049868629,5.554632004102072,5.556411739758084,5.553670651572091,5.555465289524623,5.553896201224554,5.555920197850182,5.559541703405835,5.566788400922504,5.567833463350934,5.5646909077962246,5.563801244327,5.566896453357878,5.56410861582983,5.554601151602609,5.556366685458593,5.548938770521255,5.548984529858544,5.5488728977385025,5.54670414129893,5.538340137118385,5.537229582241602,5.533528010050455,5.537137802441914,5.540004890305655,5.537069111778623,5.5305273703166415,5.530064186595736,5.5220157248633255,5.520070859364101,5.519730206898282,5.502892090025402,5.5021402461188185,5.497789674713498,5.50722119467599,5.503597141447522,5.504731792495366,5.504241140683493,5.509676860627675,5.516541382244655,5.512462798754374,5.509407454445249,5.5083659728368115,5.507698971884591,5.50070054304032,5.502475835028149,5.501470319430034,5.499352997825261,5.493470378149124,5.502194042432876,5.500928960527693,5.510048575628371,5.507333225295656,5.506302056993756,5.49839567002796,5.499481097857156,5.506254649162292,5.509438620294844,5.5041751305262245,5.495721329961504,5.500404579298838,5.502504369190762,5.504000281152272,5.501650467373076,5.505237816628956,5.501224601836432,5.507499952543349,5.500763605889819,5.506667090597606,5.50285348211016,5.50981906368619,5.506710177376157,5.5034768388384885,5.509963024230233,5.515657598631725,5.51026228041876,5.499892614001321,5.505898217927842,5.513692869458881,5.514049980753944,5.517011053221566,5.5199707371847975,5.5154044662203106,5.511140448706491,5.508607270604088,5.513128548576718,5.513647340592884,5.523804074242001,5.528883676301865,5.519614613623846,5.520253698031108,5.521194882619948,5.524264015470233,5.521648148127965,5.522039137567793,5.5155385244460335,5.524737783840725,5.515615688051496,5.502564887773423,5.506323001498268,5.503509572574071,5.501988281522478,5.51512292453221,5.516098371006194,5.515242262113663,5.5073679833185105,5.50311938808078,5.503311449005491,5.496713486171905,5.501683075087412,5.501707746869042,5.5037225053423935,5.505168269929432,5.5021297273181755,5.50551514739082,5.514042622702463,5.5114840802692235,5.513220797266279,5.520711937404815,5.5251681441352485,5.525765759604318,5.525954008102417,5.519535073779878,5.51883703299931,5.52236278170631,5.527066320464724,5.5310129551660445,5.529551301683698,5.529286930674598,5.530643490382604,5.528775377500626,5.531381386802311,5.529487744967144,5.527020208040875,5.523144924072993,5.520874686468216,5.5235353186016996,5.527138180959793,5.521108689762298,5.510325733820599,5.504271298363096,5.503176882153466,5.501274063473656,5.506967712583995,5.505855126607985,5.504529563585916,5.506680939311073,5.500517068590437,5.497984103929428,5.494994548388891,5.489595926375617,5.483933435167586,5.484424450283959,5.490246764818828,5.485027368863424,5.4804702497663955,5.474177458172753,5.474596739950634,5.480402973720006,5.480545701299395,5.485762615430923,5.485683573995317,5.48990941842397,5.482582941509429,5.481654407864525,5.48168119475955,5.482685958771479,5.484051741872515,5.488223554974511,5.489001845178151,5.4968223458244685,5.49807763894399,5.499874869982402,5.487650824728466,5.49177331470308,5.499501273745583,5.499200731232054],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\",\"dash\":\"dash\",\"width\":5},\"mode\":\"lines\",\"name\":\"Averaged Test Loss\",\"x\":[19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],\"y\":[6.622350072860719,6.554796051979065,6.5252346992492685,6.513992214202882,6.508882331848145,6.508785390853882,6.50687952041626,6.508864903450012,6.5112082958221436,6.507956695556641,6.507745337486266,6.507224535942076,6.506826543807983,6.506522011756896,6.504234886169433,6.504863309860229,6.501028990745544,6.500941991806029,6.499848389625549,6.50062608718872,6.501642251014709,6.499684596061707,6.4977013111114506,6.498248744010926,6.4963712215423595,6.495899558067322,6.496607398986816,6.497141623497009,6.493670773506164,6.49518015384674,6.496896481513978,6.498592996597291,6.496778130531312,6.4932354927063,6.494038724899292,6.494087052345276,6.4938178777694695,6.4928692102432235,6.495032501220702,6.494100451469421,6.4914425134658815,6.491822266578676,6.491966962814331,6.4931457996368405,6.496084308624267,6.49689280986786,6.498889183998108,6.4975825548172,6.500053119659423,6.498865175247192,6.4971798181533815,6.494474601745606,6.497720432281495,6.500371313095094,6.5015448331832895,6.4950391292572025,6.4986268997192385,6.496624279022217,6.497449064254761,6.50150728225708,6.504094839096069,6.5053097009658805,6.509196424484253,6.505110716819764,6.502927541732789,6.501793694496156,6.498781943321228,6.501091933250428,6.500756430625916,6.503242588043213,6.502133655548096,6.5030484914779665,6.496632075309753,6.497758269309998,6.500916695594787,6.503330183029173,6.500242948532104,6.508459544181824,6.51026725769043,6.507732009887695,6.50446252822876,6.503461790084839,6.505090737342834,6.506870865821838,6.505139660835266,6.507587766647338,6.510125231742859,6.500989270210266,6.499286484718323,6.497617030143739,6.49674096107483,6.495634531974793,6.498567581176758,6.496697473526001,6.49238088130951,6.497591733932495,6.495884132385254,6.488962292671204,6.4867902278900145,6.4868604898452755,6.494633769989014,6.498703765869141,6.491924381256105,6.494784498214722,6.493793964385987,6.491361761093141,6.486794137954713,6.495095515251161,6.49620819091797,6.500503087043763,6.499882698059082,6.500971817970276,6.5016219377517706,6.499648356437684,6.50454308986664,6.502660751342774,6.506479930877686,6.506701087951661,6.511448287963867,6.511391520500183,6.505589509010314,6.501315093040467,6.50617094039917,6.502067399024964,6.504492259025573,6.507832288742065,6.5131005287170405,6.514529037475586,6.511985874176025,6.507898402214051,6.510708165168762,6.514451813697815,6.518077445030213,6.519725775718689,6.514325833320618,6.514575099945069,6.513219690322876,6.511449313163758,6.504139304161072,6.505972838401794,6.506562995910644,6.504144763946534,6.501327848434448,6.496246004104614,6.495557260513306,6.490801835060119,6.489583587646485,6.482948255538941,6.482445240020752,6.481208920478822,6.480032110214235,6.4723557233810425,6.469059109687806,6.466037917137146,6.464214158058167,6.458647036552429,6.454588603973388,6.4562209129333485,6.461588835716247,6.456667971611022,6.454439711570739,6.455322003364563,6.454596161842346,6.464519882202149,6.464486050605775,6.467295598983766,6.463953232765199,6.462811589241029,6.461691308021545,6.457118892669677,6.457516574859619,6.460450816154481,6.452528595924379,6.456715464591981,6.4635878801345825,6.4609333038330075,6.463399815559386,6.462958097457885,6.460208177566528,6.46148579120636,6.467067670822144,6.46344211101532,6.464819669723511,6.46429591178894,6.462223911285401,6.454022693634034,6.4574013471603395,6.462040901184083,6.459752869606019,6.460651659965516,6.464477396011354,6.458425760269166,6.460789966583253,6.463022327423096,6.4587575674057005,6.468626499176026,6.474304533004762,6.47601079940796,6.473971009254456,6.4749403476715095,6.469845056533814,6.47395453453064,6.466983437538148,6.458459305763245,6.461437892913819,6.464378809928895,6.460537457466125,6.465818119049072,6.471090936660767,6.4747657775878915,6.465173482894898,6.4799683570861815,6.4756773710250854,6.471695613861084,6.466480326652527,6.45861761569977,6.4540105342865,6.448582363128662,6.447790217399598,6.441221427917481,6.444291377067565,6.4424644947052006,6.450172805786134,6.451109623908998,6.452109789848328,6.4501401662826545,6.45088267326355,6.448226404190064,6.444260549545289,6.447323346138002,6.450367903709413,6.445717787742615,6.446809864044191,6.442753744125367,6.44330198764801,6.4449912309646615,6.444481706619263,6.449095225334168,6.45149211883545,6.461981844902039,6.459422039985657,6.456412267684938,6.450340962409973,6.44967966079712,6.4406030654907225,6.446814775466919,6.454351878166199,6.4550761699676515,6.464641666412354,6.466928124427795,6.46783332824707,6.46401207447052,6.4725499868392955,6.480789470672607,6.481729984283447,6.474487805366516,6.476074814796447,6.477381658554076,6.4735815763473505,6.465866255760193,6.4663176298141485,6.463193392753602,6.462822771072387,6.467181611061096,6.47192223072052,6.466473317146302,6.457838487625123,6.452844095230103,6.4399834156036375,6.433735537528992,6.4376876831054695,6.432870769500734,6.430982398986818,6.430257463455201,6.429425668716432,6.438214159011842,6.428629469871522,6.427308654785157,6.423651432991029,6.42504484653473,6.42021884918213,6.4293283700943,6.437554836273194,6.446414494514466,6.450887250900268,6.45354392528534,6.450247716903687,6.453067016601562,6.450723886489868,6.44905595779419,6.4485218048095705,6.449915814399719,6.449079060554504,6.4429931163787835,6.4550663948059075,6.457832312583924,6.46127667427063,6.459768176078796,6.460884547233582,6.463192415237426,6.467175912857056,6.467351055145264,6.4637335538864145,6.459958171844484,6.455573940277101,6.452338337898255,6.454011225700379,6.45700011253357,6.463262987136841,6.4622398853302006,6.464441013336183,6.472673559188843,6.475225806236269,6.474219226837159,6.471883368492127,6.466912198066711,6.463440918922423,6.469114995002746,6.468585252761841,6.477817344665527,6.475421738624572,6.46716206073761,6.459576225280761,6.451842761039734,6.453649640083314,6.464603877067567,6.467529559135437,6.461785864830017,6.468385243415833,6.467935585975647,6.464992904663087,6.462563490867616,6.468326449394227,6.468845677375794,6.4560231924057,6.465668106079101,6.477427148818969,6.473607397079467,6.473971652984618,6.459016370773315,6.460245156288147,6.4608971118927006,6.465195417404175,6.465371346473694,6.462844514846802,6.453247022628784,6.453891777992249,6.457624244689942,6.451309108734133,6.462230777740479,6.458760666847229,6.460258817672729,6.440639996528625,6.434739637374878,6.430695748329162,6.412778425216675,6.406826210021974,6.400320672988893,6.40636477470398,6.4127328395843515,6.40770788192749,6.419378542900086,6.42254285812378,6.423241233825685,6.422006297111512,6.42899866104126,6.424930930137634,6.419038224220277,6.42236135005951,6.424031305313111,6.4215979337692275,6.417344546318056,6.42980444431305,6.439524960517884,6.451914620399475,6.466474866867066,6.464745235443115,6.479999661445618,6.479386067390442,6.46766529083252,6.473056960105896,6.463740730285645,6.4673614501953125,6.46466600894928,6.465858936309814,6.466129994392395,6.459126663208008,6.4576996803283695,6.454706311225891,6.443822360038758,6.449853968620301,6.447508621215821,6.446540451049805,6.442534804344177,6.437398219108582,6.433031988143922,6.432685208320618,6.41935498714447,6.409774065017701,6.413427352905274,6.42270805835724,6.425135493278504,6.420449185371399,6.41959638595581,6.421222972869873,6.415313863754273,6.421041107177735,6.4267935991287235,6.427518343925476,6.432934331893921,6.430988836288453,6.432963800430298,6.43152687549591,6.436551928520203,6.4350203514099125,6.436244487762452,6.430437159538269,6.436580991744995,6.442082715034485,6.437972211837769,6.431142902374268,6.425123500823975,6.421989750862123,6.426463556289674,6.425444126129151,6.423721289634706,6.421980547904969,6.422001767158509,6.418351936340333,6.418405246734619,6.41410641670227,6.403493118286133,6.407232546806336,6.401369500160218,6.415817975997926,6.4147502183914185,6.417148900032043,6.413321256637573,6.408851575851441,6.413774442672731,6.406951689720155,6.412018084526062,6.409001970291138,6.409087467193603,6.4144651889801025,6.414030146598816,6.418076872825623,6.409704279899598,6.404353666305542,6.400883603096008,6.401129460334779,6.414275240898133,6.4114035129547124,6.404293584823609,6.389664530754089,6.398091030120849,6.39977879524231,6.392636895179749,6.406109714508058,6.4123631715774545,6.41595244407654,6.412931299209596,6.415490937232971,6.4067479372024545,6.402416324615479,6.399676752090454,6.401873755455019,6.414333629608156,6.421569371223452,6.423771643638612,6.436913776397706,6.4272531747818,6.417784881591797,6.427929353713989,6.428834986686706,6.410015916824341,6.419410538673401,6.429318475723266,6.424297976493835,6.41714506149292,6.4174815893173225,6.415542793273927,6.410142111778259,6.421065878868103,6.4220126390457155,6.417727017402649,6.416276478767396,6.411000227928162,6.407684969902039,6.408563375473023,6.4027225255966185,6.399030303955079,6.406622004508972,6.4057759761810305,6.402912902832031,6.423963952064514,6.423365926742554,6.408325600624085,6.398427271842958,6.400457715988161,6.393421339988709,6.396471309661865,6.401423549652099,6.392499113082885,6.390368890762328,6.398413944244385,6.406122851371766,6.40901861190796,6.419620871543885,6.416829729080201,6.415289998054505,6.422318696975709,6.416643595695496,6.4205393075943,6.421400117874145,6.418875670433045,6.419087290763855,6.427870368957519,6.431045365333556,6.437985157966614,6.45301764011383,6.458399820327759,6.458671593666077,6.467029643058777,6.473388981819153,6.469664168357849,6.465359926223755,6.458716607093812,6.448339200019838,6.448399019241334,6.448001980781555,6.462957596778871,6.470199179649354,6.472743177413942,6.4612172126770036,6.449979281425477,6.447731232643128,6.449293923377992,6.444893765449525,6.4407483577728275,6.422672152519227,6.422861719131471,6.4273685216903695,6.42389132976532,6.418785095214845,6.419551301002503,6.422910857200623,6.431033658981324,6.439491200447082,6.432269334793091,6.428188085556031,6.414552259445191,6.412396740913392,6.41163935661316,6.4231420278549205,6.42590491771698,6.420228314399719,6.42137188911438,6.42095582485199,6.41422998905182,6.427469682693482,6.416826939582824,6.411946368217468,6.418227791786193,6.421304750442504,6.41938214302063,6.409215974807739,6.3998612642288215,6.392185640335083,6.393777298927308,6.385326552391054,6.382620191574097,6.378973746299743,6.371654200553894,6.377413964271545,6.373960733413696,6.367823648452759,6.366348195075989,6.372733044624329,6.3777920484542845,6.376377582550049,6.379176378250122,6.378582572937012,6.365851783752442,6.355004286766054,6.351494932174683,6.348104667663574,6.3574998617172245,6.3652893066406255,6.36073865890503,6.367722082138062,6.372577810287476,6.380567812919617,6.3756858825683596,6.371975708007812,6.357976579666138,6.3693072319030755,6.378471708297729,6.36958692073822,6.361277532577515,6.355254745483399,6.356942009925843,6.3571414947509775,6.353628134727479,6.372702908515931,6.388468837738038,6.396856427192689,6.400413775444031,6.400135040283203,6.394528937339783,6.404555034637451,6.406965732574464,6.409590196609498,6.411157035827638,6.407631278038025,6.424749207496643,6.414049196243286,6.407956123352052,6.424666929244996,6.435419058799743,6.426696515083313,6.427889680862426,6.429719567298889,6.447471070289612,6.4412717580795285,6.4374738216400145,6.441626048088074,6.424967288970948,6.413049507141114,6.420601272583008,6.408357548713684,6.3983596563339225,6.392080688476562,6.404135751724243,6.4115334272384645,6.418012404441834,6.435702872276306,6.438327598571778,6.432530903816224,6.42280833721161,6.4267846107482915,6.424737024307253,6.42165837287903,6.412306690216065,6.415557551383973,6.413041353225709,6.402983117103577,6.411679720878602,6.422949981689453,6.428546380996704,6.431872487068175,6.436046767234803,6.433267140388489,6.4173415184021,6.404186153411866,6.391988730430603,6.3791230678558355,6.365818786621094,6.363627243041992,6.369429779052735,6.3785137891769415,6.3827754974365245,6.388871765136719,6.385414171218872,6.377616906166076,6.373083925247192,6.37851083278656,6.365215396881104,6.359757161140443,6.360357093811037,6.353731346130372,6.353379583358764,6.358016848564147,6.361512851715088,6.36257688999176,6.363300681114198,6.368569493293764,6.373586130142213,6.363102078437806,6.37417495250702,6.364629125595093,6.362090802192689,6.3686112642288215,6.383228397369384,6.3712665081024165,6.374784660339356,6.368044781684875,6.374580430984498,6.368044924736024,6.364568614959717,6.371742010116577,6.370160079002381,6.372336006164551,6.374062490463257,6.386089706420899,6.381296920776368,6.374392938613892,6.380924487113952,6.391860079765319,6.388480377197266,6.404768705368042,6.412778997421265,6.4008029460907,6.390949559211733,6.4017199277877825,6.396584796905519,6.399708104133607,6.408003401756289,6.413181018829347,6.408834981918336,6.414187979698181,6.418708443641663,6.413322806358337,6.408781671524048,6.411094570159912,6.416623926162719,6.4126119136810305,6.4153968572616575,6.408325457572937,6.403123164176942,6.392594718933107,6.384321403503419,6.380852127075197,6.376168799400331,6.368284511566163,6.369641494750977,6.36504397392273,6.363399624824524,6.375645804405211,6.368032193183899,6.360930418968201,6.353631210327149,6.365006184577943,6.375611805915833,6.372336459159852,6.3733870744705206,6.3822997331619264,6.377273035049439,6.386870169639587,6.390888285636901,6.399669742584228,6.405929398536682,6.419438266754151,6.431916236877441,6.4401303052902215,6.448459029197693,6.449664115905762,6.44666075706482,6.440348076820374,6.456721830368043,6.452146863937379,6.454762673377992,6.441390562057496,6.446173238754272,6.436311054229737,6.433122014999391,6.431603479385377,6.426419734954834,6.434238243103028,6.425783920288087,6.411789202690125,6.398233723640443,6.391998553276062,6.389166641235351,6.386820125579834,6.379082202911377,6.374488496780396,6.376523542404175,6.370911931991578,6.368479585647584,6.373153591156006,6.377028894424439,6.388419842720031,6.377744913101195,6.393191576004028,6.408310103416444,6.417376160621644,6.4247740745544455,6.411510705947877,6.410821318626405,6.423194360733033,6.4349520206451425,6.43560152053833,6.429220962524415,6.42322120666504,6.4177274942398075,6.4320454597473145,6.4200818061828615,6.407672333717347,6.4051079750061035,6.398731446266174,6.403480386734008,6.3887554645538325,6.386724376678466,6.380978918075561,6.363320922851562,6.356534504890441,6.345691275596618,6.350517511367799,6.3552086591720585,6.344371557235718,6.347272586822509,6.34851939678192,6.326957297325134,6.334232735633849,6.337794542312622,6.326586031913757,6.3342742919921875,6.350019383430481,6.354159092903138,6.355805325508118,6.35214765071869,6.359936761856081,6.351179909706117,6.351329588890076,6.356843900680542,6.356135773658752,6.361954712867736,6.362613320350647,6.354912543296814,6.359912896156311,6.349890351295472,6.34195442199707,6.366134572029113,6.362979698181152,6.374671196937562,6.382601666450501,6.381387543678285,6.3807043790817275,6.377627158164979,6.38020899295807,6.383747267723085,6.376203560829164,6.382743883132935,6.376810240745544,6.387390518188476,6.3757839679718025,6.387429594993591,6.385920119285584,6.394261980056763,6.404036760330201,6.400297713279725,6.398551654815674,6.39684660434723,6.410096883773805,6.397580909729005,6.3973552942276015,6.403755521774293,6.407513666152956,6.410416746139528,6.4145182371139535,6.406568813323975,6.419837141036988,6.427302289009095,6.429244685173036,6.4117960929870605,6.42214868068695,6.417745327949523,6.420491099357604,6.4239575386047365,6.409882044792176,6.424302411079408,6.429012680053711,6.431659245491028,6.422910094261169,6.413843345642091,6.412231683731079,6.411124062538148,6.407738757133485,6.411005592346192,6.411605095863343,6.408784508705139,6.403650712966919,6.405445694923401,6.409806132316589,6.417526006698608,6.426640343666077,6.410878753662109,6.40253415107727,6.393013548851013,6.390261006355286,6.391873121261597,6.391632294654846,6.388071417808532,6.391152501106261,6.4024681568145745,6.402740383148194,6.404968214035035,6.41418104171753,6.4044375181198125,6.399875450134278,6.397191619873047,6.389541506767274,6.390997695922852,6.375737094879151,6.378178644180298,6.370929265022279,6.377857303619386,6.379133915901185,6.372493481636048,6.376894855499268,6.364299511909485,6.373355293273927,6.3811234712600715,6.371664428710938,6.376221084594727,6.373013520240783,6.366412115097045,6.3489929199218755,6.36239140033722,6.372770500183106,6.37182116508484,6.377415132522583,6.37880539894104,6.392074203491211,6.383239841461182,6.376674628257752,6.385978364944459,6.3957137107849125,6.416438484191896,6.396273732185365,6.397193646430969,6.3903466224670415,6.376423621177674,6.3934039115905765,6.382716250419618,6.387146997451783,6.380079126358033,6.392406368255616,6.386377954483033,6.3841299533844005,6.38636817932129,6.390896415710451,6.378857231140137,6.37163965702057,6.381022047996522,6.383611607551575,6.365854215621948,6.365493750572205,6.353204154968262,6.371864080429076,6.369878554344177,6.371865296363831,6.375994014739991,6.370553350448609,6.375644469261171,6.372014570236207,6.390073418617249,6.395546531677247,6.39066333770752,6.3822702169418335,6.39078254699707,6.390639781951904,6.398400139808655,6.399212002754211,6.389321088790894,6.3917078495025645,6.411581659317017,6.407043671607973,6.4045616626739506,6.406747460365296,6.400883865356446,6.399649596214295,6.414590024948121,6.407678818702698,6.396068453788757,6.394857120513917,6.392821097373963,6.373633193969727,6.38977897167206,6.385995292663575,6.382397890090942,6.366481280326844,6.366376805305482,6.371683907508851,6.375913405418396,6.3602223873138435,6.3442932844161986,6.344639182090758,6.3417671680450445,6.3491973161697395],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Average batch mean loss during training from dataset with 751 users and 657 books\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('50a90494-3a96-4d60-aa5b-d0f11b6d8bb0');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Small recommendation test"
      ],
      "metadata": {
        "id": "2zQTli3Fn0pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tensor from the pickle file\n",
        "with open(embeddings_save_path, 'rb') as f:\n",
        "    loaded_embeddings = pickle.load(f)"
      ],
      "metadata": {
        "id": "xs5W5tG5n0Bz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "harry_potter_sorc_stone_emb_id = books[\"Book-Embedding-ID\"][\n",
        "    books[\"Book-Title\"].str.contains(\n",
        "        \"Harry Potter and the Sorcerer's Stone\")]\n",
        "\n",
        "books[\n",
        "    books[\"Book-Title\"].str.contains(\n",
        "        \"Harry Potter and the Sorcerer's Stone\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "8_WHDxEbd4uh",
        "outputId": "8927e083-4d3a-4587-e65b-fd38d88da485"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Book-Embedding-ID  Author-Embedding-ID  Year-Embedding-ID  Book-ID  \\\n",
              "129                 34                10383                 11     2062   \n",
              "\n",
              "           ISBN                             Book-Title    Book-Author  \\\n",
              "129  0590353403  Harry Potter and the Sorcerer's Stone  J. K. Rowling   \n",
              "\n",
              "     Year-Of-Publication   Publisher  \\\n",
              "129                 1998  Scholastic   \n",
              "\n",
              "                                           Image-URL-M  \n",
              "129  http://images.amazon.com/images/P/0590353403.0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59c69d7b-99b6-4bad-9da6-b9857110f5a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Embedding-ID</th>\n",
              "      <th>Author-Embedding-ID</th>\n",
              "      <th>Year-Embedding-ID</th>\n",
              "      <th>Book-ID</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Book-Title</th>\n",
              "      <th>Book-Author</th>\n",
              "      <th>Year-Of-Publication</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Image-URL-M</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>34</td>\n",
              "      <td>10383</td>\n",
              "      <td>11</td>\n",
              "      <td>2062</td>\n",
              "      <td>0590353403</td>\n",
              "      <td>Harry Potter and the Sorcerer's Stone</td>\n",
              "      <td>J. K. Rowling</td>\n",
              "      <td>1998</td>\n",
              "      <td>Scholastic</td>\n",
              "      <td>http://images.amazon.com/images/P/0590353403.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59c69d7b-99b6-4bad-9da6-b9857110f5a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59c69d7b-99b6-4bad-9da6-b9857110f5a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59c69d7b-99b6-4bad-9da6-b9857110f5a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a7c900c3-a1f0-42d8-aadc-1e536d442636\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a7c900c3-a1f0-42d8-aadc-1e536d442636')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a7c900c3-a1f0-42d8-aadc-1e536d442636 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_nearest_neighbours_model(embeddings, metric=\"cosine\"):\n",
        "    knn_model = NearestNeighbors(metric=metric, n_jobs=-1)\n",
        "    knn_model.fit(embeddings)\n",
        "    return knn_model\n",
        "\n",
        "def get_k_neighbours_for_vector(vector, knn_model, k=5):\n",
        "    _, cos_indices = knn_model.kneighbors(\n",
        "        vector, n_neighbors=k)\n",
        "    return cos_indices\n",
        "\n",
        "def convert_emb_ids_to_book_ids(ratings, emb_ids):\n",
        "  recommended_book_ids = ratings[\n",
        "      ratings[\"Book-Embedding-ID\"].isin(emb_ids[0])]\n",
        "\n",
        "  sorted_recommended_book_ids = recommended_book_ids.sort_values(\n",
        "      by=[\"Book-Embedding-ID\"],\n",
        "      key=lambda x: x.map(\n",
        "          {v: i for i, v in enumerate(emb_ids[0])}))\n",
        "\n",
        "  sorted_recommended_book_ids = sorted_recommended_book_ids[\"Book-ID\"].unique()\n",
        "  return sorted_recommended_book_ids\n",
        "\n",
        "def get_book_titles_from_book_ids(books_metadata, book_ids):\n",
        "  recommended_books = books_metadata[\n",
        "      books_metadata['Book-ID'].isin(book_ids)].sort_values(\n",
        "          by=[\"Book-ID\"], key=lambda x: x.map(\n",
        "              {v: i for i, v in enumerate(book_ids)}))\n",
        "  return recommended_books['Book-Title'].unique()\n",
        "\n",
        "def get_book_recommendations(\n",
        "    emb_id, embeddings,\n",
        "    book_ratings, book_metadata,\n",
        "    number_of_recommendations):\n",
        "  book_emb = embeddings[emb_id].reshape(1,-1)\n",
        "\n",
        "  knn_model = get_k_nearest_neighbours_model(\n",
        "      embeddings, metric=\"cosine\")\n",
        "  recommended_book_emb_ids = get_k_neighbours_for_vector(\n",
        "      book_emb, knn_model, k=number_of_recommendations)\n",
        "  recommended_book_ids = convert_emb_ids_to_book_ids(\n",
        "      book_ratings, recommended_book_emb_ids)\n",
        "  recommended_book_titles = get_book_titles_from_book_ids(\n",
        "      book_metadata, recommended_book_ids)\n",
        "\n",
        "  return recommended_book_titles"
      ],
      "metadata": {
        "id": "gxju2deZ2rvU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_book_recommendations(\n",
        "    harry_potter_sorc_stone_emb_id,\n",
        "    loaded_embeddings, ratings, books, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ditFrKO28Ap5",
        "outputId": "2d05b40c-ca9a-4620-e857-bdac8f98ff26"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Harry Potter and the Sorcerer's Stone\",\n",
              "       'Harry Potter and the Chamber of Secrets', 'The Horse Whisperer',\n",
              "       'Harry Potter and the Goblet of Fire',\n",
              "       'Harry Potter and the Prisoner of Azkaban'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}